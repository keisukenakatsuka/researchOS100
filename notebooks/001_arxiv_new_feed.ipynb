{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "faa36963-8d33-4505-b1d6-0dde8ee49327",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 001_arxiv_new_feed\n",
    "# ============================================================\n",
    "#\n",
    "# Overview\n",
    "# --------\n",
    "# This notebook is responsible for fetching and normalizing newly published\n",
    "# papers from arXiv on a periodic basis.\n",
    "#\n",
    "# The primary goal is to build a lightweight, reproducible \"research intake\"\n",
    "# layer that continuously monitors arXiv categories of interest and converts\n",
    "# raw feed data into a structured format suitable for downstream processing\n",
    "# (e.g., summarization, classification, note-taking, and slide generation).\n",
    "#\n",
    "# This notebook is intentionally scoped to:\n",
    "#   - Retrieval of new arXiv entries\n",
    "#   - Minimal cleaning / normalization\n",
    "#   - Deduplication and basic metadata handling\n",
    "#\n",
    "# Any heavy analysis, LLM-based interpretation, or knowledge synthesis should\n",
    "# be handled in subsequent notebooks.\n",
    "#\n",
    "#\n",
    "# Structure\n",
    "# ---------\n",
    "# 1. Configuration\n",
    "#    - arXiv categories\n",
    "#    - query parameters (date range, max results, update window)\n",
    "#    - output paths / storage settings\n",
    "#\n",
    "# 2. Feed Retrieval\n",
    "#    - Fetch recent arXiv entries via API / RSS\n",
    "#    - Handle pagination and update windows\n",
    "#\n",
    "# 3. Normalization\n",
    "#    - Standardize metadata (title, authors, abstract, published date, URL)\n",
    "#    - Convert into a tabular / JSON-friendly format\n",
    "#\n",
    "# 4. Deduplication & Filtering\n",
    "#    - Remove already-seen papers\n",
    "#    - Apply basic keyword or category-level filters if needed\n",
    "#\n",
    "# 5. Output\n",
    "#    - Persist normalized results for downstream notebooks\n",
    "#    - Log execution summary (counts, timestamps)\n",
    "#\n",
    "#\n",
    "# Notes\n",
    "# -----\n",
    "# - This notebook is designed to be idempotent when run with the same inputs.\n",
    "# - Downstream notebooks should treat its outputs as append-only sources.\n",
    "# - Keep this notebook free of model-specific or product-specific logic\n",
    "#   to ensure long-term maintainability.\n",
    "#\n",
    "# - Naming convention:\n",
    "#   001_xxx notebooks = \"data intake / ingestion\" layer\n",
    "#\n",
    "# ============================================================\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1e8ca74c-51ed-4277-aaf3-58d14ec6293b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fff6d8b0854c4d939180426c9affb898",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<h3>arXiv New Feed ‚Äî Configuration</h3>'), Textarea(value='venture capital OR start‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ============================================================\n",
    "# 1. Configuration (Widgets) ‚Äî show output only after Apply\n",
    "# ============================================================\n",
    "\n",
    "from dataclasses import dataclass\n",
    "from datetime import datetime, timezone, timedelta\n",
    "import os\n",
    "\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, clear_output\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class ArxivFeedConfig:\n",
    "    query_text: str\n",
    "    max_results: int\n",
    "    lookback_days: int | None\n",
    "\n",
    "    sort_by: str = \"submittedDate\"\n",
    "    sort_order: str = \"descending\"\n",
    "    output_format: str = \"csv\"    \n",
    "    output_basename: str = \"arxiv_new_feed\"\n",
    "    enable_dedup: bool = True\n",
    "\n",
    "\n",
    "# ------------------------\n",
    "# Widgets (User Inputs)\n",
    "# ------------------------\n",
    "w_query = widgets.Textarea(\n",
    "    value=\"venture capital OR startup OR innovation policy\",\n",
    "    description=\"query_text\",\n",
    "    layout=widgets.Layout(width=\"900px\", height=\"80px\"),\n",
    ")\n",
    "\n",
    "w_max = widgets.IntSlider(\n",
    "    value=30, min=1, max=200, step=1,\n",
    "    description=\"max_results\",\n",
    "    continuous_update=False,\n",
    "    layout=widgets.Layout(width=\"900px\"),\n",
    ")\n",
    "\n",
    "w_lookback = widgets.IntSlider(\n",
    "    value=14, min=0, max=365, step=1,\n",
    "    description=\"lookback_days\",\n",
    "    continuous_update=False,\n",
    "    layout=widgets.Layout(width=\"900px\"),\n",
    ")\n",
    "\n",
    "btn_apply = widgets.Button(description=\"Apply\", button_style=\"primary\")\n",
    "out = widgets.Output()\n",
    "\n",
    "\n",
    "# ------------------------\n",
    "# Paths (Project Layout)\n",
    "# ------------------------\n",
    "NOTEBOOK_DIR = os.getcwd()\n",
    "PROJECT_ROOT = os.path.abspath(os.path.join(NOTEBOOK_DIR, \"..\"))  # assume ./notebooks\n",
    "DATA_DIR = os.path.join(PROJECT_ROOT, \"data\")\n",
    "ARXIV_DIR = os.path.join(DATA_DIR, \"arxiv\")\n",
    "CACHE_DIR = os.path.join(DATA_DIR, \"cache\")\n",
    "\n",
    "os.makedirs(ARXIV_DIR, exist_ok=True)\n",
    "os.makedirs(CACHE_DIR, exist_ok=True)\n",
    "\n",
    "\n",
    "# ------------------------\n",
    "# Apply Handler\n",
    "# ------------------------\n",
    "def apply_config(_):\n",
    "    global CFG, RUN_TS, CUTOFF_DT, OUTPUT_PATH\n",
    "\n",
    "    lookback_days = int(w_lookback.value)\n",
    "    lookback_days = None if lookback_days == 0 else lookback_days\n",
    "\n",
    "    CFG = ArxivFeedConfig(\n",
    "        query_text=w_query.value.strip(),\n",
    "        max_results=int(w_max.value),\n",
    "        lookback_days=lookback_days,\n",
    "    )\n",
    "\n",
    "    RUN_TS = datetime.now(timezone.utc).strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "    CUTOFF_DT = None\n",
    "    if CFG.lookback_days is not None:\n",
    "        CUTOFF_DT = datetime.now(timezone.utc) - timedelta(days=int(CFG.lookback_days))\n",
    "\n",
    "    OUTPUT_PATH = os.path.join(\n",
    "        ARXIV_DIR,\n",
    "        f\"{CFG.output_basename}_{RUN_TS}.{CFG.output_format}\"\n",
    "    )\n",
    "\n",
    "    with out:\n",
    "        clear_output(wait=True)\n",
    "        print(\"‚úÖ Configuration applied\")\n",
    "        print(f\"  query_text   : {CFG.query_text}\")\n",
    "        print(f\"  max_results  : {CFG.max_results}\")\n",
    "        print(f\"  lookback_days: {CFG.lookback_days}\")\n",
    "        print(f\"  cutoff_dt    : {CUTOFF_DT}\")\n",
    "        print(f\"  output_path  : {OUTPUT_PATH}\")\n",
    "\n",
    "\n",
    "btn_apply.on_click(apply_config)\n",
    "\n",
    "\n",
    "# ------------------------\n",
    "# Render UI (NO auto-apply)\n",
    "# ------------------------\n",
    "display(widgets.VBox([\n",
    "    widgets.HTML(\"<h3>arXiv New Feed ‚Äî Configuration</h3>\"),\n",
    "    w_query,\n",
    "    w_max,\n",
    "    w_lookback,\n",
    "    btn_apply,\n",
    "    out\n",
    "]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f87b476b-b648-4689-b422-87060566fd82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîé Fetching arXiv entries...\n",
      "‚úÖ Retrieved 30 entries\n",
      "  - latest published: 2025-12-29 18:59:33+00:00\n",
      "  - oldest  published: 2025-12-29 08:26:27+00:00\n",
      "  - example arxiv_id : 2512.23707v1\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# 2. Feed Retrieval\n",
    "# ============================================================\n",
    "#\n",
    "# This section fetches recent arXiv entries using the official arXiv API (Atom feed).\n",
    "# It is intentionally lightweight:\n",
    "#   - Build a query from CFG\n",
    "#   - Fetch entries (with pagination safety)\n",
    "#   - Parse Atom -> list[dict]\n",
    "#   - (Optional) apply lookback filter using CUTOFF_DT\n",
    "#\n",
    "# Output:\n",
    "#   - entries: List[dict] with normalized raw fields (still \"pre-DataFrame\")\n",
    "#\n",
    "# Prereq:\n",
    "#   - Run \"1. Configuration\" and click Apply so CFG / CUTOFF_DT exist.\n",
    "\n",
    "import time\n",
    "import urllib.parse\n",
    "import requests\n",
    "import feedparser\n",
    "\n",
    "\n",
    "# ------------------------\n",
    "# Helpers\n",
    "# ------------------------\n",
    "def build_arxiv_api_url(\n",
    "    query_text: str,\n",
    "    start: int = 0,\n",
    "    max_results: int = 30,\n",
    "    sort_by: str = \"submittedDate\",\n",
    "    sort_order: str = \"descending\",\n",
    ") -> str:\n",
    "    base = \"http://export.arxiv.org/api/query\"\n",
    "    params = {\n",
    "        \"search_query\": f\"all:{query_text}\",\n",
    "        \"start\": start,\n",
    "        \"max_results\": max_results,\n",
    "        \"sortBy\": sort_by,\n",
    "        \"sortOrder\": sort_order,\n",
    "    }\n",
    "    return f\"{base}?{urllib.parse.urlencode(params)}\"\n",
    "\n",
    "\n",
    "def parse_arxiv_entry(e) -> dict:\n",
    "    \"\"\"\n",
    "    Convert feedparser entry into a dict with stable keys.\n",
    "    \"\"\"\n",
    "    # arXiv id is usually in e.id (e.g., http://arxiv.org/abs/xxxx.xxxxxv1)\n",
    "    arxiv_id = None\n",
    "    if getattr(e, \"id\", None):\n",
    "        arxiv_id = e.id.split(\"/abs/\")[-1]\n",
    "\n",
    "    # \"published_parsed\" is a time.struct_time\n",
    "    published_dt = None\n",
    "    if getattr(e, \"published_parsed\", None):\n",
    "        published_dt = datetime(*e.published_parsed[:6], tzinfo=timezone.utc)\n",
    "\n",
    "    updated_dt = None\n",
    "    if getattr(e, \"updated_parsed\", None):\n",
    "        updated_dt = datetime(*e.updated_parsed[:6], tzinfo=timezone.utc)\n",
    "\n",
    "    authors = []\n",
    "    if getattr(e, \"authors\", None):\n",
    "        authors = [a.get(\"name\") for a in e.authors if a.get(\"name\")]\n",
    "\n",
    "    # Primary link (abs page)\n",
    "    abs_url = getattr(e, \"link\", None)\n",
    "\n",
    "    # PDF link (sometimes in links)\n",
    "    pdf_url = None\n",
    "    if getattr(e, \"links\", None):\n",
    "        for l in e.links:\n",
    "            if l.get(\"type\") == \"application/pdf\":\n",
    "                pdf_url = l.get(\"href\")\n",
    "                break\n",
    "\n",
    "    # Categories\n",
    "    categories = []\n",
    "    if getattr(e, \"tags\", None):\n",
    "        categories = [t.get(\"term\") for t in e.tags if t.get(\"term\")]\n",
    "\n",
    "    return {\n",
    "        \"arxiv_id\": arxiv_id,\n",
    "        \"title\": (getattr(e, \"title\", \"\") or \"\").strip().replace(\"\\n\", \" \"),\n",
    "        \"authors\": authors,\n",
    "        \"abstract\": (getattr(e, \"summary\", \"\") or \"\").strip().replace(\"\\n\", \" \"),\n",
    "        \"published_dt\": published_dt,\n",
    "        \"updated_dt\": updated_dt,\n",
    "        \"abs_url\": abs_url,\n",
    "        \"pdf_url\": pdf_url,\n",
    "        \"categories\": categories,\n",
    "    }\n",
    "\n",
    "\n",
    "def fetch_arxiv_entries(\n",
    "    cfg,\n",
    "    cutoff_dt=None,\n",
    "    timeout_sec: int = 30,\n",
    "    polite_sleep_sec: float = 1.0,\n",
    ") -> list[dict]:\n",
    "    \"\"\"\n",
    "    Fetch entries from arXiv API (single page).\n",
    "    NOTE: We request cfg.max_results in one shot. If you later need >2000,\n",
    "          add pagination (start offsets).\n",
    "    \"\"\"\n",
    "    url = build_arxiv_api_url(\n",
    "        query_text=cfg.query_text,\n",
    "        start=0,\n",
    "        max_results=cfg.max_results,\n",
    "        sort_by=cfg.sort_by,\n",
    "        sort_order=cfg.sort_order,\n",
    "    )\n",
    "\n",
    "    # arXiv requests polite usage; we also set a User-Agent.\n",
    "    headers = {\"User-Agent\": \"researchOS/001_arxiv_new_feed (contact: you@example.com)\"}\n",
    "\n",
    "    resp = requests.get(url, headers=headers, timeout=timeout_sec)\n",
    "    resp.raise_for_status()\n",
    "\n",
    "    # polite delay (helps avoid being rate-limited if you run repeatedly)\n",
    "    time.sleep(polite_sleep_sec)\n",
    "\n",
    "    feed = feedparser.parse(resp.text)\n",
    "\n",
    "    entries = []\n",
    "    for e in feed.entries:\n",
    "        d = parse_arxiv_entry(e)\n",
    "\n",
    "        # optional lookback filter\n",
    "        if cutoff_dt is not None and d[\"published_dt\"] is not None:\n",
    "            if d[\"published_dt\"] < cutoff_dt:\n",
    "                continue\n",
    "\n",
    "        entries.append(d)\n",
    "\n",
    "    return entries\n",
    "\n",
    "\n",
    "# ------------------------\n",
    "# Run Retrieval\n",
    "# ------------------------\n",
    "if \"CFG\" not in globals():\n",
    "    raise RuntimeError(\"CFG is not defined. Run '1. Configuration' and click Apply first.\")\n",
    "\n",
    "print(\"üîé Fetching arXiv entries...\")\n",
    "entries = fetch_arxiv_entries(CFG, cutoff_dt=CUTOFF_DT)\n",
    "\n",
    "print(f\"‚úÖ Retrieved {len(entries)} entries\")\n",
    "if len(entries) > 0:\n",
    "    print(\"  - latest published:\", max(e[\"published_dt\"] for e in entries if e[\"published_dt\"] is not None))\n",
    "    print(\"  - oldest  published:\", min(e[\"published_dt\"] for e in entries if e[\"published_dt\"] is not None))\n",
    "    print(\"  - example arxiv_id :\", entries[0][\"arxiv_id\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bccc4660-dd7e-4935-916a-2c707a38c88d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Normalization complete\n",
      "  rows   : 30\n",
      "  columns: ['arxiv_id', 'title', 'authors_str', 'authors', 'abstract', 'published_dt', 'updated_dt', 'abs_url', 'pdf_url', 'categories_str', 'categories', 'source']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>arxiv_id</th>\n",
       "      <th>title</th>\n",
       "      <th>authors_str</th>\n",
       "      <th>authors</th>\n",
       "      <th>abstract</th>\n",
       "      <th>published_dt</th>\n",
       "      <th>updated_dt</th>\n",
       "      <th>abs_url</th>\n",
       "      <th>pdf_url</th>\n",
       "      <th>categories_str</th>\n",
       "      <th>categories</th>\n",
       "      <th>source</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2512.23707v1</td>\n",
       "      <td>Training AI Co-Scientists Using Rubric Rewards</td>\n",
       "      <td>Shashwat Goel, Rishi Hazra, Dulhan Jayalath, T...</td>\n",
       "      <td>[Shashwat Goel, Rishi Hazra, Dulhan Jayalath, ...</td>\n",
       "      <td>AI co-scientists are emerging as a tool to ass...</td>\n",
       "      <td>2025-12-29 18:59:33+00:00</td>\n",
       "      <td>2025-12-29 18:59:33+00:00</td>\n",
       "      <td>https://arxiv.org/abs/2512.23707v1</td>\n",
       "      <td>https://arxiv.org/pdf/2512.23707v1</td>\n",
       "      <td>cs.LG, cs.CL, cs.HC</td>\n",
       "      <td>[cs.LG, cs.CL, cs.HC]</td>\n",
       "      <td>arxiv_api</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2512.23703v1</td>\n",
       "      <td>Robo-Dopamine: General Process Reward Modeling...</td>\n",
       "      <td>Huajie Tan, Sixiang Chen, Yijie Xu, Zixiao Wan...</td>\n",
       "      <td>[Huajie Tan, Sixiang Chen, Yijie Xu, Zixiao Wa...</td>\n",
       "      <td>The primary obstacle for applying reinforcemen...</td>\n",
       "      <td>2025-12-29 18:57:44+00:00</td>\n",
       "      <td>2025-12-29 18:57:44+00:00</td>\n",
       "      <td>https://arxiv.org/abs/2512.23703v1</td>\n",
       "      <td>https://arxiv.org/pdf/2512.23703v1</td>\n",
       "      <td>cs.RO</td>\n",
       "      <td>[cs.RO]</td>\n",
       "      <td>arxiv_api</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2512.23694v1</td>\n",
       "      <td>Bellman Calibration for V-Learning in Offline ...</td>\n",
       "      <td>Lars van der Laan, Nathan Kallus</td>\n",
       "      <td>[Lars van der Laan, Nathan Kallus]</td>\n",
       "      <td>We introduce Iterated Bellman Calibration, a s...</td>\n",
       "      <td>2025-12-29 18:52:18+00:00</td>\n",
       "      <td>2025-12-29 18:52:18+00:00</td>\n",
       "      <td>https://arxiv.org/abs/2512.23694v1</td>\n",
       "      <td>https://arxiv.org/pdf/2512.23694v1</td>\n",
       "      <td>stat.ML, cs.LG, econ.EM</td>\n",
       "      <td>[stat.ML, cs.LG, econ.EM]</td>\n",
       "      <td>arxiv_api</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2512.23688v1</td>\n",
       "      <td>Unlocking WebRTC for End User Driven Innovation</td>\n",
       "      <td>Kundan Singh</td>\n",
       "      <td>[Kundan Singh]</td>\n",
       "      <td>We present a software architecture to enable e...</td>\n",
       "      <td>2025-12-29 18:44:59+00:00</td>\n",
       "      <td>2025-12-29 18:44:59+00:00</td>\n",
       "      <td>https://arxiv.org/abs/2512.23688v1</td>\n",
       "      <td>https://arxiv.org/pdf/2512.23688v1</td>\n",
       "      <td>cs.MM, cs.NI</td>\n",
       "      <td>[cs.MM, cs.NI]</td>\n",
       "      <td>arxiv_api</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2512.23650v1</td>\n",
       "      <td>Do You Have Freestyle? Expressive Humanoid Loc...</td>\n",
       "      <td>Zhe Li, Cheng Chi, Yangyang Wei, Boan Zhu, Tao...</td>\n",
       "      <td>[Zhe Li, Cheng Chi, Yangyang Wei, Boan Zhu, Ta...</td>\n",
       "      <td>Humans intuitively move to sound, but current ...</td>\n",
       "      <td>2025-12-29 17:59:24+00:00</td>\n",
       "      <td>2025-12-29 17:59:24+00:00</td>\n",
       "      <td>https://arxiv.org/abs/2512.23650v1</td>\n",
       "      <td>https://arxiv.org/pdf/2512.23650v1</td>\n",
       "      <td>cs.RO</td>\n",
       "      <td>[cs.RO]</td>\n",
       "      <td>arxiv_api</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2512.23649v1</td>\n",
       "      <td>RoboMirror: Understand Before You Imitate for ...</td>\n",
       "      <td>Zhe Li, Cheng Chi, Yangyang Wei, Boan Zhu, Tao...</td>\n",
       "      <td>[Zhe Li, Cheng Chi, Yangyang Wei, Boan Zhu, Ta...</td>\n",
       "      <td>Humans learn locomotion through visual observa...</td>\n",
       "      <td>2025-12-29 17:59:19+00:00</td>\n",
       "      <td>2025-12-29 17:59:19+00:00</td>\n",
       "      <td>https://arxiv.org/abs/2512.23649v1</td>\n",
       "      <td>https://arxiv.org/pdf/2512.23649v1</td>\n",
       "      <td>cs.RO, cs.CV</td>\n",
       "      <td>[cs.RO, cs.CV]</td>\n",
       "      <td>arxiv_api</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2512.23626v1</td>\n",
       "      <td>Regret-Based Federated Causal Discovery with U...</td>\n",
       "      <td>Federico Baldo, Charles K. Assaad</td>\n",
       "      <td>[Federico Baldo, Charles K. Assaad]</td>\n",
       "      <td>Most causal discovery methods recover a comple...</td>\n",
       "      <td>2025-12-29 17:30:01+00:00</td>\n",
       "      <td>2025-12-29 17:30:01+00:00</td>\n",
       "      <td>https://arxiv.org/abs/2512.23626v1</td>\n",
       "      <td>https://arxiv.org/pdf/2512.23626v1</td>\n",
       "      <td>cs.AI, cs.LG</td>\n",
       "      <td>[cs.AI, cs.LG]</td>\n",
       "      <td>arxiv_api</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2512.23618v1</td>\n",
       "      <td>Verifiable Off-Chain Governance</td>\n",
       "      <td>Jake Hartnell, Eugenio Battaglia</td>\n",
       "      <td>[Jake Hartnell, Eugenio Battaglia]</td>\n",
       "      <td>Current DAO governance praxis limits organizat...</td>\n",
       "      <td>2025-12-29 17:24:10+00:00</td>\n",
       "      <td>2025-12-29 17:24:10+00:00</td>\n",
       "      <td>https://arxiv.org/abs/2512.23618v1</td>\n",
       "      <td>https://arxiv.org/pdf/2512.23618v1</td>\n",
       "      <td>cs.GT, cs.CE</td>\n",
       "      <td>[cs.GT, cs.CE]</td>\n",
       "      <td>arxiv_api</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2512.23617v1</td>\n",
       "      <td>Le Cam Distortion: A Decision-Theoretic Framew...</td>\n",
       "      <td>Deniz Akdemir</td>\n",
       "      <td>[Deniz Akdemir]</td>\n",
       "      <td>Distribution shift is the defining challenge o...</td>\n",
       "      <td>2025-12-29 17:21:44+00:00</td>\n",
       "      <td>2025-12-29 17:21:44+00:00</td>\n",
       "      <td>https://arxiv.org/abs/2512.23617v1</td>\n",
       "      <td>https://arxiv.org/pdf/2512.23617v1</td>\n",
       "      <td>cs.LG, cs.AI, math.ST, stat.ME, stat.ML</td>\n",
       "      <td>[cs.LG, cs.AI, math.ST, stat.ME, stat.ML]</td>\n",
       "      <td>arxiv_api</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2512.23611v1</td>\n",
       "      <td>Close the Loop: Synthesizing Infinite Tool-Use...</td>\n",
       "      <td>Yuwen Li, Wei Zhang, Zelong Huang, Mason Yang,...</td>\n",
       "      <td>[Yuwen Li, Wei Zhang, Zelong Huang, Mason Yang...</td>\n",
       "      <td>Enabling Large Language Models (LLMs) to relia...</td>\n",
       "      <td>2025-12-29 17:12:39+00:00</td>\n",
       "      <td>2025-12-29 17:12:39+00:00</td>\n",
       "      <td>https://arxiv.org/abs/2512.23611v1</td>\n",
       "      <td>https://arxiv.org/pdf/2512.23611v1</td>\n",
       "      <td>cs.CL</td>\n",
       "      <td>[cs.CL]</td>\n",
       "      <td>arxiv_api</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       arxiv_id                                              title  \\\n",
       "0  2512.23707v1     Training AI Co-Scientists Using Rubric Rewards   \n",
       "1  2512.23703v1  Robo-Dopamine: General Process Reward Modeling...   \n",
       "2  2512.23694v1  Bellman Calibration for V-Learning in Offline ...   \n",
       "3  2512.23688v1    Unlocking WebRTC for End User Driven Innovation   \n",
       "4  2512.23650v1  Do You Have Freestyle? Expressive Humanoid Loc...   \n",
       "5  2512.23649v1  RoboMirror: Understand Before You Imitate for ...   \n",
       "6  2512.23626v1  Regret-Based Federated Causal Discovery with U...   \n",
       "7  2512.23618v1                    Verifiable Off-Chain Governance   \n",
       "8  2512.23617v1  Le Cam Distortion: A Decision-Theoretic Framew...   \n",
       "9  2512.23611v1  Close the Loop: Synthesizing Infinite Tool-Use...   \n",
       "\n",
       "                                         authors_str  \\\n",
       "0  Shashwat Goel, Rishi Hazra, Dulhan Jayalath, T...   \n",
       "1  Huajie Tan, Sixiang Chen, Yijie Xu, Zixiao Wan...   \n",
       "2                   Lars van der Laan, Nathan Kallus   \n",
       "3                                       Kundan Singh   \n",
       "4  Zhe Li, Cheng Chi, Yangyang Wei, Boan Zhu, Tao...   \n",
       "5  Zhe Li, Cheng Chi, Yangyang Wei, Boan Zhu, Tao...   \n",
       "6                  Federico Baldo, Charles K. Assaad   \n",
       "7                   Jake Hartnell, Eugenio Battaglia   \n",
       "8                                      Deniz Akdemir   \n",
       "9  Yuwen Li, Wei Zhang, Zelong Huang, Mason Yang,...   \n",
       "\n",
       "                                             authors  \\\n",
       "0  [Shashwat Goel, Rishi Hazra, Dulhan Jayalath, ...   \n",
       "1  [Huajie Tan, Sixiang Chen, Yijie Xu, Zixiao Wa...   \n",
       "2                 [Lars van der Laan, Nathan Kallus]   \n",
       "3                                     [Kundan Singh]   \n",
       "4  [Zhe Li, Cheng Chi, Yangyang Wei, Boan Zhu, Ta...   \n",
       "5  [Zhe Li, Cheng Chi, Yangyang Wei, Boan Zhu, Ta...   \n",
       "6                [Federico Baldo, Charles K. Assaad]   \n",
       "7                 [Jake Hartnell, Eugenio Battaglia]   \n",
       "8                                    [Deniz Akdemir]   \n",
       "9  [Yuwen Li, Wei Zhang, Zelong Huang, Mason Yang...   \n",
       "\n",
       "                                            abstract  \\\n",
       "0  AI co-scientists are emerging as a tool to ass...   \n",
       "1  The primary obstacle for applying reinforcemen...   \n",
       "2  We introduce Iterated Bellman Calibration, a s...   \n",
       "3  We present a software architecture to enable e...   \n",
       "4  Humans intuitively move to sound, but current ...   \n",
       "5  Humans learn locomotion through visual observa...   \n",
       "6  Most causal discovery methods recover a comple...   \n",
       "7  Current DAO governance praxis limits organizat...   \n",
       "8  Distribution shift is the defining challenge o...   \n",
       "9  Enabling Large Language Models (LLMs) to relia...   \n",
       "\n",
       "               published_dt                updated_dt  \\\n",
       "0 2025-12-29 18:59:33+00:00 2025-12-29 18:59:33+00:00   \n",
       "1 2025-12-29 18:57:44+00:00 2025-12-29 18:57:44+00:00   \n",
       "2 2025-12-29 18:52:18+00:00 2025-12-29 18:52:18+00:00   \n",
       "3 2025-12-29 18:44:59+00:00 2025-12-29 18:44:59+00:00   \n",
       "4 2025-12-29 17:59:24+00:00 2025-12-29 17:59:24+00:00   \n",
       "5 2025-12-29 17:59:19+00:00 2025-12-29 17:59:19+00:00   \n",
       "6 2025-12-29 17:30:01+00:00 2025-12-29 17:30:01+00:00   \n",
       "7 2025-12-29 17:24:10+00:00 2025-12-29 17:24:10+00:00   \n",
       "8 2025-12-29 17:21:44+00:00 2025-12-29 17:21:44+00:00   \n",
       "9 2025-12-29 17:12:39+00:00 2025-12-29 17:12:39+00:00   \n",
       "\n",
       "                              abs_url                             pdf_url  \\\n",
       "0  https://arxiv.org/abs/2512.23707v1  https://arxiv.org/pdf/2512.23707v1   \n",
       "1  https://arxiv.org/abs/2512.23703v1  https://arxiv.org/pdf/2512.23703v1   \n",
       "2  https://arxiv.org/abs/2512.23694v1  https://arxiv.org/pdf/2512.23694v1   \n",
       "3  https://arxiv.org/abs/2512.23688v1  https://arxiv.org/pdf/2512.23688v1   \n",
       "4  https://arxiv.org/abs/2512.23650v1  https://arxiv.org/pdf/2512.23650v1   \n",
       "5  https://arxiv.org/abs/2512.23649v1  https://arxiv.org/pdf/2512.23649v1   \n",
       "6  https://arxiv.org/abs/2512.23626v1  https://arxiv.org/pdf/2512.23626v1   \n",
       "7  https://arxiv.org/abs/2512.23618v1  https://arxiv.org/pdf/2512.23618v1   \n",
       "8  https://arxiv.org/abs/2512.23617v1  https://arxiv.org/pdf/2512.23617v1   \n",
       "9  https://arxiv.org/abs/2512.23611v1  https://arxiv.org/pdf/2512.23611v1   \n",
       "\n",
       "                            categories_str  \\\n",
       "0                      cs.LG, cs.CL, cs.HC   \n",
       "1                                    cs.RO   \n",
       "2                  stat.ML, cs.LG, econ.EM   \n",
       "3                             cs.MM, cs.NI   \n",
       "4                                    cs.RO   \n",
       "5                             cs.RO, cs.CV   \n",
       "6                             cs.AI, cs.LG   \n",
       "7                             cs.GT, cs.CE   \n",
       "8  cs.LG, cs.AI, math.ST, stat.ME, stat.ML   \n",
       "9                                    cs.CL   \n",
       "\n",
       "                                  categories     source  \n",
       "0                      [cs.LG, cs.CL, cs.HC]  arxiv_api  \n",
       "1                                    [cs.RO]  arxiv_api  \n",
       "2                  [stat.ML, cs.LG, econ.EM]  arxiv_api  \n",
       "3                             [cs.MM, cs.NI]  arxiv_api  \n",
       "4                                    [cs.RO]  arxiv_api  \n",
       "5                             [cs.RO, cs.CV]  arxiv_api  \n",
       "6                             [cs.AI, cs.LG]  arxiv_api  \n",
       "7                             [cs.GT, cs.CE]  arxiv_api  \n",
       "8  [cs.LG, cs.AI, math.ST, stat.ME, stat.ML]  arxiv_api  \n",
       "9                                    [cs.CL]  arxiv_api  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ============================================================\n",
    "# 3. Normalization\n",
    "# ============================================================\n",
    "#\n",
    "# This section converts raw `entries` (list[dict]) into a normalized DataFrame\n",
    "# with stable, downstream-friendly columns.\n",
    "#\n",
    "# Goals:\n",
    "#   - Ensure consistent schema\n",
    "#   - Create a few convenience fields (e.g., authors_str)\n",
    "#   - Basic text cleanup (whitespace)\n",
    "#   - Prepare for dedup/filter/output steps\n",
    "#\n",
    "# Output:\n",
    "#   - df: normalized pandas.DataFrame\n",
    "\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "\n",
    "# ------------------------\n",
    "# Helpers\n",
    "# ------------------------\n",
    "def _clean_text(s: str) -> str:\n",
    "    if s is None:\n",
    "        return \"\"\n",
    "    s = str(s)\n",
    "    s = s.replace(\"\\n\", \" \").replace(\"\\r\", \" \").strip()\n",
    "    s = re.sub(r\"\\s+\", \" \", s)\n",
    "    return s\n",
    "\n",
    "\n",
    "def normalize_entries_to_df(entries: list[dict]) -> pd.DataFrame:\n",
    "    if entries is None:\n",
    "        entries = []\n",
    "\n",
    "    rows = []\n",
    "    for e in entries:\n",
    "        authors = e.get(\"authors\") or []\n",
    "        categories = e.get(\"categories\") or []\n",
    "\n",
    "        row = {\n",
    "            \"arxiv_id\": e.get(\"arxiv_id\"),\n",
    "            \"title\": _clean_text(e.get(\"title\", \"\")),\n",
    "            \"authors\": authors,\n",
    "            \"authors_str\": \", \".join([_clean_text(a) for a in authors if a]),\n",
    "            \"abstract\": _clean_text(e.get(\"abstract\", \"\")),\n",
    "            \"published_dt\": e.get(\"published_dt\"),\n",
    "            \"updated_dt\": e.get(\"updated_dt\"),\n",
    "            \"abs_url\": e.get(\"abs_url\"),\n",
    "            \"pdf_url\": e.get(\"pdf_url\"),\n",
    "            \"categories\": categories,\n",
    "            \"categories_str\": \", \".join([_clean_text(c) for c in categories if c]),\n",
    "            \"source\": \"arxiv_api\",\n",
    "        }\n",
    "        rows.append(row)\n",
    "\n",
    "    df = pd.DataFrame(rows)\n",
    "\n",
    "    # Enforce column order (stable schema)\n",
    "    desired_cols = [\n",
    "        \"arxiv_id\",\n",
    "        \"title\",\n",
    "        \"authors_str\",\n",
    "        \"authors\",\n",
    "        \"abstract\",\n",
    "        \"published_dt\",\n",
    "        \"updated_dt\",\n",
    "        \"abs_url\",\n",
    "        \"pdf_url\",\n",
    "        \"categories_str\",\n",
    "        \"categories\",\n",
    "        \"source\",\n",
    "    ]\n",
    "    for c in desired_cols:\n",
    "        if c not in df.columns:\n",
    "            df[c] = None\n",
    "    df = df[desired_cols]\n",
    "\n",
    "    # Normalize dtypes\n",
    "    if not df.empty:\n",
    "        df[\"published_dt\"] = pd.to_datetime(df[\"published_dt\"], utc=True, errors=\"coerce\")\n",
    "        df[\"updated_dt\"] = pd.to_datetime(df[\"updated_dt\"], utc=True, errors=\"coerce\")\n",
    "\n",
    "        # Ensure strings are strings (avoid NaN surprises downstream)\n",
    "        for c in [\"arxiv_id\", \"title\", \"authors_str\", \"abstract\", \"abs_url\", \"pdf_url\", \"categories_str\", \"source\"]:\n",
    "            df[c] = df[c].fillna(\"\").astype(str).map(_clean_text)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "# ------------------------\n",
    "# Run Normalization\n",
    "# ------------------------\n",
    "if \"entries\" not in globals():\n",
    "    raise RuntimeError(\"entries is not defined. Run '2. Feed Retrieval' first.\")\n",
    "\n",
    "df = normalize_entries_to_df(entries)\n",
    "\n",
    "print(\"‚úÖ Normalization complete\")\n",
    "print(\"  rows   :\", len(df))\n",
    "print(\"  columns:\", list(df.columns))\n",
    "\n",
    "# Quick peek\n",
    "display(df.head(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4a65637d-fff7-4539-a37c-5b48c13f6573",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Deduplication & Filtering complete\n",
      "  input rows (normalized) : 30\n",
      "  after sanity filters    : 30\n",
      "  new rows (this run)     : 0\n",
      "  seen_ids file           : /Users/yuetoya/Desktop/researchOS100-private/data/cache/arxiv_seen_ids.json\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>arxiv_id</th>\n",
       "      <th>title</th>\n",
       "      <th>authors_str</th>\n",
       "      <th>authors</th>\n",
       "      <th>abstract</th>\n",
       "      <th>published_dt</th>\n",
       "      <th>updated_dt</th>\n",
       "      <th>abs_url</th>\n",
       "      <th>pdf_url</th>\n",
       "      <th>categories_str</th>\n",
       "      <th>categories</th>\n",
       "      <th>source</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [arxiv_id, title, authors_str, authors, abstract, published_dt, updated_dt, abs_url, pdf_url, categories_str, categories, source]\n",
       "Index: []"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ============================================================\n",
    "# 4. Deduplication & Filtering\n",
    "# ============================================================\n",
    "#\n",
    "# This section removes already-seen papers and applies lightweight filters.\n",
    "#\n",
    "# Dedup strategy:\n",
    "#   - Primary key: arxiv_id\n",
    "#   - Maintain an \"append-only\" history index file under CACHE_DIR\n",
    "#   - On each run: load seen_ids -> drop -> update seen_ids\n",
    "#\n",
    "# Filtering (optional):\n",
    "#   - Drop rows with empty title / arxiv_id\n",
    "#   - (Optional) keyword include/exclude rules (lightweight; keep simple here)\n",
    "#\n",
    "# Output:\n",
    "#   - df_new: DataFrame containing only \"new\" entries for this run\n",
    "\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# ------------------------\n",
    "# Configurable knobs (optional)\n",
    "# ------------------------\n",
    "# Lightweight include/exclude keyword rules (case-insensitive).\n",
    "# Leave empty lists to disable.\n",
    "INCLUDE_KEYWORDS = []   # e.g., [\"startup\", \"venture\", \"innovation\"]\n",
    "EXCLUDE_KEYWORDS = []   # e.g., [\"survey\", \"benchmark\"]\n",
    "\n",
    "SEEN_IDS_PATH = os.path.join(CACHE_DIR, \"arxiv_seen_ids.json\")\n",
    "\n",
    "\n",
    "# ------------------------\n",
    "# Helpers\n",
    "# ------------------------\n",
    "def load_seen_ids(path: str) -> set[str]:\n",
    "    if not os.path.exists(path):\n",
    "        return set()\n",
    "    try:\n",
    "        with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "            data = json.load(f)\n",
    "        if isinstance(data, list):\n",
    "            return set([str(x) for x in data])\n",
    "        if isinstance(data, dict) and \"seen_ids\" in data:\n",
    "            return set([str(x) for x in data[\"seen_ids\"]])\n",
    "    except Exception:\n",
    "        # If corrupted, fail softly and rebuild from scratch\n",
    "        return set()\n",
    "    return set()\n",
    "\n",
    "\n",
    "def save_seen_ids(path: str, seen_ids: set[str]) -> None:\n",
    "    tmp_path = path + \".tmp\"\n",
    "    payload = {\n",
    "        \"seen_ids\": sorted(list(seen_ids)),\n",
    "        \"updated_at_utc\": datetime.now(timezone.utc).isoformat(),\n",
    "    }\n",
    "    with open(tmp_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(payload, f, ensure_ascii=False, indent=2)\n",
    "    os.replace(tmp_path, path)\n",
    "\n",
    "\n",
    "def apply_keyword_filters(df: pd.DataFrame,\n",
    "                          include_keywords: list[str],\n",
    "                          exclude_keywords: list[str]) -> pd.DataFrame:\n",
    "    if df.empty:\n",
    "        return df\n",
    "\n",
    "    text = (df[\"title\"].fillna(\"\") + \" \" + df[\"abstract\"].fillna(\"\")).str.lower()\n",
    "\n",
    "    # Include: keep rows that match ANY include keyword\n",
    "    if include_keywords:\n",
    "        inc = pd.Series(False, index=df.index)\n",
    "        for kw in include_keywords:\n",
    "            kw = kw.strip().lower()\n",
    "            if kw:\n",
    "                inc = inc | text.str.contains(kw, regex=False)\n",
    "        df = df[inc].copy()\n",
    "\n",
    "    # Exclude: drop rows that match ANY exclude keyword\n",
    "    if exclude_keywords:\n",
    "        exc = pd.Series(False, index=df.index)\n",
    "        for kw in exclude_keywords:\n",
    "            kw = kw.strip().lower()\n",
    "            if kw:\n",
    "                exc = exc | text.str.contains(kw, regex=False)\n",
    "        df = df[~exc].copy()\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "# ------------------------\n",
    "# Run Dedup & Filtering\n",
    "# ------------------------\n",
    "if \"df\" not in globals():\n",
    "    raise RuntimeError(\"df is not defined. Run '3. Normalization' first.\")\n",
    "\n",
    "df_work = df.copy()\n",
    "\n",
    "# Basic sanity filter\n",
    "df_work = df_work[df_work[\"arxiv_id\"].str.len() > 0].copy()\n",
    "df_work = df_work[df_work[\"title\"].str.len() > 0].copy()\n",
    "\n",
    "# Load seen ids\n",
    "seen_ids = load_seen_ids(SEEN_IDS_PATH)\n",
    "\n",
    "before = len(df_work)\n",
    "\n",
    "# Dedup within current batch (safety)\n",
    "df_work = df_work.drop_duplicates(subset=[\"arxiv_id\"], keep=\"first\").copy()\n",
    "\n",
    "# Dedup vs history\n",
    "if getattr(CFG, \"enable_dedup\", True):\n",
    "    is_new = ~df_work[\"arxiv_id\"].isin(seen_ids)\n",
    "    df_new = df_work[is_new].copy()\n",
    "else:\n",
    "    df_new = df_work.copy()\n",
    "\n",
    "# Optional keyword filtering\n",
    "df_new = apply_keyword_filters(df_new, INCLUDE_KEYWORDS, EXCLUDE_KEYWORDS)\n",
    "\n",
    "after = len(df_new)\n",
    "\n",
    "# Update seen ids with ALL ids we encountered this run (not only df_new),\n",
    "# so reruns won't re-emit the same papers.\n",
    "seen_ids_updated = seen_ids | set(df_work[\"arxiv_id\"].tolist())\n",
    "save_seen_ids(SEEN_IDS_PATH, seen_ids_updated)\n",
    "\n",
    "print(\"‚úÖ Deduplication & Filtering complete\")\n",
    "print(f\"  input rows (normalized) : {len(df)}\")\n",
    "print(f\"  after sanity filters    : {before}\")\n",
    "print(f\"  new rows (this run)     : {after}\")\n",
    "print(f\"  seen_ids file           : {SEEN_IDS_PATH}\")\n",
    "\n",
    "display(df_new.head(20))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "137a7a67-e8e8-4e0e-a0c5-2b4b431b1446",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ö†Ô∏è No new entries to save. Skipping output.\n",
      "\n",
      "üìå Run Summary\n",
      "  query_text     : venture capital OR startup OR innovation policy\n",
      "  max_results    : 30\n",
      "  lookback_days  : 14\n",
      "  retrieved_rows : 30\n",
      "  new_rows       : 0\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# 5. Output\n",
    "# ============================================================\n",
    "#\n",
    "# Persist deduplicated results as CSV.\n",
    "# No optional dependencies required.\n",
    "\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "\n",
    "# ------------------------\n",
    "# Run Output\n",
    "# ------------------------\n",
    "if \"df_new\" not in globals():\n",
    "    raise RuntimeError(\"df_new is not defined. Run '4. Deduplication & Filtering' first.\")\n",
    "\n",
    "if \"OUTPUT_PATH\" not in globals():\n",
    "    raise RuntimeError(\"OUTPUT_PATH is not defined. Run '1. Configuration' first.\")\n",
    "\n",
    "if df_new.empty:\n",
    "    print(\"‚ö†Ô∏è No new entries to save. Skipping output.\")\n",
    "else:\n",
    "    df_new.to_csv(OUTPUT_PATH, index=False)\n",
    "\n",
    "    latest_path = os.path.join(\n",
    "        ARXIV_DIR,\n",
    "        f\"{CFG.output_basename}_latest.csv\"\n",
    "    )\n",
    "\n",
    "    # overwrite latest pointer (copy, not symlink for portability)\n",
    "    shutil.copy2(OUTPUT_PATH, latest_path)\n",
    "\n",
    "    print(\"‚úÖ Output saved\")\n",
    "    print(f\"  rows written : {len(df_new)}\")\n",
    "    print(f\"  output_path  : {OUTPUT_PATH}\")\n",
    "    print(f\"  latest_path  : {latest_path}\")\n",
    "\n",
    "\n",
    "# ------------------------\n",
    "# Run Summary\n",
    "# ------------------------\n",
    "print(\"\\nüìå Run Summary\")\n",
    "print(\"  query_text     :\", CFG.query_text)\n",
    "print(\"  max_results    :\", CFG.max_results)\n",
    "print(\"  lookback_days  :\", CFG.lookback_days)\n",
    "print(\"  retrieved_rows :\", len(df))\n",
    "print(\"  new_rows       :\", len(df_new))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f86f07d-75ce-49bf-b482-f0d2320a9b51",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
