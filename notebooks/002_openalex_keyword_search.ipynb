{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f3ff10e0-830a-4e44-bd82-20b7ec9a8175",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 002_OpenAlex Keyword Search\n",
    "# ============================================================\n",
    "#\n",
    "# ⚠️ Important Notes (Please Read First)\n",
    "# ------------------------------------\n",
    "# - This notebook uses the OpenAlex API to retrieve academic works based on\n",
    "#   keyword-based search queries (OpenAlex /works).\n",
    "# - OpenAlex API behavior (rate limits, pagination, available fields) may change\n",
    "#   over time. When retrieving a large number of results, always proceed\n",
    "#   incrementally and save intermediate outputs (this notebook can optionally\n",
    "#   persist raw responses as JSONL).\n",
    "# - The OpenAlex \"search\" parameter is not a strict Boolean query language.\n",
    "#   Operators like OR can be helpful, but expect both noise and omissions.\n",
    "#   Treat the retrieved results as a *first-pass candidate set*, not a final corpus.\n",
    "# - The metadata provided by OpenAlex is not guaranteed to be complete or error-free.\n",
    "#   For validation, critical analysis, or citation, always refer to the original\n",
    "#   publisher pages, DOIs, or PDFs.\n",
    "# - Do not hard-code sensitive information (e.g., API keys) in this notebook.\n",
    "#   This notebook is designed to run without an API key.\n",
    "# - (Optional) For polite API usage, consider setting a mailto parameter.\n",
    "#\n",
    "# Overview\n",
    "# --------\n",
    "# This notebook performs keyword-based retrieval of academic works using the\n",
    "# OpenAlex API. The goal is to collect a structured dataset of papers matching\n",
    "# a given query, optionally filtered by publication year and open-access status.\n",
    "# The retrieved results are normalized into a tabular format (DataFrame / CSV)\n",
    "# so that they can be reused in downstream steps such as deduplication,\n",
    "# ranking, topic modeling, or LLM-based synthesis.\n",
    "#\n",
    "# Structure\n",
    "# ---------\n",
    "# 1) Parameters / Query Design\n",
    "#    - Define search keywords, time range, open-access filters,\n",
    "#      pagination size, and maximum number of results\n",
    "#    - (Optional) Configure parameters via ipywidgets UI\n",
    "# 2) API Core Functions (Search & Pagination)\n",
    "#    - Construct OpenAlex /works requests and retrieve results\n",
    "#      using cursor-based pagination (and basic retry/backoff)\n",
    "# 3) Parsing & Normalization\n",
    "#    - Extract and standardize key metadata fields\n",
    "#      (title, authors, year, venue, DOI, citation count, OA status, etc.)\n",
    "# 4) Output & Export\n",
    "#    - Export normalized CSV (+ metadata JSON, optional dedup view)\n",
    "#\n",
    "# Notes\n",
    "# -----\n",
    "# - For reproducibility, log the exact query text, filters, and total number\n",
    "#   of retrieved records.\n",
    "# - When working with large queries, persist partial results to avoid data loss\n",
    "#   and enable safe retries.\n",
    "# - To improve search precision:\n",
    "#     (1) expand queries with synonyms using OR,\n",
    "#     (2) explicitly exclude noisy terms,\n",
    "#     (3) combine keyword search with field or year filters,\n",
    "#     (4) start small and scale up iteratively.\n",
    "# - Deduplication (e.g., by DOI or fuzzy title matching) and quality filtering\n",
    "#   are intentionally handled in downstream steps.\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fd21686f-ef83-4cb8-aca7-252a3310bfd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 1) Parameters / Query Design\n",
    "# ============================================================\n",
    "#\n",
    "# This section defines all parameters used for the OpenAlex\n",
    "# keyword-based search. Keeping them centralized here improves\n",
    "# readability, reproducibility, and ease of iteration.\n",
    "#\n",
    "# Recommended workflow:\n",
    "# - Start with a small max_results to validate query quality\n",
    "# - Inspect sample outputs\n",
    "# - Gradually expand the retrieval scope\n",
    "#\n",
    "\n",
    "# ------------------------\n",
    "# Core keyword query\n",
    "# ------------------------\n",
    "# OpenAlex uses a simple keyword-based full-text search over titles,\n",
    "# abstracts, and other indexed fields.\n",
    "# Complex logic (AND / OR) should be expressed explicitly in the string.\n",
    "query_text = \"venture capital OR startup OR innovation policy\"\n",
    "\n",
    "\n",
    "# ------------------------\n",
    "# Time range filter\n",
    "# ------------------------\n",
    "# Publication year range (inclusive).\n",
    "# Set to None to disable year-based filtering.\n",
    "from_year = 2010\n",
    "to_year = 2025\n",
    "\n",
    "\n",
    "# ------------------------\n",
    "# Open Access filter\n",
    "# ------------------------\n",
    "# If True, restrict results to Open Access works only.\n",
    "# This is useful when downstream steps require PDF access.\n",
    "open_access_only = False\n",
    "\n",
    "\n",
    "# ------------------------\n",
    "# Pagination & volume control\n",
    "# ------------------------\n",
    "# Number of records per API call (OpenAlex max is typically 200).\n",
    "per_page = 200\n",
    "\n",
    "# Maximum number of records to retrieve in total.\n",
    "# Use a small number for testing, then increase gradually.\n",
    "max_results = 1000\n",
    "\n",
    "\n",
    "# ------------------------\n",
    "# Sorting strategy\n",
    "# ------------------------\n",
    "# Common options include:\n",
    "# - relevance_score (default search relevance)\n",
    "# - publication_year\n",
    "# - cited_by_count\n",
    "sort_by = \"relevance_score\"\n",
    "\n",
    "\n",
    "# ------------------------\n",
    "# Output & logging\n",
    "# ------------------------\n",
    "# Directory where intermediate and final outputs will be saved.\n",
    "output_dir = \"./data/openalex\"\n",
    "\n",
    "# Whether to print progress logs during retrieval.\n",
    "verbose = True\n",
    "\n",
    "\n",
    "# ------------------------\n",
    "# Reproducibility note\n",
    "# ------------------------\n",
    "# All parameters defined in this section should be logged together\n",
    "# with the output dataset so that the exact retrieval conditions\n",
    "# can be reconstructed later.\n",
    "#\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "076a3e92-ec73-4381-865e-397b6b9f18fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 2) API Core Functions (Search & Pagination)\n",
    "# ============================================================\n",
    "#\n",
    "# This section implements:\n",
    "# - request construction for OpenAlex /works\n",
    "# - cursor-based pagination (recommended for large result sets)\n",
    "# - basic retry / backoff for transient errors or rate limits\n",
    "# - optional intermediate saving (JSONL) for safe long runs\n",
    "#\n",
    "\n",
    "import os\n",
    "import time\n",
    "import json\n",
    "import math\n",
    "import random\n",
    "from typing import Dict, Any, List, Optional, Tuple\n",
    "\n",
    "import requests\n",
    "\n",
    "\n",
    "# ------------------------\n",
    "# OpenAlex endpoint\n",
    "# ------------------------\n",
    "OPENALEX_BASE_URL = \"https://api.openalex.org\"\n",
    "WORKS_ENDPOINT = f\"{OPENALEX_BASE_URL}/works\"\n",
    "\n",
    "\n",
    "# ------------------------\n",
    "# Utilities\n",
    "# ------------------------\n",
    "def _ensure_dir(path: str) -> None:\n",
    "    os.makedirs(path, exist_ok=True)\n",
    "\n",
    "\n",
    "def _build_openalex_filters(\n",
    "    from_year: Optional[int],\n",
    "    to_year: Optional[int],\n",
    "    open_access_only: bool\n",
    ") -> Optional[str]:\n",
    "    \"\"\"\n",
    "    Build OpenAlex filter string.\n",
    "    We use publication_date rather than publication_year to be explicit.\n",
    "    \"\"\"\n",
    "    filters = []\n",
    "\n",
    "    if from_year is not None:\n",
    "        filters.append(f\"from_publication_date:{from_year}-01-01\")\n",
    "    if to_year is not None:\n",
    "        filters.append(f\"to_publication_date:{to_year}-12-31\")\n",
    "    if open_access_only:\n",
    "        filters.append(\"open_access.is_oa:true\")\n",
    "\n",
    "    return \",\".join(filters) if filters else None\n",
    "\n",
    "\n",
    "def _build_sort_params(sort_by: str) -> Dict[str, str]:\n",
    "    \"\"\"\n",
    "    OpenAlex supports 'sort' in the form 'field:direction'.\n",
    "    If using relevance, we simply omit sort to let OpenAlex rank by relevance.\n",
    "    \"\"\"\n",
    "    sort_by = (sort_by or \"\").strip().lower()\n",
    "\n",
    "    if sort_by in [\"relevance\", \"relevance_score\", \"\"]:\n",
    "        return {}  # OpenAlex default relevance ordering for search queries\n",
    "\n",
    "    # Common examples:\n",
    "    # - \"cited_by_count\" -> cited_by_count:desc\n",
    "    # - \"publication_year\" -> publication_year:desc\n",
    "    # You can customize direction as needed.\n",
    "    if sort_by in [\"cited_by_count\", \"publication_year\", \"publication_date\"]:\n",
    "        return {\"sort\": f\"{sort_by}:desc\"}\n",
    "\n",
    "    # Fall back to user-provided value if they already passed \"field:dir\"\n",
    "    if \":\" in sort_by:\n",
    "        return {\"sort\": sort_by}\n",
    "\n",
    "    # Default to descending if unknown field is given\n",
    "    return {\"sort\": f\"{sort_by}:desc\"}\n",
    "\n",
    "\n",
    "def _request_with_retry(\n",
    "    url: str,\n",
    "    params: Dict[str, Any],\n",
    "    headers: Optional[Dict[str, str]] = None,\n",
    "    max_retries: int = 6,\n",
    "    timeout: int = 60,\n",
    "    verbose: bool = True\n",
    ") -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Simple retry with exponential backoff + jitter.\n",
    "    Handles transient network errors and basic rate limiting.\n",
    "    \"\"\"\n",
    "    headers = headers or {}\n",
    "    last_err = None\n",
    "\n",
    "    for attempt in range(1, max_retries + 1):\n",
    "        try:\n",
    "            r = requests.get(url, params=params, headers=headers, timeout=timeout)\n",
    "\n",
    "            # Rate limit / transient server errors\n",
    "            if r.status_code in [429, 500, 502, 503, 504]:\n",
    "                wait = min(60, (2 ** (attempt - 1)) + random.random())\n",
    "                if verbose:\n",
    "                    print(f\"⚠️ HTTP {r.status_code}. Retry {attempt}/{max_retries} in {wait:.1f}s\")\n",
    "                time.sleep(wait)\n",
    "                continue\n",
    "\n",
    "            r.raise_for_status()\n",
    "            return r.json()\n",
    "\n",
    "        except Exception as e:\n",
    "            last_err = e\n",
    "            wait = min(60, (2 ** (attempt - 1)) + random.random())\n",
    "            if verbose:\n",
    "                print(f\"⚠️ Request error ({type(e).__name__}). Retry {attempt}/{max_retries} in {wait:.1f}s\")\n",
    "            time.sleep(wait)\n",
    "\n",
    "    raise RuntimeError(f\"Request failed after {max_retries} retries. Last error: {last_err}\")\n",
    "\n",
    "\n",
    "# ------------------------\n",
    "# Core search function\n",
    "# ------------------------\n",
    "def openalex_keyword_search_works(\n",
    "    query_text: str,\n",
    "    from_year: Optional[int] = None,\n",
    "    to_year: Optional[int] = None,\n",
    "    open_access_only: bool = False,\n",
    "    per_page: int = 200,\n",
    "    max_results: int = 1000,\n",
    "    sort_by: str = \"relevance_score\",\n",
    "    output_dir: str = \"./data/openalex\",\n",
    "    save_jsonl: bool = True,\n",
    "    polite_delay_sec: float = 0.1,\n",
    "    mailto: Optional[str] = None,   # Optional: OpenAlex recommends mailto for polite usage\n",
    "    verbose: bool = True\n",
    ") -> Tuple[List[Dict[str, Any]], Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Retrieve works from OpenAlex using cursor-based pagination.\n",
    "\n",
    "    Returns:\n",
    "      - results: list of raw OpenAlex 'works' items (JSON dicts)\n",
    "      - meta:    retrieval metadata (query, filters, counts, etc.)\n",
    "    \"\"\"\n",
    "    if not query_text or not query_text.strip():\n",
    "        raise ValueError(\"query_text must be a non-empty string.\")\n",
    "\n",
    "    per_page = int(per_page)\n",
    "    if per_page <= 0:\n",
    "        raise ValueError(\"per_page must be > 0.\")\n",
    "    if per_page > 200:\n",
    "        if verbose:\n",
    "            print(\"ℹ️ per_page > 200 detected; capping to 200 (OpenAlex typical maximum).\")\n",
    "        per_page = 200\n",
    "\n",
    "    max_results = int(max_results)\n",
    "    if max_results <= 0:\n",
    "        raise ValueError(\"max_results must be > 0.\")\n",
    "\n",
    "    _ensure_dir(output_dir)\n",
    "\n",
    "    filters = _build_openalex_filters(from_year, to_year, open_access_only)\n",
    "    sort_params = _build_sort_params(sort_by)\n",
    "\n",
    "    # Save path for intermediate results\n",
    "    jsonl_path = os.path.join(output_dir, \"openalex_works_raw.jsonl\")\n",
    "\n",
    "    # Cursor pagination\n",
    "    cursor = \"*\"\n",
    "    collected: List[Dict[str, Any]] = []\n",
    "    total_available = None\n",
    "\n",
    "    # Base params\n",
    "    params_base: Dict[str, Any] = {\n",
    "        \"search\": query_text,\n",
    "        \"per_page\": per_page,\n",
    "        \"cursor\": cursor,\n",
    "    }\n",
    "    if filters:\n",
    "        params_base[\"filter\"] = filters\n",
    "    if mailto:\n",
    "        params_base[\"mailto\"] = mailto\n",
    "    params_base.update(sort_params)\n",
    "\n",
    "    # Retrieval meta\n",
    "    meta: Dict[str, Any] = {\n",
    "        \"query_text\": query_text,\n",
    "        \"from_year\": from_year,\n",
    "        \"to_year\": to_year,\n",
    "        \"open_access_only\": open_access_only,\n",
    "        \"per_page\": per_page,\n",
    "        \"max_results\": max_results,\n",
    "        \"sort_by\": sort_by,\n",
    "        \"filter_string\": filters,\n",
    "        \"output_dir\": output_dir,\n",
    "        \"jsonl_path\": jsonl_path if save_jsonl else None,\n",
    "        \"retrieved\": 0,\n",
    "        \"total_available_reported_by_api\": None,\n",
    "    }\n",
    "\n",
    "    if verbose:\n",
    "        print(\"=== OpenAlex Keyword Search: Start ===\")\n",
    "        print(f\"- query_text        : {query_text}\")\n",
    "        print(f\"- filter            : {filters if filters else '(none)'}\")\n",
    "        print(f\"- per_page          : {per_page}\")\n",
    "        print(f\"- max_results       : {max_results}\")\n",
    "        print(f\"- sort              : {sort_params.get('sort', '(default relevance)')}\")\n",
    "        if save_jsonl:\n",
    "            print(f\"- saving JSONL to   : {jsonl_path}\")\n",
    "\n",
    "    # If saving, truncate existing file for a clean run\n",
    "    if save_jsonl:\n",
    "        with open(jsonl_path, \"w\", encoding=\"utf-8\") as f:\n",
    "            pass\n",
    "\n",
    "    page_idx = 0\n",
    "    while len(collected) < max_results:\n",
    "        page_idx += 1\n",
    "\n",
    "        # Update cursor param\n",
    "        params = dict(params_base)\n",
    "        params[\"cursor\"] = cursor\n",
    "\n",
    "        data = _request_with_retry(\n",
    "            WORKS_ENDPOINT,\n",
    "            params=params,\n",
    "            headers=None,\n",
    "            verbose=verbose\n",
    "        )\n",
    "\n",
    "        # API meta\n",
    "        api_meta = data.get(\"meta\", {}) or {}\n",
    "        next_cursor = api_meta.get(\"next_cursor\")\n",
    "        count = api_meta.get(\"count\")\n",
    "\n",
    "        if total_available is None and isinstance(count, int):\n",
    "            total_available = count\n",
    "            meta[\"total_available_reported_by_api\"] = total_available\n",
    "\n",
    "        results = data.get(\"results\", []) or []\n",
    "        if not results:\n",
    "            if verbose:\n",
    "                print(f\"ℹ️ No more results returned (page {page_idx}). Stopping.\")\n",
    "            break\n",
    "\n",
    "        # Take only what we still need\n",
    "        remaining = max_results - len(collected)\n",
    "        batch = results[:remaining]\n",
    "\n",
    "        collected.extend(batch)\n",
    "        meta[\"retrieved\"] = len(collected)\n",
    "\n",
    "        # Save intermediate batch (raw)\n",
    "        if save_jsonl:\n",
    "            with open(jsonl_path, \"a\", encoding=\"utf-8\") as f:\n",
    "                for item in batch:\n",
    "                    f.write(json.dumps(item, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "        if verbose:\n",
    "            total_str = f\"{total_available}\" if total_available is not None else \"?\"\n",
    "            print(f\"Page {page_idx:03d}: +{len(batch):4d}  (total={len(collected)}/{total_str})\")\n",
    "\n",
    "        # Stop if no next cursor\n",
    "        if not next_cursor:\n",
    "            if verbose:\n",
    "                print(\"ℹ️ next_cursor not found. Stopping.\")\n",
    "            break\n",
    "\n",
    "        cursor = next_cursor\n",
    "\n",
    "        # Polite delay to reduce load\n",
    "        if polite_delay_sec and polite_delay_sec > 0:\n",
    "            time.sleep(polite_delay_sec)\n",
    "\n",
    "    if verbose:\n",
    "        print(\"=== OpenAlex Keyword Search: Done ===\")\n",
    "        print(f\"Retrieved: {len(collected)} works\")\n",
    "\n",
    "    return collected, meta\n",
    "\n",
    "\n",
    "# ------------------------\n",
    "# Example execution (optional)\n",
    "# ------------------------\n",
    "# raw_works, retrieval_meta = openalex_keyword_search_works(\n",
    "#     query_text=query_text,\n",
    "#     from_year=from_year,\n",
    "#     to_year=to_year,\n",
    "#     open_access_only=open_access_only,\n",
    "#     per_page=per_page,\n",
    "#     max_results=max_results,\n",
    "#     sort_by=sort_by,\n",
    "#     output_dir=output_dir,\n",
    "#     save_jsonl=True,\n",
    "#     polite_delay_sec=0.1,\n",
    "#     mailto=None,   # e.g., \"your_email@domain.com\"\n",
    "#     verbose=verbose\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6010ef19-f800-4baa-b005-0d461c2f8cc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 3) Parsing & Normalization\n",
    "# ============================================================\n",
    "#\n",
    "# This section converts raw OpenAlex \"works\" JSON objects into a\n",
    "# flat, analysis-friendly tabular format (pandas DataFrame).\n",
    "#\n",
    "# Design goals:\n",
    "# - Keep a stable schema for downstream steps\n",
    "# - Preserve important identifiers (OpenAlex ID, DOI, PMID/PMCID if available)\n",
    "# - Extract author / institution / concept info in a lightweight way\n",
    "# - Be robust to missing fields\n",
    "#\n",
    "\n",
    "from typing import Any, Dict, List, Optional\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# ------------------------\n",
    "# Helpers\n",
    "# ------------------------\n",
    "def _safe_get(d: Any, keys: List[str], default=None):\n",
    "    \"\"\"\n",
    "    Safely access nested dictionaries.\n",
    "    keys: list like [\"primary_location\", \"source\", \"display_name\"]\n",
    "    \"\"\"\n",
    "    cur = d\n",
    "    for k in keys:\n",
    "        if cur is None:\n",
    "            return default\n",
    "        if isinstance(cur, dict) and k in cur:\n",
    "            cur = cur[k]\n",
    "        else:\n",
    "            return default\n",
    "    return cur\n",
    "\n",
    "\n",
    "def _normalize_doi(doi: Optional[str]) -> Optional[str]:\n",
    "    \"\"\"\n",
    "    Normalize DOI to a canonical lowercase '10.xxxx/...' format when possible.\n",
    "    OpenAlex often returns DOI as 'https://doi.org/...'\n",
    "    \"\"\"\n",
    "    if not doi:\n",
    "        return None\n",
    "    doi = doi.strip()\n",
    "    doi = doi.replace(\"https://doi.org/\", \"\").replace(\"http://doi.org/\", \"\")\n",
    "    doi = doi.replace(\"https://dx.doi.org/\", \"\").replace(\"http://dx.doi.org/\", \"\")\n",
    "    doi = doi.strip()\n",
    "    return doi.lower() if doi else None\n",
    "\n",
    "\n",
    "def _join_nonempty(xs: List[str], sep: str = \"; \") -> str:\n",
    "    xs2 = [x.strip() for x in xs if isinstance(x, str) and x.strip()]\n",
    "    return sep.join(xs2)\n",
    "\n",
    "\n",
    "def _extract_authors(work: Dict[str, Any], max_authors: int = 25) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Extract author-level information from authorships.\n",
    "    Returns:\n",
    "      - authors: joined author names\n",
    "      - author_ids: joined OpenAlex author IDs\n",
    "      - institutions: joined institution names (unique)\n",
    "      - countries: joined institution country codes (unique)\n",
    "    \"\"\"\n",
    "    authorships = work.get(\"authorships\", []) or []\n",
    "\n",
    "    author_names = []\n",
    "    author_ids = []\n",
    "    inst_names = set()\n",
    "    inst_countries = set()\n",
    "\n",
    "    for a in authorships[:max_authors]:\n",
    "        author = a.get(\"author\") or {}\n",
    "        name = author.get(\"display_name\")\n",
    "        if name:\n",
    "            author_names.append(name)\n",
    "\n",
    "        aid = author.get(\"id\")\n",
    "        if aid:\n",
    "            author_ids.append(aid)\n",
    "\n",
    "        # institutions (may be multiple per authorship)\n",
    "        for inst in (a.get(\"institutions\", []) or []):\n",
    "            iname = inst.get(\"display_name\")\n",
    "            if iname:\n",
    "                inst_names.add(iname)\n",
    "\n",
    "            ccode = inst.get(\"country_code\")\n",
    "            if ccode:\n",
    "                inst_countries.add(ccode)\n",
    "\n",
    "    return {\n",
    "        \"authors\": _join_nonempty(author_names),\n",
    "        \"author_ids\": _join_nonempty(author_ids),\n",
    "        \"institutions\": _join_nonempty(sorted(inst_names)),\n",
    "        \"institution_country_codes\": _join_nonempty(sorted(inst_countries)),\n",
    "        \"n_authors\": len(authorships),\n",
    "    }\n",
    "\n",
    "\n",
    "def _extract_concepts(work: Dict[str, Any], top_k: int = 8) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Extract top concepts (name + score).\n",
    "    \"\"\"\n",
    "    concepts = work.get(\"concepts\", []) or []\n",
    "    # Sort by score desc (OpenAlex provides score)\n",
    "    concepts_sorted = sorted(\n",
    "        concepts,\n",
    "        key=lambda x: (x.get(\"score\") is not None, x.get(\"score\")),\n",
    "        reverse=True\n",
    "    )\n",
    "\n",
    "    names = []\n",
    "    ids = []\n",
    "    scored = []\n",
    "\n",
    "    for c in concepts_sorted[:top_k]:\n",
    "        cname = c.get(\"display_name\")\n",
    "        cid = c.get(\"id\")\n",
    "        score = c.get(\"score\")\n",
    "\n",
    "        if cname:\n",
    "            names.append(cname)\n",
    "        if cid:\n",
    "            ids.append(cid)\n",
    "        if cname and score is not None:\n",
    "            scored.append(f\"{cname} ({score:.3f})\")\n",
    "\n",
    "    return {\n",
    "        \"concepts_top\": _join_nonempty(names),\n",
    "        \"concept_ids_top\": _join_nonempty(ids),\n",
    "        \"concepts_scored_top\": _join_nonempty(scored),\n",
    "    }\n",
    "\n",
    "\n",
    "def _extract_primary_venue(work: Dict[str, Any]) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Extract primary venue / source info.\n",
    "    Prefer primary_location.source if present, otherwise fall back to host_venue.\n",
    "    \"\"\"\n",
    "    source_name = _safe_get(work, [\"primary_location\", \"source\", \"display_name\"])\n",
    "    source_id = _safe_get(work, [\"primary_location\", \"source\", \"id\"])\n",
    "    source_type = _safe_get(work, [\"primary_location\", \"source\", \"type\"])\n",
    "    source_issn = _safe_get(work, [\"primary_location\", \"source\", \"issn_l\"])\n",
    "\n",
    "    # Fallback: host_venue (often populated)\n",
    "    if not source_name:\n",
    "        source_name = _safe_get(work, [\"host_venue\", \"display_name\"])\n",
    "    if not source_id:\n",
    "        source_id = _safe_get(work, [\"host_venue\", \"id\"])\n",
    "    if not source_issn:\n",
    "        source_issn = _safe_get(work, [\"host_venue\", \"issn_l\"])\n",
    "\n",
    "    return {\n",
    "        \"venue_name\": source_name,\n",
    "        \"venue_id\": source_id,\n",
    "        \"venue_type\": source_type,\n",
    "        \"venue_issn_l\": source_issn,\n",
    "    }\n",
    "\n",
    "\n",
    "def _extract_open_access(work: Dict[str, Any]) -> Dict[str, Any]:\n",
    "    oa = work.get(\"open_access\") or {}\n",
    "    return {\n",
    "        \"is_oa\": oa.get(\"is_oa\"),\n",
    "        \"oa_status\": oa.get(\"oa_status\"),\n",
    "        \"oa_url\": oa.get(\"oa_url\"),\n",
    "    }\n",
    "\n",
    "\n",
    "def _extract_ids(work: Dict[str, Any]) -> Dict[str, Any]:\n",
    "    ids = work.get(\"ids\") or {}\n",
    "    doi = ids.get(\"doi\") or work.get(\"doi\")\n",
    "    return {\n",
    "        \"openalex_id\": work.get(\"id\"),\n",
    "        \"openalex_work_id\": work.get(\"id\"),\n",
    "        \"doi\": _normalize_doi(doi),\n",
    "        \"doi_url\": ids.get(\"doi\"),\n",
    "        \"pmid\": ids.get(\"pmid\"),\n",
    "        \"pmcid\": ids.get(\"pmcid\"),\n",
    "        \"mag\": ids.get(\"mag\"),\n",
    "    }\n",
    "\n",
    "\n",
    "def _extract_biblio(work: Dict[str, Any]) -> Dict[str, Any]:\n",
    "    b = work.get(\"biblio\") or {}\n",
    "    return {\n",
    "        \"volume\": b.get(\"volume\"),\n",
    "        \"issue\": b.get(\"issue\"),\n",
    "        \"first_page\": b.get(\"first_page\"),\n",
    "        \"last_page\": b.get(\"last_page\"),\n",
    "    }\n",
    "\n",
    "\n",
    "# ------------------------\n",
    "# Main normalization\n",
    "# ------------------------\n",
    "def normalize_openalex_works(\n",
    "    raw_works: List[Dict[str, Any]],\n",
    "    max_authors: int = 25,\n",
    "    concepts_top_k: int = 8\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Convert raw works JSON into a normalized DataFrame.\n",
    "    \"\"\"\n",
    "    rows: List[Dict[str, Any]] = []\n",
    "\n",
    "    for w in raw_works or []:\n",
    "        # Core fields\n",
    "        title = w.get(\"display_name\")\n",
    "        year = w.get(\"publication_year\")\n",
    "        pub_date = w.get(\"publication_date\")\n",
    "\n",
    "        # Abstract (OpenAlex may provide inverted index; keep raw pointer only)\n",
    "        # We'll store a boolean flag here; full reconstruction can be done later if needed.\n",
    "        abstract_inverted = w.get(\"abstract_inverted_index\")\n",
    "        has_abstract = abstract_inverted is not None\n",
    "\n",
    "        row = {}\n",
    "        row.update(_extract_ids(w))\n",
    "        row.update({\n",
    "            \"title\": title,\n",
    "            \"publication_year\": year,\n",
    "            \"publication_date\": pub_date,\n",
    "            \"type\": w.get(\"type\"),\n",
    "            \"language\": w.get(\"language\"),\n",
    "            \"has_abstract_inverted_index\": has_abstract,\n",
    "            \"cited_by_count\": w.get(\"cited_by_count\"),\n",
    "        })\n",
    "        row.update(_extract_primary_venue(w))\n",
    "        row.update(_extract_open_access(w))\n",
    "        row.update(_extract_biblio(w))\n",
    "\n",
    "        # Authorships & concepts\n",
    "        row.update(_extract_authors(w, max_authors=max_authors))\n",
    "        row.update(_extract_concepts(w, top_k=concepts_top_k))\n",
    "\n",
    "        # URLs\n",
    "        row.update({\n",
    "            \"openalex_url\": w.get(\"id\"),\n",
    "            \"work_url\": w.get(\"id\"),\n",
    "            \"landing_page_url\": _safe_get(w, [\"primary_location\", \"landing_page_url\"]),\n",
    "            \"pdf_url\": _safe_get(w, [\"primary_location\", \"pdf_url\"]),\n",
    "        })\n",
    "\n",
    "        rows.append(row)\n",
    "\n",
    "    df = pd.DataFrame(rows)\n",
    "\n",
    "    # Basic cleanup: enforce column order (stable schema)\n",
    "    preferred_cols = [\n",
    "        \"openalex_id\",\n",
    "        \"doi\",\n",
    "        \"title\",\n",
    "        \"publication_year\",\n",
    "        \"publication_date\",\n",
    "        \"type\",\n",
    "        \"language\",\n",
    "        \"venue_name\",\n",
    "        \"venue_id\",\n",
    "        \"venue_issn_l\",\n",
    "        \"cited_by_count\",\n",
    "        \"is_oa\",\n",
    "        \"oa_status\",\n",
    "        \"oa_url\",\n",
    "        \"landing_page_url\",\n",
    "        \"pdf_url\",\n",
    "        \"authors\",\n",
    "        \"n_authors\",\n",
    "        \"institutions\",\n",
    "        \"institution_country_codes\",\n",
    "        \"concepts_top\",\n",
    "        \"concepts_scored_top\",\n",
    "        \"concept_ids_top\",\n",
    "        \"has_abstract_inverted_index\",\n",
    "        \"pmid\",\n",
    "        \"pmcid\",\n",
    "        \"volume\",\n",
    "        \"issue\",\n",
    "        \"first_page\",\n",
    "        \"last_page\",\n",
    "        \"openalex_url\",\n",
    "    ]\n",
    "    existing = [c for c in preferred_cols if c in df.columns]\n",
    "    remaining = [c for c in df.columns if c not in existing]\n",
    "    df = df[existing + remaining]\n",
    "\n",
    "    # Remove exact duplicates by OpenAlex ID (if any)\n",
    "    if \"openalex_id\" in df.columns:\n",
    "        df = df.drop_duplicates(subset=[\"openalex_id\"]).reset_index(drop=True)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "# ------------------------\n",
    "# Example execution (optional)\n",
    "# ------------------------\n",
    "# df_works = normalize_openalex_works(\n",
    "#     raw_works=raw_works,\n",
    "#     max_authors=25,\n",
    "#     concepts_top_k=8\n",
    "# )\n",
    "# df_works.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "00526cc7-f052-4eb9-8cbb-d7a10b717714",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5560858bd83f4fca8d4213bf364404d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<h3>OpenAlex Keyword Search</h3>'), Text(value='venture capital OR startup OR innov…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ============================================================\n",
    "# 4) Output & Export\n",
    "# ============================================================\n",
    "#\n",
    "# This section:\n",
    "# - creates lightweight run artifacts (CSV + metadata JSON)\n",
    "# - prints basic summary stats for quick sanity checks\n",
    "# - (optional) creates a minimal dedup view by DOI / title\n",
    "#\n",
    "\n",
    "import os\n",
    "import json\n",
    "import re\n",
    "from datetime import datetime, timezone\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# ------------------------\n",
    "# Utilities\n",
    "# ------------------------\n",
    "def _utc_timestamp() -> str:\n",
    "    return datetime.now(timezone.utc).strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "\n",
    "def _ensure_dir(path: str) -> None:\n",
    "    os.makedirs(path, exist_ok=True)\n",
    "\n",
    "\n",
    "def _safe_slug(text: str, max_len: int = 80) -> str:\n",
    "    \"\"\"\n",
    "    Create a filesystem-friendly slug from query text.\n",
    "    \"\"\"\n",
    "    if not text:\n",
    "        return \"query\"\n",
    "    s = text.lower().strip()\n",
    "    s = re.sub(r\"\\s+\", \"_\", s)\n",
    "    s = re.sub(r\"[^a-z0-9_\\-]+\", \"\", s)\n",
    "    s = s.strip(\"_-\")\n",
    "    return s[:max_len] if s else \"query\"\n",
    "\n",
    "\n",
    "def _write_json(path: str, obj: dict) -> None:\n",
    "    with open(path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(obj, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "\n",
    "def _print_summary(df: pd.DataFrame) -> None:\n",
    "    n = len(df)\n",
    "    print(\"=== Summary ===\")\n",
    "    print(f\"- rows: {n}\")\n",
    "\n",
    "    if \"publication_year\" in df.columns:\n",
    "        years = df[\"publication_year\"].dropna()\n",
    "        if not years.empty:\n",
    "            print(f\"- year range: {int(years.min())} – {int(years.max())}\")\n",
    "\n",
    "    if \"is_oa\" in df.columns:\n",
    "        oa = df[\"is_oa\"].dropna()\n",
    "        if not oa.empty:\n",
    "            print(f\"- OA ratio: {oa.mean():.3f}  (True share among non-null)\")\n",
    "\n",
    "    if \"doi\" in df.columns:\n",
    "        doi_nonnull = df[\"doi\"].notna().sum()\n",
    "        print(f\"- DOI non-null: {doi_nonnull} / {n} ({doi_nonnull/n:.1%})\")\n",
    "        if doi_nonnull:\n",
    "            doi_dup = df[\"doi\"].dropna().duplicated().sum()\n",
    "            print(f\"- DOI duplicates: {doi_dup}\")\n",
    "\n",
    "    if \"title\" in df.columns:\n",
    "        title_nonnull = df[\"title\"].notna().sum()\n",
    "        print(f\"- Title non-null: {title_nonnull} / {n} ({title_nonnull/n:.1%})\")\n",
    "\n",
    "    if \"venue_name\" in df.columns:\n",
    "        top_venues = (\n",
    "            df[\"venue_name\"]\n",
    "            .dropna()\n",
    "            .value_counts()\n",
    "            .head(10)\n",
    "        )\n",
    "        if len(top_venues):\n",
    "            print(\"- Top venues (count):\")\n",
    "            for k, v in top_venues.items():\n",
    "                print(f\"  - {k}: {v}\")\n",
    "\n",
    "\n",
    "def _basic_dedup_view(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Create a minimal dedup key view (for quick inspection).\n",
    "    This does NOT remove duplicates; it just provides a key table.\n",
    "    \"\"\"\n",
    "    cols = []\n",
    "    for c in [\"openalex_id\", \"doi\", \"title\", \"publication_year\", \"venue_name\", \"cited_by_count\"]:\n",
    "        if c in df.columns:\n",
    "            cols.append(c)\n",
    "\n",
    "    view = df[cols].copy() if cols else df.copy()\n",
    "    if \"doi\" in view.columns:\n",
    "        view[\"dup_doi\"] = view[\"doi\"].notna() & view[\"doi\"].duplicated(keep=False)\n",
    "    else:\n",
    "        view[\"dup_doi\"] = False\n",
    "\n",
    "    # Title duplicate flag (exact match only; fuzzy matching is downstream)\n",
    "    if \"title\" in view.columns:\n",
    "        view[\"dup_title_exact\"] = view[\"title\"].notna() & view[\"title\"].duplicated(keep=False)\n",
    "    else:\n",
    "        view[\"dup_title_exact\"] = False\n",
    "\n",
    "    return view\n",
    "\n",
    "\n",
    "# ------------------------\n",
    "# Export function\n",
    "# ------------------------\n",
    "def export_openalex_outputs(\n",
    "    df_works: pd.DataFrame,\n",
    "    retrieval_meta: dict,\n",
    "    query_text: str,\n",
    "    output_dir: str = \"./data/openalex\",\n",
    "    include_timestamp: bool = True,\n",
    "    export_dedup_view: bool = True\n",
    ") -> dict:\n",
    "    \"\"\"\n",
    "    Export:\n",
    "      - normalized CSV\n",
    "      - retrieval metadata JSON\n",
    "      - optional dedup view CSV\n",
    "    Returns paths as a dict.\n",
    "    \"\"\"\n",
    "    _ensure_dir(output_dir)\n",
    "\n",
    "    ts = _utc_timestamp() if include_timestamp else None\n",
    "    slug = _safe_slug(query_text)\n",
    "\n",
    "    base = f\"openalex_works__{slug}\"\n",
    "    if ts:\n",
    "        base = f\"{base}__{ts}\"\n",
    "\n",
    "    csv_path = os.path.join(output_dir, f\"{base}.csv\")\n",
    "    meta_path = os.path.join(output_dir, f\"{base}__meta.json\")\n",
    "\n",
    "    # Ensure meta contains export context\n",
    "    meta_out = dict(retrieval_meta or {})\n",
    "    meta_out.update({\n",
    "        \"exported_at_utc\": datetime.now(timezone.utc).isoformat(),\n",
    "        \"export_csv\": csv_path,\n",
    "        \"rows_exported\": int(len(df_works)),\n",
    "    })\n",
    "\n",
    "    # Write outputs\n",
    "    df_works.to_csv(csv_path, index=False)\n",
    "    _write_json(meta_path, meta_out)\n",
    "\n",
    "    paths = {\n",
    "        \"csv_path\": csv_path,\n",
    "        \"meta_path\": meta_path,\n",
    "    }\n",
    "\n",
    "    if export_dedup_view:\n",
    "        dedup_view = _basic_dedup_view(df_works)\n",
    "        dedup_path = os.path.join(output_dir, f\"{base}__dedup_view.csv\")\n",
    "        dedup_view.to_csv(dedup_path, index=False)\n",
    "        paths[\"dedup_view_path\"] = dedup_path\n",
    "\n",
    "    return paths\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# UI: OpenAlex Keyword Search Controls (ipywidgets)\n",
    "# ============================================================\n",
    "\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, clear_output\n",
    "import pandas as pd\n",
    "\n",
    "# --- Widgets ---\n",
    "w_query = widgets.Text(\n",
    "    value=\"venture capital OR startup OR innovation policy\",\n",
    "    description=\"query_text\",\n",
    "    layout=widgets.Layout(width=\"900px\")\n",
    ")\n",
    "\n",
    "w_from_year = widgets.IntText(\n",
    "    value=2010,\n",
    "    description=\"from_year\",\n",
    "    layout=widgets.Layout(width=\"250px\")\n",
    ")\n",
    "\n",
    "w_to_year = widgets.IntText(\n",
    "    value=2025,\n",
    "    description=\"to_year\",\n",
    "    layout=widgets.Layout(width=\"250px\")\n",
    ")\n",
    "\n",
    "w_per_page = widgets.IntSlider(\n",
    "    value=200,\n",
    "    min=25,\n",
    "    max=200,\n",
    "    step=25,\n",
    "    description=\"per_page\",\n",
    "    continuous_update=False,\n",
    "    layout=widgets.Layout(width=\"500px\")\n",
    ")\n",
    "\n",
    "w_max_results = widgets.IntText(\n",
    "    value=1000,\n",
    "    description=\"max_results\",\n",
    "    layout=widgets.Layout(width=\"250px\")\n",
    ")\n",
    "\n",
    "w_open_access = widgets.Checkbox(\n",
    "    value=False,\n",
    "    description=\"open_access_only\"\n",
    ")\n",
    "\n",
    "w_run = widgets.Button(\n",
    "    description=\"Run Search\",\n",
    "    button_style=\"success\",\n",
    "    tooltip=\"Call OpenAlex API and export CSV\",\n",
    "    icon=\"play\"\n",
    ")\n",
    "\n",
    "w_status = widgets.HTML(value=\"\")\n",
    "out = widgets.Output()\n",
    "\n",
    "# Optional: show the computed OpenAlex filter string\n",
    "w_filter_preview = widgets.HTML(value=\"\")\n",
    "\n",
    "\n",
    "def _update_filter_preview(*args):\n",
    "    filt = _build_openalex_filters(\n",
    "        from_year=w_from_year.value,\n",
    "        to_year=w_to_year.value,\n",
    "        open_access_only=w_open_access.value\n",
    "    )\n",
    "    w_filter_preview.value = f\"<b>filter</b>: {filt if filt else '(none)'}\"\n",
    "\n",
    "for w in [w_from_year, w_to_year, w_open_access]:\n",
    "    w.observe(_update_filter_preview, names=\"value\")\n",
    "\n",
    "_update_filter_preview()\n",
    "\n",
    "\n",
    "# --- Runner callback ---\n",
    "def _on_run_clicked(btn):\n",
    "    with out:\n",
    "        clear_output(wait=True)\n",
    "\n",
    "        # Basic validation\n",
    "        if not w_query.value.strip():\n",
    "            print(\"⚠️ query_text is empty.\")\n",
    "            return\n",
    "        if w_from_year.value and w_to_year.value and w_from_year.value > w_to_year.value:\n",
    "            print(\"⚠️ from_year must be <= to_year.\")\n",
    "            return\n",
    "        if w_per_page.value < 1 or w_per_page.value > 200:\n",
    "            print(\"⚠️ per_page must be between 1 and 200.\")\n",
    "            return\n",
    "        if w_max_results.value < 1:\n",
    "            print(\"⚠️ max_results must be >= 1.\")\n",
    "            return\n",
    "\n",
    "        # Display selected params\n",
    "        filt = _build_openalex_filters(\n",
    "            from_year=w_from_year.value,\n",
    "            to_year=w_to_year.value,\n",
    "            open_access_only=w_open_access.value\n",
    "        )\n",
    "\n",
    "        print(\"=== Selected Parameters ===\")\n",
    "        print(f\"- query_text        : {w_query.value}\")\n",
    "        print(f\"- filter            : {filt if filt else '(none)'}\")\n",
    "        print(f\"- per_page          : {w_per_page.value}\")\n",
    "        print(f\"- max_results       : {w_max_results.value}\")\n",
    "        print(f\"- open_access_only  : {w_open_access.value}\")\n",
    "        print(\"\")\n",
    "\n",
    "        # 1) Retrieve\n",
    "        raw_works, retrieval_meta = openalex_keyword_search_works(\n",
    "            query_text=w_query.value,\n",
    "            from_year=w_from_year.value,\n",
    "            to_year=w_to_year.value,\n",
    "            open_access_only=w_open_access.value,\n",
    "            per_page=w_per_page.value,\n",
    "            max_results=w_max_results.value,\n",
    "            sort_by=\"relevance_score\",\n",
    "            output_dir=output_dir,\n",
    "            save_jsonl=True,\n",
    "            polite_delay_sec=0.1,\n",
    "            mailto=None,\n",
    "            verbose=True\n",
    "        )\n",
    "\n",
    "        print(\"\")\n",
    "        print(f\"Retrieved raw works: {len(raw_works)}\")\n",
    "\n",
    "        # 2) Normalize\n",
    "        df_works = normalize_openalex_works(raw_works)\n",
    "        print(f\"Normalized rows   : {len(df_works)}\")\n",
    "\n",
    "        # 3) Summary\n",
    "        print(\"\")\n",
    "        _print_summary(df_works)\n",
    "\n",
    "        # 4) Export\n",
    "        print(\"\")\n",
    "        paths = export_openalex_outputs(\n",
    "            df_works=df_works,\n",
    "            retrieval_meta=retrieval_meta,\n",
    "            query_text=w_query.value,\n",
    "            output_dir=output_dir,\n",
    "            include_timestamp=True,\n",
    "            export_dedup_view=True\n",
    "        )\n",
    "\n",
    "        print(\"\")\n",
    "        print(\"=== Exported files ===\")\n",
    "        for k, v in paths.items():\n",
    "            print(f\"- {k}: {v}\")\n",
    "\n",
    "        display(df_works.head(10))\n",
    "\n",
    "        # Keep latest result in globals for convenience\n",
    "        globals()[\"raw_works\"] = raw_works\n",
    "        globals()[\"retrieval_meta\"] = retrieval_meta\n",
    "        globals()[\"df_works\"] = df_works\n",
    "        globals()[\"export_paths\"] = paths\n",
    "\n",
    "\n",
    "w_run.on_click(_on_run_clicked)\n",
    "\n",
    "\n",
    "# --- Layout ---\n",
    "row1 = widgets.HBox([w_from_year, w_to_year, w_open_access])\n",
    "row2 = widgets.HBox([w_per_page, w_max_results])\n",
    "panel = widgets.VBox([\n",
    "    widgets.HTML(\"<h3>OpenAlex Keyword Search</h3>\"),\n",
    "    w_query,\n",
    "    row1,\n",
    "    w_filter_preview,\n",
    "    row2,\n",
    "    w_run,\n",
    "    out\n",
    "])\n",
    "\n",
    "display(panel)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b007513a-0fe6-4e31-9922-7f7ea0814dab",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
