{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "6dba0eb8-9ab0-4e80-9d74-29b2feeb06b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 006_RAG-Based Research Question Generator\n",
    "# ============================================================\n",
    "#\n",
    "# Overview\n",
    "# ----------------\n",
    "# This notebook implements a Retrieval-Augmented Generation (RAG) workflow\n",
    "# to generate *researchable* questions grounded in your existing materials.\n",
    "#\n",
    "# Unlike summarization-only pipelines, the goal here is to turn a broad\n",
    "# research idea into a shortlist of concrete, empirically testable RQs by:\n",
    "#   1) retrieving relevant context from your knowledge base, then\n",
    "#   2) generating structured RQ drafts conditioned on that evidence.\n",
    "#\n",
    "# In this project setup, the knowledge base is built from:\n",
    "# - Google Drive: PDFs (academic papers, reports)\n",
    "# - Notion: prior RQ memos and notes\n",
    "# and retrieval is performed via an OpenAI Vector Store.\n",
    "#\n",
    "# The final outputs are aligned to a Notion database schema, enabling a\n",
    "# \"generate ‚Üí review ‚Üí refine ‚Üí store\" loop inside a personal Research OS.\n",
    "#\n",
    "#\n",
    "# Inputs / Outputs\n",
    "# ----------------\n",
    "# Inputs:\n",
    "# - Google Drive folder (PDF corpus) identified by DRIVE_FOLDER_ID\n",
    "# - Notion database / data source (RQ memos) via NOTION_TOKEN and NOTION_RQ_DB_ID\n",
    "# - OpenAI API key for vector store retrieval and LLM generation\n",
    "# - Prompt templates and runtime parameters (top-k, temperature, etc.)\n",
    "#\n",
    "# Outputs:\n",
    "# - RQ candidates in a DB-aligned schema:\n",
    "#     - Name (Japanese research question)\n",
    "#     - Rationale / Background (JP)\n",
    "#     - Gap Identified (JP)\n",
    "#     - Proposed Approach (JP)\n",
    "#     - Priority (fixed: Medium)\n",
    "#     - Status (fixed: New)\n",
    "#     - Tags (English keywords, ~5)\n",
    "# - Evidence references used for each RQ (e.g., evidence_used: [1, 2, ...])\n",
    "# - Exportable artifacts (CSV / JSON) and an execution manifest for traceability\n",
    "# - Optionally: newly created pages in the Notion RQ database\n",
    "#\n",
    "#\n",
    "# Structure\n",
    "# ----------------\n",
    "# Cell 0 : Purpose, scope, and design assumptions\n",
    "# Cell 1 : Imports and global configuration (OpenAI / Notion / Google)\n",
    "# Cell 2 : Load source materials (Google Drive PDFs + Notion RQ notes)\n",
    "# Cell 3 : Embedding setup and vector store initialization (OpenAI Vector Store)\n",
    "# Cell 4 : Retrieval logic (top-k semantic search over the vector store)\n",
    "# Cell 5 : Prompt construction (DB-aligned JSON schema + Japanese output constraints)\n",
    "# Cell 6 : Interactive generation UI (retrieve ‚Üí generate ‚Üí Go/No-Go ‚Üí edit)\n",
    "# Cell 7 : Post-processing (normalization, filtering, deduplication; DB-aligned)\n",
    "# Cell 8 : Export results and quick inspection (CSV/JSON + run manifest)\n",
    "# Cell 9 : Write selected RQs to Notion (DB-aligned, schema + option checks)\n",
    "#\n",
    "#\n",
    "# Notes\n",
    "# ----------------\n",
    "# - This notebook generates questions; it does not attempt to answer them.\n",
    "# - Retrieval quality determines generation quality. If outputs look noisy,\n",
    "#   inspect the retrieved contexts (Cell 4/6) and adjust top-k or corpus inputs.\n",
    "# - Treat generated RQs as drafts. Use the Go/No-Go + manual edit step to\n",
    "#   improve clarity, feasibility, and empirical specificity.\n",
    "# - The pipeline is modular: you can swap retrieval backends, prompts, models,\n",
    "#   or output schema without rewriting the entire workflow.\n",
    "# - If Notion writes fail, the most common causes are:\n",
    "#   (1) the DB property types do not match, or\n",
    "#   (2) required select/status options (e.g., Medium/New) are missing.\n",
    "# ============================================================\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "638a918f-6576-4a35-a6bf-56d3b0eccd5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ OPENAI_API_KEY loaded successfully\n",
      "‚úÖ Notion auth OK\n",
      "‚úÖ Google OAuth client secret loaded: client_secret_750875982200-85rnsoqhr2af2b13peueev0bm60q22sh.apps.googleusercontent.com.json\n",
      "‚úÖ Google Drive client initialized\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# Cell 1 : Imports and Global Configuration\n",
    "# ============================================================\n",
    "#\n",
    "# This cell defines all core imports and global configuration variables\n",
    "# used throughout the notebook.\n",
    "#\n",
    "# It also:\n",
    "# - loads environment variables from env.txt\n",
    "# - validates API credentials (OpenAI / Notion / Google Drive)\n",
    "# - prepares reusable service clients (OpenAI, Notion headers, Drive service factory)\n",
    "#\n",
    "# All environment-dependent settings should be modified here.\n",
    "#\n",
    "\n",
    "# ----------------\n",
    "# Standard Library\n",
    "# ----------------\n",
    "import os\n",
    "import json\n",
    "import pathlib\n",
    "from typing import List, Dict, Any, Optional\n",
    "\n",
    "# ----------------\n",
    "# Data Handling\n",
    "# ----------------\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# ----------------\n",
    "# Environment Variables\n",
    "# ----------------\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# ----------------\n",
    "# HTTP\n",
    "# ----------------\n",
    "import requests\n",
    "\n",
    "# ----------------\n",
    "# Google API (Drive)\n",
    "# ----------------\n",
    "from googleapiclient.discovery import build\n",
    "from googleapiclient.http import MediaIoBaseDownload\n",
    "from google.auth.transport.requests import Request\n",
    "from google.oauth2.credentials import Credentials\n",
    "from google_auth_oauthlib.flow import InstalledAppFlow\n",
    "\n",
    "# ----------------\n",
    "# OpenAI\n",
    "# ----------------\n",
    "from openai import OpenAI\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Load environment variables\n",
    "# ------------------------------------------------------------\n",
    "# Explicitly load env.txt (instead of default .env)\n",
    "load_dotenv(\"env.txt\")\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# Global Paths\n",
    "# ============================================================\n",
    "BASE_DIR = pathlib.Path(\".\")\n",
    "DATA_DIR = BASE_DIR / \"data\"\n",
    "OUTPUT_DIR = BASE_DIR / \"outputs\"\n",
    "\n",
    "DATA_DIR.mkdir(exist_ok=True)\n",
    "OUTPUT_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# Utility: required env var checker\n",
    "# ============================================================\n",
    "def require_env(name: str) -> str:\n",
    "    \"\"\"\n",
    "    Raise a clear error if a required environment variable is missing.\n",
    "    \"\"\"\n",
    "    val = os.getenv(name)\n",
    "    if not val:\n",
    "        raise ValueError(f\"{name} is missing in env.txt\")\n",
    "    return val\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# OpenAI configuration\n",
    "# ============================================================\n",
    "OPENAI_API_KEY = require_env(\"OPENAI_API_KEY\")\n",
    "print(\"‚úÖ OPENAI_API_KEY loaded successfully\")\n",
    "\n",
    "openai_client = OpenAI(api_key=OPENAI_API_KEY)\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# Notion configuration\n",
    "# ============================================================\n",
    "NOTION_TOKEN = require_env(\"NOTION_TOKEN\")\n",
    "NOTION_VERSION = require_env(\"NOTION_VERSION\")            # e.g., \"2022-06-28\" or your pinned version\n",
    "NOTION_RQ_DB_ID = require_env(\"NOTION_RQ_DB_ID\")          # Research Question database ID\n",
    "# Optional (only if you use it elsewhere)\n",
    "NOTION_RQ_DATA_SOURCE_ID = os.getenv(\"NOTION_RQ_DATA_SOURCE_ID\")\n",
    "\n",
    "NOTION_HEADERS = {\n",
    "    \"Authorization\": f\"Bearer {NOTION_TOKEN}\",\n",
    "    \"Notion-Version\": NOTION_VERSION,\n",
    "    \"Content-Type\": \"application/json\",\n",
    "}\n",
    "\n",
    "# Quick auth check (safe: does not reveal token)\n",
    "try:\n",
    "    r = requests.get(\"https://api.notion.com/v1/users/me\", headers=NOTION_HEADERS, timeout=30)\n",
    "    if r.status_code == 200:\n",
    "        print(\"‚úÖ Notion auth OK\")\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è Notion auth check failed:\", r.status_code, r.text[:200])\n",
    "except Exception as e:\n",
    "    print(\"‚ö†Ô∏è Notion auth check error:\", type(e).__name__, str(e))\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# Google Drive configuration\n",
    "# ============================================================\n",
    "\n",
    "# Read-only scope is sufficient for paper ingestion\n",
    "DRIVE_SCOPES = [\"https://www.googleapis.com/auth/drive.readonly\"]\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# OAuth client secret handling\n",
    "# ------------------------------------------------------------\n",
    "# Priority:\n",
    "# 1) GOOGLE_OAUTH_CLIENT_SECRET_JSON in env.txt\n",
    "# 2) Default local filename (checked in working directory)\n",
    "#\n",
    "DEFAULT_GOOGLE_CLIENT_SECRET = (\n",
    "    \"client_secret_750875982200-85rnsoqhr2af2b13peueev0bm60q22sh.apps.googleusercontent.com.json\"\n",
    ")\n",
    "\n",
    "GOOGLE_OAUTH_CLIENT_SECRET_JSON = os.getenv(\"GOOGLE_OAUTH_CLIENT_SECRET_JSON\")\n",
    "\n",
    "if GOOGLE_OAUTH_CLIENT_SECRET_JSON:\n",
    "    client_secret_path = pathlib.Path(GOOGLE_OAUTH_CLIENT_SECRET_JSON)\n",
    "else:\n",
    "    client_secret_path = pathlib.Path(DEFAULT_GOOGLE_CLIENT_SECRET)\n",
    "\n",
    "if not client_secret_path.exists():\n",
    "    raise ValueError(\n",
    "        \"Google OAuth client secret JSON not found.\\n\"\n",
    "        \"Either:\\n\"\n",
    "        \"  - set GOOGLE_OAUTH_CLIENT_SECRET_JSON in env.txt, or\\n\"\n",
    "        f\"  - place the file at: {DEFAULT_GOOGLE_CLIENT_SECRET}\"\n",
    "    )\n",
    "\n",
    "print(f\"‚úÖ Google OAuth client secret loaded: {client_secret_path}\")\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Token cache path (still configurable via env.txt)\n",
    "# ------------------------------------------------------------\n",
    "GOOGLE_TOKEN_JSON = os.getenv(\"GOOGLE_TOKEN_JSON\", \"google_token.json\")\n",
    "DRIVE_FOLDER_ID = require_env(\"DRIVE_FOLDER_ID\")\n",
    "\n",
    "def get_drive_service():\n",
    "    \"\"\"\n",
    "    Authenticate with Google Drive API and return a Drive service object.\n",
    "    Uses OAuth with token caching to avoid repeated authentication.\n",
    "    \"\"\"\n",
    "    creds = None\n",
    "    token_path = pathlib.Path(GOOGLE_TOKEN_JSON)\n",
    "\n",
    "    if token_path.exists():\n",
    "        creds = Credentials.from_authorized_user_file(\n",
    "            str(token_path), DRIVE_SCOPES\n",
    "        )\n",
    "\n",
    "    if not creds or not creds.valid:\n",
    "        if creds and creds.expired and creds.refresh_token:\n",
    "            creds.refresh(Request())\n",
    "        else:\n",
    "            flow = InstalledAppFlow.from_client_secrets_file(\n",
    "                str(client_secret_path), DRIVE_SCOPES\n",
    "            )\n",
    "            # Local environment: browser-based auth\n",
    "            creds = flow.run_local_server(port=0)\n",
    "\n",
    "        token_path.write_text(creds.to_json(), encoding=\"utf-8\")\n",
    "\n",
    "    return build(\"drive\", \"v3\", credentials=creds)\n",
    "\n",
    "# Initialize Drive client\n",
    "try:\n",
    "    drive = get_drive_service()\n",
    "    print(\"‚úÖ Google Drive client initialized\")\n",
    "except Exception as e:\n",
    "    drive = None\n",
    "    print(\"‚ö†Ô∏è Google Drive init error:\", type(e).__name__, str(e))\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# Retrieval / Generation Parameters\n",
    "# ============================================================\n",
    "TOP_K_RETRIEVAL = 5      # Number of documents retrieved per query\n",
    "MIN_SIMILARITY = 0.2    # Optional similarity threshold\n",
    "\n",
    "EMBEDDING_MODEL = \"text-embedding-3-small\"\n",
    "LLM_MODEL = \"gpt-4.1-mini\"\n",
    "MAX_TOKENS = 2000\n",
    "TEMPERATURE = 0.7\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# Similarity Utility (NumPy only; avoids sklearn/pyarrow import issues)\n",
    "# ============================================================\n",
    "def cosine_similarity_matrix(A: np.ndarray, B: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Compute cosine similarity between two 2D arrays:\n",
    "      A: (n, d)\n",
    "      B: (m, d)\n",
    "    Returns:\n",
    "      (n, m) cosine similarity matrix\n",
    "    \"\"\"\n",
    "    A = A.astype(np.float32)\n",
    "    B = B.astype(np.float32)\n",
    "    A = A / (np.linalg.norm(A, axis=1, keepdims=True) + 1e-12)\n",
    "    B = B / (np.linalg.norm(B, axis=1, keepdims=True) + 1e-12)\n",
    "    return A @ B.T\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# Reproducibility & Display\n",
    "# ============================================================\n",
    "RANDOM_SEED = 42\n",
    "np.random.seed(RANDOM_SEED)\n",
    "\n",
    "pd.set_option(\"display.max_colwidth\", 120)\n",
    "pd.set_option(\"display.max_rows\", 50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "590be286-0cb6-49a2-88cd-283d20e539bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Google Drive: found 35 PDFs in folder.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 35/35 [03:30<00:00,  6.02s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Google Drive: downloaded 34, skipped 1 (already present).\n",
      "üìÅ PDFs stored at: outputs/drive_pdfs\n",
      "‚úÖ Notion: fetched 29 pages from RQ database.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 29/29 [00:00<00:00, 2234.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Notion: wrote 29, skipped 0 (already present).\n",
      "üìÅ Notion RQs stored at: outputs/notion_rqs\n",
      "\n",
      "--- Summary ---\n",
      "Drive PDF paths: 35\n",
      "Notion txt paths: 29\n",
      "Total RAG file paths: 64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# Cell 2 : Ingest Sources (Google Drive PDFs + Notion RQs)\n",
    "# ============================================================\n",
    "#\n",
    "# This cell \"bulk-ingests\" two sources into local artifacts for RAG:\n",
    "# 1) Google Drive folder: downloads PDF papers into outputs/drive_pdfs/\n",
    "# 2) Notion RQ database: exports each row/page into outputs/notion_rqs/*.txt\n",
    "#\n",
    "# Output artifacts are designed to be:\n",
    "# - traceable (stable filenames / ids)\n",
    "# - incrementally updatable (skip if already downloaded/exported)\n",
    "# - vector-store friendly (ready to upload as files)\n",
    "#\n",
    "# Requirements (from Cell 1):\n",
    "# - drive: authenticated Google Drive service object\n",
    "# - DRIVE_FOLDER_ID\n",
    "# - NOTION_HEADERS, NOTION_RQ_DB_ID\n",
    "# - OUTPUT_DIR\n",
    "#\n",
    "\n",
    "import io\n",
    "import time\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "import requests\n",
    "\n",
    "# ----------------------------\n",
    "# Output directories\n",
    "# ----------------------------\n",
    "DRIVE_DOWNLOAD_DIR = OUTPUT_DIR / \"drive_pdfs\"\n",
    "NOTION_EXPORT_DIR = OUTPUT_DIR / \"notion_rqs\"\n",
    "DRIVE_DOWNLOAD_DIR.mkdir(parents=True, exist_ok=True)\n",
    "NOTION_EXPORT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# ----------------------------\n",
    "# Google Drive: list + download\n",
    "# ----------------------------\n",
    "def list_all_pdfs_in_folder(folder_id: str, page_size: int = 200):\n",
    "    \"\"\"\n",
    "    List all PDF files in a Google Drive folder (pagination).\n",
    "    Returns: list of dicts with keys like id, name, modifiedTime, size.\n",
    "    \"\"\"\n",
    "    if drive is None:\n",
    "        raise RuntimeError(\"Google Drive client is not initialized (drive is None).\")\n",
    "\n",
    "    q = (\n",
    "        f\"'{folder_id}' in parents \"\n",
    "        f\"and mimeType='application/pdf' \"\n",
    "        f\"and trashed=false\"\n",
    "    )\n",
    "\n",
    "    files = []\n",
    "    page_token = None\n",
    "    while True:\n",
    "        resp = drive.files().list(\n",
    "            q=q,\n",
    "            pageSize=page_size,\n",
    "            pageToken=page_token,\n",
    "            fields=\"nextPageToken, files(id,name,modifiedTime,size)\"\n",
    "        ).execute()\n",
    "\n",
    "        files.extend(resp.get(\"files\", []))\n",
    "        page_token = resp.get(\"nextPageToken\")\n",
    "        if not page_token:\n",
    "            break\n",
    "\n",
    "    return files\n",
    "\n",
    "def download_drive_pdf(file_id: str, dest_path: Path):\n",
    "    \"\"\"\n",
    "    Download a Drive file (PDF) to dest_path.\n",
    "    \"\"\"\n",
    "    request = drive.files().get_media(fileId=file_id)\n",
    "    bio = io.BytesIO()\n",
    "    downloader = MediaIoBaseDownload(bio, request)\n",
    "\n",
    "    done = False\n",
    "    while not done:\n",
    "        status, done = downloader.next_chunk()\n",
    "\n",
    "    dest_path.write_bytes(bio.getvalue())\n",
    "\n",
    "def sanitize_filename(name: str) -> str:\n",
    "    \"\"\"\n",
    "    Minimal filename sanitizer (safe for macOS/Linux/Windows).\n",
    "    \"\"\"\n",
    "    bad = ['/', '\\\\', ':', '*', '?', '\"', '<', '>', '|']\n",
    "    for ch in bad:\n",
    "        name = name.replace(ch, \"_\")\n",
    "    return name.strip()\n",
    "\n",
    "# ---- Drive run\n",
    "drive_pdf_files = list_all_pdfs_in_folder(DRIVE_FOLDER_ID)\n",
    "print(f\"‚úÖ Google Drive: found {len(drive_pdf_files)} PDFs in folder.\")\n",
    "\n",
    "drive_downloaded_paths = []\n",
    "drive_skipped = 0\n",
    "drive_downloaded = 0\n",
    "\n",
    "for f in tqdm(drive_pdf_files):\n",
    "    file_id = f[\"id\"]\n",
    "    name = sanitize_filename(f[\"name\"])\n",
    "    out_path = DRIVE_DOWNLOAD_DIR / name\n",
    "\n",
    "    # Incremental behavior: skip if already exists and non-empty\n",
    "    if out_path.exists() and out_path.stat().st_size > 0:\n",
    "        drive_skipped += 1\n",
    "        drive_downloaded_paths.append(str(out_path))\n",
    "        continue\n",
    "\n",
    "    download_drive_pdf(file_id, out_path)\n",
    "    drive_downloaded += 1\n",
    "    drive_downloaded_paths.append(str(out_path))\n",
    "\n",
    "print(f\"‚úÖ Google Drive: downloaded {drive_downloaded}, skipped {drive_skipped} (already present).\")\n",
    "print(f\"üìÅ PDFs stored at: {DRIVE_DOWNLOAD_DIR}\")\n",
    "\n",
    "# ----------------------------\n",
    "# Notion: query all pages + export to txt\n",
    "# ----------------------------\n",
    "def notion_query_all_pages(database_id: str, page_size: int = 100):\n",
    "    \"\"\"\n",
    "    Query a Notion database and return all pages (pagination).\n",
    "    \"\"\"\n",
    "    url = f\"https://api.notion.com/v1/databases/{database_id}/query\"\n",
    "    all_results = []\n",
    "    payload = {\"page_size\": page_size}\n",
    "\n",
    "    has_more = True\n",
    "    next_cursor = None\n",
    "\n",
    "    while has_more:\n",
    "        if next_cursor:\n",
    "            payload[\"start_cursor\"] = next_cursor\n",
    "\n",
    "        r = requests.post(url, headers=NOTION_HEADERS, json=payload, timeout=60)\n",
    "        r.raise_for_status()\n",
    "        data = r.json()\n",
    "\n",
    "        all_results.extend(data.get(\"results\", []))\n",
    "        has_more = data.get(\"has_more\", False)\n",
    "        next_cursor = data.get(\"next_cursor\")\n",
    "\n",
    "    return all_results\n",
    "\n",
    "def extract_plain_text_from_property(prop: dict) -> str:\n",
    "    \"\"\"\n",
    "    Best-effort extraction of human-readable text from common Notion property types.\n",
    "    \"\"\"\n",
    "    t = prop.get(\"type\")\n",
    "    if t == \"title\":\n",
    "        return \" \".join([x.get(\"plain_text\", \"\") for x in prop.get(\"title\", [])]).strip()\n",
    "    if t == \"rich_text\":\n",
    "        return \" \".join([x.get(\"plain_text\", \"\") for x in prop.get(\"rich_text\", [])]).strip()\n",
    "    if t == \"select\":\n",
    "        s = prop.get(\"select\")\n",
    "        return (s.get(\"name\") if s else \"\").strip()\n",
    "    if t == \"multi_select\":\n",
    "        return \", \".join([x.get(\"name\", \"\") for x in prop.get(\"multi_select\", []) if x.get(\"name\")]).strip()\n",
    "    if t == \"number\":\n",
    "        n = prop.get(\"number\")\n",
    "        return \"\" if n is None else str(n)\n",
    "    if t == \"status\":\n",
    "        s = prop.get(\"status\")\n",
    "        return (s.get(\"name\") if s else \"\").strip()\n",
    "    if t == \"date\":\n",
    "        d = prop.get(\"date\")\n",
    "        return (d.get(\"start\") if d else \"\").strip()\n",
    "    if t == \"url\":\n",
    "        return (prop.get(\"url\") or \"\").strip()\n",
    "    if t == \"people\":\n",
    "        return \", \".join([p.get(\"name\", \"\") for p in prop.get(\"people\", []) if p.get(\"name\")]).strip()\n",
    "    return \"\"\n",
    "\n",
    "def notion_page_to_text(page: dict) -> str:\n",
    "    \"\"\"\n",
    "    Convert a Notion database row/page into a single text document for RAG.\n",
    "    \"\"\"\n",
    "    props = page.get(\"properties\", {})\n",
    "    lines = []\n",
    "    lines.append(f\"NOTION_PAGE_ID: {page.get('id','')}\")\n",
    "    lines.append(f\"NOTION_URL: {page.get('url','')}\")\n",
    "    lines.append(\"\")\n",
    "\n",
    "    # Dump properties in a stable order (alphabetical) for determinism\n",
    "    for key in sorted(props.keys()):\n",
    "        val = extract_plain_text_from_property(props[key])\n",
    "        if val:\n",
    "            lines.append(f\"{key}: {val}\")\n",
    "\n",
    "    return \"\\n\".join(lines).strip()\n",
    "\n",
    "# ---- Notion run\n",
    "notion_pages = notion_query_all_pages(NOTION_RQ_DB_ID)\n",
    "print(f\"‚úÖ Notion: fetched {len(notion_pages)} pages from RQ database.\")\n",
    "\n",
    "notion_text_paths = []\n",
    "notion_written = 0\n",
    "notion_skipped = 0\n",
    "\n",
    "for p in tqdm(notion_pages):\n",
    "    # Stable filename based on page_id (strip hyphens)\n",
    "    page_id = p.get(\"id\", \"\").replace(\"-\", \"\")\n",
    "    out_path = NOTION_EXPORT_DIR / f\"rq_{page_id}.txt\"\n",
    "\n",
    "    # Incremental behavior: skip if already exists and non-empty\n",
    "    if out_path.exists() and out_path.stat().st_size > 0:\n",
    "        notion_skipped += 1\n",
    "        notion_text_paths.append(str(out_path))\n",
    "        continue\n",
    "\n",
    "    text = notion_page_to_text(p)\n",
    "    out_path.write_text(text, encoding=\"utf-8\")\n",
    "    notion_written += 1\n",
    "    notion_text_paths.append(str(out_path))\n",
    "\n",
    "print(f\"‚úÖ Notion: wrote {notion_written}, skipped {notion_skipped} (already present).\")\n",
    "print(f\"üìÅ Notion RQs stored at: {NOTION_EXPORT_DIR}\")\n",
    "\n",
    "# ----------------------------\n",
    "# Summary outputs for downstream cells\n",
    "# ----------------------------\n",
    "print(\"\\n--- Summary ---\")\n",
    "print(\"Drive PDF paths:\", len(drive_downloaded_paths))\n",
    "print(\"Notion txt paths:\", len(notion_text_paths))\n",
    "\n",
    "# These variables are used in the next step (Vector Store upload)\n",
    "ALL_RAG_FILE_PATHS = drive_downloaded_paths + notion_text_paths\n",
    "print(\"Total RAG file paths:\", len(ALL_RAG_FILE_PATHS))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "479f2b2e-45db-4fca-91d9-caec7e30c2a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Created vector store: vs_695b45e293f48191a78759668bb0cba8 (RQ-RAG-Drive+Notion)\n",
      "Total local artifacts to upload: 64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading files to OpenAI: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 64/64 [02:18<00:00,  2.17s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Uploaded 64 files.\n",
      "‚úÖ File batch created: vsfb_ibj_695b466e332881f4bc635011f77c33bd (status=in_progress)\n",
      "‚úÖ Final batch status: completed\n",
      "file_counts: FileCounts(cancelled=0, completed=64, failed=0, in_progress=0, total=64)\n",
      "\n",
      "‚úÖ Vector store ready for retrieval.\n",
      "VECTOR_STORE_ID = vs_695b45e293f48191a78759668bb0cba8\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# Cell 3 : Embedding Setup and Vector Store Initialization\n",
    "# ============================================================\n",
    "#\n",
    "# This notebook uses OpenAI Vector Stores as the retrieval backend.\n",
    "# When files are added to a vector store, OpenAI automatically:\n",
    "# - parses file contents,\n",
    "# - chunks them,\n",
    "# - builds embeddings,\n",
    "# - and indexes them for semantic search / file_search usage.\n",
    "#\n",
    "# Therefore, we do NOT manually compute embeddings in this notebook.\n",
    "# Instead, we:\n",
    "# 1) create (or reuse) a vector store\n",
    "# 2) upload local artifacts (Drive PDFs + Notion TXT exports) as OpenAI Files\n",
    "# 3) attach them to the vector store via a file batch\n",
    "# 4) poll until the vector store is ready for retrieval\n",
    "#\n",
    "# Requirements (from previous cells):\n",
    "# - openai_client (OpenAI client)\n",
    "# - ALL_RAG_FILE_PATHS (list[str]) created in Cell 2\n",
    "#\n",
    "# References:\n",
    "# - Vector stores API :contentReference[oaicite:1]{index=1}\n",
    "# - Vector store file batches :contentReference[oaicite:2]{index=2}\n",
    "# - Vector store files status :contentReference[oaicite:3]{index=3}\n",
    "# ============================================================\n",
    "\n",
    "import os\n",
    "import time\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "\n",
    "# ----------------------------\n",
    "# Vector Store Configuration\n",
    "# ----------------------------\n",
    "VECTOR_STORE_NAME = \"RQ-RAG-Drive+Notion\"\n",
    "REUSE_VECTOR_STORE_ID = os.getenv(\"VECTOR_STORE_ID\")  # optional: reuse an existing store if provided\n",
    "\n",
    "# Upload batching\n",
    "UPLOAD_PURPOSE = \"assistants\"  # purpose used for files that will be searched by file_search-style tools\n",
    "\n",
    "# Polling\n",
    "POLL_INTERVAL_SEC = 2\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# Helpers\n",
    "# ----------------------------\n",
    "def create_or_reuse_vector_store(name: str, reuse_id: str | None = None):\n",
    "    \"\"\"\n",
    "    Create a new vector store, or reuse an existing one if an ID is provided.\n",
    "    \"\"\"\n",
    "    if reuse_id:\n",
    "        vs = openai_client.vector_stores.retrieve(reuse_id)\n",
    "        print(f\"‚úÖ Reusing vector store: {vs.id} ({getattr(vs, 'name', '')})\")\n",
    "        return vs\n",
    "\n",
    "    vs = openai_client.vector_stores.create(name=name)\n",
    "    print(f\"‚úÖ Created vector store: {vs.id} ({name})\")\n",
    "    return vs\n",
    "\n",
    "\n",
    "def upload_files(file_paths: list[str]) -> list[str]:\n",
    "    \"\"\"\n",
    "    Upload local files to OpenAI Files API and return file_ids.\n",
    "    \"\"\"\n",
    "    file_ids = []\n",
    "    for p in tqdm(file_paths, desc=\"Uploading files to OpenAI\"):\n",
    "        path = Path(p)\n",
    "        if not path.exists():\n",
    "            raise FileNotFoundError(f\"Missing file: {path}\")\n",
    "\n",
    "        with open(path, \"rb\") as f:\n",
    "            up = openai_client.files.create(file=f, purpose=UPLOAD_PURPOSE)\n",
    "        file_ids.append(up.id)\n",
    "\n",
    "    return file_ids\n",
    "\n",
    "\n",
    "def attach_files_to_vector_store(vector_store_id: str, file_ids: list[str]):\n",
    "    \"\"\"\n",
    "    Attach uploaded files to a vector store using a file batch.\n",
    "    \"\"\"\n",
    "    batch = openai_client.vector_stores.file_batches.create(\n",
    "        vector_store_id=vector_store_id,\n",
    "        file_ids=file_ids\n",
    "    )\n",
    "    print(f\"‚úÖ File batch created: {batch.id} (status={batch.status})\")\n",
    "    return batch\n",
    "\n",
    "\n",
    "def poll_file_batch(vector_store_id: str, batch_id: str, poll_interval_sec: int = 2):\n",
    "    \"\"\"\n",
    "    Poll the file batch until it reaches a terminal state.\n",
    "    Terminal states: completed / failed / cancelled\n",
    "    \"\"\"\n",
    "    terminal = {\"completed\", \"failed\", \"cancelled\"}\n",
    "\n",
    "    while True:\n",
    "        b = openai_client.vector_stores.file_batches.retrieve(\n",
    "            vector_store_id=vector_store_id,\n",
    "            batch_id=batch_id\n",
    "        )\n",
    "\n",
    "        if b.status in terminal:\n",
    "            print(f\"‚úÖ Final batch status: {b.status}\")\n",
    "            # file_counts may include completed/failed/in_progress totals\n",
    "            if hasattr(b, \"file_counts\"):\n",
    "                print(\"file_counts:\", b.file_counts)\n",
    "            return b\n",
    "\n",
    "        time.sleep(poll_interval_sec)\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# 1) Create or reuse vector store\n",
    "# ----------------------------\n",
    "vector_store = create_or_reuse_vector_store(VECTOR_STORE_NAME, REUSE_VECTOR_STORE_ID)\n",
    "VECTOR_STORE_ID = vector_store.id\n",
    "\n",
    "# ----------------------------\n",
    "# 2) Upload local artifacts as OpenAI Files\n",
    "# ----------------------------\n",
    "if \"ALL_RAG_FILE_PATHS\" not in globals() or not ALL_RAG_FILE_PATHS:\n",
    "    raise ValueError(\n",
    "        \"ALL_RAG_FILE_PATHS is empty. Run Cell 2 first to export Drive PDFs and Notion RQ txt files.\"\n",
    "    )\n",
    "\n",
    "print(f\"Total local artifacts to upload: {len(ALL_RAG_FILE_PATHS)}\")\n",
    "uploaded_file_ids = upload_files(ALL_RAG_FILE_PATHS)\n",
    "print(f\"‚úÖ Uploaded {len(uploaded_file_ids)} files.\")\n",
    "\n",
    "# ----------------------------\n",
    "# 3) Attach uploaded files to the vector store (file batch)\n",
    "# ----------------------------\n",
    "batch = attach_files_to_vector_store(VECTOR_STORE_ID, uploaded_file_ids)\n",
    "\n",
    "# ----------------------------\n",
    "# 4) Poll until indexing is complete\n",
    "# ----------------------------\n",
    "final_batch = poll_file_batch(VECTOR_STORE_ID, batch.id, poll_interval_sec=POLL_INTERVAL_SEC)\n",
    "\n",
    "# Expose variables for downstream cells\n",
    "VECTOR_STORE_FILE_IDS = uploaded_file_ids\n",
    "print(\"\\n‚úÖ Vector store ready for retrieval.\")\n",
    "print(\"VECTOR_STORE_ID =\", VECTOR_STORE_ID)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "bb08d263-a672-4cc4-a263-a08a94bb296e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Cell 4 : Retrieval logic (top-k semantic search) [PATCHED]\n",
    "# ============================================================\n",
    "\n",
    "from typing import Any, Dict, List, Tuple, Optional\n",
    "import textwrap\n",
    "\n",
    "DEFAULT_TOP_K = int(globals().get(\"TOP_K_RETRIEVAL\", 5))\n",
    "DEFAULT_REWRITE_QUERY = True\n",
    "DEFAULT_MAX_CHARS_PER_CHUNK = 800\n",
    "\n",
    "def _as_dict(x: Any) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Convert SDK objects (pydantic-like) or dataclasses to dict safely.\n",
    "    \"\"\"\n",
    "    if x is None:\n",
    "        return {}\n",
    "    if isinstance(x, dict):\n",
    "        return x\n",
    "    # pydantic v2\n",
    "    if hasattr(x, \"model_dump\"):\n",
    "        try:\n",
    "            return x.model_dump()\n",
    "        except Exception:\n",
    "            pass\n",
    "    # pydantic v1\n",
    "    if hasattr(x, \"dict\"):\n",
    "        try:\n",
    "            return x.dict()\n",
    "        except Exception:\n",
    "            pass\n",
    "    # generic fallback\n",
    "    try:\n",
    "        return dict(x)\n",
    "    except Exception:\n",
    "        pass\n",
    "    try:\n",
    "        return vars(x)\n",
    "    except Exception:\n",
    "        return {}\n",
    "\n",
    "def _get_attr_or_key(obj: Any, key: str, default: Any = None) -> Any:\n",
    "    \"\"\"\n",
    "    Read either obj.key (attribute) or obj[key] (dict).\n",
    "    \"\"\"\n",
    "    if obj is None:\n",
    "        return default\n",
    "    if hasattr(obj, key):\n",
    "        return getattr(obj, key, default)\n",
    "    if isinstance(obj, dict):\n",
    "        return obj.get(key, default)\n",
    "    return default\n",
    "\n",
    "def search_vector_store(\n",
    "    query: str,\n",
    "    top_k: int = DEFAULT_TOP_K,\n",
    "    rewrite_query: bool = DEFAULT_REWRITE_QUERY,\n",
    "    file_ids: Optional[List[str]] = None,\n",
    "    filters: Optional[Dict[str, Any]] = None,\n",
    ") -> Any:\n",
    "    if not query or not isinstance(query, str):\n",
    "        raise ValueError(\"query must be a non-empty string\")\n",
    "\n",
    "    kwargs = {\n",
    "        \"vector_store_id\": VECTOR_STORE_ID,\n",
    "        \"query\": query,\n",
    "        \"max_num_results\": int(top_k),\n",
    "        \"rewrite_query\": bool(rewrite_query),\n",
    "    }\n",
    "    if file_ids:\n",
    "        kwargs[\"file_ids\"] = file_ids\n",
    "    if filters:\n",
    "        kwargs[\"filters\"] = filters\n",
    "\n",
    "    return openai_client.vector_stores.search(**kwargs)\n",
    "\n",
    "def _flatten_content_blocks(content: Any) -> str:\n",
    "    \"\"\"\n",
    "    Flatten `content` blocks to plain text.\n",
    "    Handles both dict blocks and SDK Content objects.\n",
    "    Typical shapes:\n",
    "      - [{\"type\":\"text\",\"text\":\"...\"}]\n",
    "      - [Content(type=\"text\", text=\"...\"), ...]\n",
    "    \"\"\"\n",
    "    if not content:\n",
    "        return \"\"\n",
    "\n",
    "    parts = []\n",
    "    # content might be a list-like of blocks\n",
    "    for block in content:\n",
    "        # block can be dict or object\n",
    "        b_type = _get_attr_or_key(block, \"type\", None)\n",
    "        if b_type == \"text\":\n",
    "            txt = _get_attr_or_key(block, \"text\", \"\")\n",
    "            if txt:\n",
    "                parts.append(str(txt))\n",
    "        else:\n",
    "            # sometimes the text is nested; try dict conversion fallback\n",
    "            bd = _as_dict(block)\n",
    "            if bd.get(\"type\") == \"text\" and bd.get(\"text\"):\n",
    "                parts.append(str(bd[\"text\"]))\n",
    "\n",
    "    return \"\\n\".join(parts).strip()\n",
    "\n",
    "def retrieve_topk_context(\n",
    "    query: str,\n",
    "    top_k: int = DEFAULT_TOP_K,\n",
    "    rewrite_query: bool = DEFAULT_REWRITE_QUERY,\n",
    "    max_chars_per_chunk: int = DEFAULT_MAX_CHARS_PER_CHUNK,\n",
    ") -> Tuple[List[Dict[str, Any]], str]:\n",
    "    res = search_vector_store(query=query, top_k=top_k, rewrite_query=rewrite_query)\n",
    "\n",
    "    evidence: List[Dict[str, Any]] = []\n",
    "    context_blocks: List[str] = []\n",
    "\n",
    "    items = _get_attr_or_key(res, \"data\", []) or []\n",
    "    for i, item in enumerate(items):\n",
    "        filename = _get_attr_or_key(item, \"filename\", None)\n",
    "        file_id = _get_attr_or_key(item, \"file_id\", None)\n",
    "        score = _get_attr_or_key(item, \"score\", None)\n",
    "        content = _get_attr_or_key(item, \"content\", None)\n",
    "        attributes = _get_attr_or_key(item, \"attributes\", None)\n",
    "\n",
    "        text = _flatten_content_blocks(content)\n",
    "        if max_chars_per_chunk and len(text) > max_chars_per_chunk:\n",
    "            text = text[:max_chars_per_chunk].rstrip() + \"‚Ä¶\"\n",
    "\n",
    "        ev = {\n",
    "            \"rank\": i + 1,\n",
    "            \"file_id\": file_id,\n",
    "            \"filename\": filename,\n",
    "            \"score\": score,\n",
    "            \"text\": text,\n",
    "            \"attributes\": _as_dict(attributes) if attributes else {},\n",
    "        }\n",
    "        evidence.append(ev)\n",
    "\n",
    "        context_blocks.append(\n",
    "            f\"[{i+1}] file={filename} (score={score})\\n{text}\"\n",
    "        )\n",
    "\n",
    "    context_str = \"\\n\\n\".join(context_blocks).strip()\n",
    "    return evidence, context_str\n",
    "\n",
    "def pretty_print_results(evidence: List[Dict[str, Any]]) -> None:\n",
    "    if not evidence:\n",
    "        print(\"No results.\")\n",
    "        return\n",
    "\n",
    "    for ev in evidence:\n",
    "        print(\"=\" * 80)\n",
    "        print(f\"Rank: {ev['rank']} | Score: {ev.get('score')} | File: {ev.get('filename')}\")\n",
    "        if ev.get(\"attributes\"):\n",
    "            print(\"Attributes:\", ev[\"attributes\"])\n",
    "        print(\"-\" * 80)\n",
    "        print(textwrap.fill(ev.get(\"text\", \"\"), width=110))\n",
    "    print(\"=\" * 80)\n",
    "\n",
    "# Optional quick test:\n",
    "# evidence, context = retrieve_topk_context(\"VC syndication empirical strategy\", top_k=5)\n",
    "# pretty_print_results(evidence)\n",
    "# print(context[:1200])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b0312e75-0211-4d2f-bd08-d6a4fcef8257",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Cell 5 : Prompt Construction for Research Question Generation\n",
    "# ============================================================\n",
    "#\n",
    "# This cell defines prompt templates and helper functions to construct\n",
    "# an LLM-ready prompt for generating research questions (RQs) grounded\n",
    "# in retrieved evidence from the vector store.\n",
    "#\n",
    "# Output of this cell:\n",
    "# - `build_rq_messages(...)` : returns a list of messages (system + user)\n",
    "# - `build_rq_input(...)`    : returns a single combined user prompt string (optional)\n",
    "#\n",
    "# Requirements (from previous cells):\n",
    "# - retrieve_topk_context(...) (Cell 4) OR an equivalent `context_str`\n",
    "# - LLM_MODEL, TEMPERATURE, MAX_TOKENS (Cell 1)\n",
    "#\n",
    "# Notes:\n",
    "# - We keep the prompt modular to allow easy iteration on tone, format,\n",
    "#   and constraints without touching downstream generation logic.\n",
    "#\n",
    "\n",
    "from typing import Dict, Any, List, Optional\n",
    "import json\n",
    "import textwrap\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# Prompt Settings\n",
    "# ----------------------------\n",
    "N_RQ_CANDIDATES = 5          # how many RQs to generate per run\n",
    "MAX_CONTEXT_CHARS = 8000     # safeguard to avoid overly large prompts\n",
    "STYLE_CONSTRAINTS = [\n",
    "    \"Write ALL research questions and explanations in Japanese.\",\n",
    "    \"Use clear, academic Japanese suitable for research planning.\",\n",
    "    \"Do not mix English into the Japanese text, except for variable names or citations.\",\n",
    "    \"Keep each research question specific, empirical, and testable.\",\n",
    "    \"Avoid overly broad or purely descriptive questions.\",\n",
    "    \"Ground each question in the provided evidence (cite evidence IDs).\",\n",
    "    \"Prefer questions suitable for a PhD-level research agenda.\",\n",
    "]\n",
    "\n",
    "\n",
    "# Optional: a simple rubric to score candidates\n",
    "SCORING_RUBRIC = {\n",
    "    \"novelty\": \"How original / non-obvious is the question given the evidence?\",\n",
    "    \"testability\": \"Can it be empirically tested with plausible data/identification?\",\n",
    "    \"impact\": \"Does answering it matter for theory, policy, or practice?\",\n",
    "    \"feasibility\": \"Is it feasible within a typical research timeline?\",\n",
    "}\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# System Prompt\n",
    "# ----------------------------\n",
    "SYSTEM_PROMPT = \"\"\"You are a careful research assistant helping a researcher develop empirical research questions.\n",
    "You must ground proposals in the provided evidence snippets and avoid hallucinating citations.\n",
    "If evidence is insufficient, propose a narrower question or explicitly request what additional evidence would be needed.\n",
    "\"\"\"\n",
    "\n",
    "# ----------------------------\n",
    "# Output Schema (JSON-only)\n",
    "# ----------------------------\n",
    "# We ask the model to output strict JSON so the notebook can parse results reliably.\n",
    "\n",
    "OUTPUT_SCHEMA_INSTRUCTIONS = \"\"\"Return ONLY valid JSON (no markdown, no backticks).\n",
    "\n",
    "Schema:\n",
    "{\n",
    "  \"seed_idea\": \"<string>\",\n",
    "  \"rq_candidates\": [\n",
    "    {\n",
    "      \"Name\": \"<Êó•Êú¨Ë™û„ÅÆResearch QuestionÔºà1ÊñáÔºâ>\",\n",
    "      \"Rationale / Background\": \"<Êó•Êú¨Ë™û„ÅßÊÑèÁæ©Ôºà2-4ÊñáÔºâ>\",\n",
    "      \"Gap Identified\": \"<Êó•Êú¨Ë™û„Åß„ÇÆ„É£„ÉÉ„Éó/Êñ∞Ë¶èÊÄßÔºà2-4ÊñáÔºâ>\",\n",
    "      \"Proposed Approach\": \"<Êó•Êú¨Ë™û„ÅßÊ§úË®ºÊâãÊ≥ïÔºà2-5ÊñáÔºâ>\",\n",
    "      \"Priority\": \"Medium\",\n",
    "      \"Status\": \"New\",\n",
    "      \"Tags\": [\"<english tag 1>\", \"<english tag 2>\", \"<english tag 3>\", \"<english tag 4>\", \"<english tag 5>\"],\n",
    "      \"evidence_used\": [1, 2]\n",
    "    }\n",
    "  ]\n",
    "}\n",
    "\n",
    "Rules:\n",
    "- Write Name / Rationale / Gap / Proposed Approach in Japanese.\n",
    "- Tags must be English, about 5 items, concise nouns (e.g., \"VC\", \"Syndication\", \"Policy Shock\").\n",
    "- Priority must be exactly \"Medium\".\n",
    "- Status must be exactly \"New\".\n",
    "- evidence_used must reference evidence IDs in the provided context (e.g., [1], [2], ...).\n",
    "- Keep each field concise. Avoid long lists or bullet points.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# Helper: truncate context safely\n",
    "# ----------------------------\n",
    "def truncate_context(context_str: str, max_chars: int = MAX_CONTEXT_CHARS) -> str:\n",
    "    if not context_str:\n",
    "        return \"\"\n",
    "    context_str = context_str.strip()\n",
    "    if len(context_str) <= max_chars:\n",
    "        return context_str\n",
    "    return context_str[:max_chars].rstrip() + \"\\n‚Ä¶(truncated)‚Ä¶\"\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# Main builder: messages (system + user)\n",
    "# ----------------------------\n",
    "def build_rq_messages(\n",
    "    seed_idea: str,\n",
    "    context_str: str,\n",
    "    n_candidates: int = N_RQ_CANDIDATES,\n",
    "    style_constraints: Optional[List[str]] = None,\n",
    "    scoring_rubric: Optional[Dict[str, str]] = None,\n",
    ") -> List[Dict[str, str]]:\n",
    "    \"\"\"\n",
    "    Construct chat messages for RQ generation.\n",
    "\n",
    "    Args:\n",
    "        seed_idea: The researcher's initial idea / interest statement.\n",
    "        context_str: Retrieved evidence string (from Cell 4).\n",
    "        n_candidates: Number of RQ candidates to generate.\n",
    "        style_constraints: Optional list of style constraints.\n",
    "        scoring_rubric: Optional rubric dict for scoring dimensions.\n",
    "\n",
    "    Returns:\n",
    "        messages: [{\"role\":\"system\",\"content\":...}, {\"role\":\"user\",\"content\":...}]\n",
    "    \"\"\"\n",
    "    if not seed_idea or not isinstance(seed_idea, str):\n",
    "        raise ValueError(\"seed_idea must be a non-empty string\")\n",
    "\n",
    "    style_constraints = style_constraints or STYLE_CONSTRAINTS\n",
    "    scoring_rubric = scoring_rubric or SCORING_RUBRIC\n",
    "\n",
    "    context_str = truncate_context(context_str)\n",
    "\n",
    "    user_prompt = f\"\"\"\n",
    "You will generate {n_candidates} candidate research questions based on the seed idea and the evidence.\n",
    "\n",
    "Seed idea:\n",
    "{seed_idea}\n",
    "\n",
    "Style constraints:\n",
    "- \"\"\" + \"\\n- \".join(style_constraints) + f\"\"\"\n",
    "\n",
    "Scoring rubric (1-5 each):\n",
    "{json.dumps(scoring_rubric, indent=2)}\n",
    "\n",
    "Evidence context:\n",
    "{context_str}\n",
    "\n",
    "{OUTPUT_SCHEMA_INSTRUCTIONS}\n",
    "\"\"\".strip()\n",
    "\n",
    "    return [\n",
    "        {\"role\": \"system\", \"content\": SYSTEM_PROMPT.strip()},\n",
    "        {\"role\": \"user\", \"content\": user_prompt},\n",
    "    ]\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# Optional builder: single combined input string\n",
    "# (Useful if you prefer a single `input=` field later.)\n",
    "# ----------------------------\n",
    "def build_rq_input(seed_idea: str, context_str: str) -> str:\n",
    "    msgs = build_rq_messages(seed_idea=seed_idea, context_str=context_str)\n",
    "    # join into a single string (system + user) for debugging/inspection\n",
    "    return \"SYSTEM:\\n\" + msgs[0][\"content\"] + \"\\n\\nUSER:\\n\" + msgs[1][\"content\"]\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# Quick preview (optional)\n",
    "# ----------------------------\n",
    "# seed_idea = \"I want to study how VC syndication affects startup outcomes when policy shocks change investor constraints.\"\n",
    "# evidence, context = retrieve_topk_context(seed_idea, top_k=5)\n",
    "# print(build_rq_input(seed_idea, context)[:2000])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "0edf7eb1-68d1-4226-8c96-92e7660dfa6d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "65353d84e54f40f7b27c2cdf025a75e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Textarea(value='', description='Seed:', layout=Layout(height='120px', width='900px'), placeholder='Á†îÁ©∂„Ç¢„Ç§„Éá„Ç¢„ÇíÂÖ•ÂäõÔºà2‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1f412b79b4224f13b3e6cd28e1ede330",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntSlider(value=5, continuous_update=False, description='Top-k:', max=20, min=1), IntSlider(val‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a8f03e1e5154199ad262f7f00d6b25a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bbb15539556e40ca8143ce981d6adf7b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "80d9f5c7c3104a0f803aa9b17f93ca9a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ============================================================\n",
    "# Cell 6 : LLM-based Question Generation using Retrieved Contexts (DB-aligned)\n",
    "# ============================================================\n",
    "\n",
    "from pydantic import BaseModel, Field\n",
    "from typing import List, Literal\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, clear_output\n",
    "from datetime import datetime\n",
    "import json\n",
    "\n",
    "LLM_MODEL_STRUCTURED = \"gpt-4o-mini\"  # good default for structured parsing\n",
    "\n",
    "# ----------------------------\n",
    "# Pydantic schema aligned with your Notion DB\n",
    "# ----------------------------\n",
    "class RQCandidateNotion(BaseModel):\n",
    "    Name: str\n",
    "    Rationale___Background: str = Field(alias=\"Rationale / Background\")\n",
    "    Gap_Identified: str = Field(alias=\"Gap Identified\")\n",
    "    Proposed_Approach: str = Field(alias=\"Proposed Approach\")\n",
    "    Priority: Literal[\"Medium\"] = \"Medium\"\n",
    "    Status: Literal[\"New\"] = \"New\"\n",
    "    Tags: List[str] = Field(min_length=3, max_length=8)  # \"about 5\" ‚Üí allow some flex\n",
    "    evidence_used: List[int] = []\n",
    "\n",
    "class RQOutputNotion(BaseModel):\n",
    "    seed_idea: str\n",
    "    rq_candidates: List[RQCandidateNotion]\n",
    "\n",
    "# ----------------------------\n",
    "# LLM call (structured parse)\n",
    "# ----------------------------\n",
    "def generate_rqs_structured(seed_idea: str, top_k: int, n_candidates: int, temperature: float):\n",
    "    evidence, context_str = retrieve_topk_context(seed_idea, top_k=top_k)\n",
    "\n",
    "    messages = build_rq_messages(\n",
    "        seed_idea=seed_idea,\n",
    "        context_str=context_str,\n",
    "        n_candidates=n_candidates,\n",
    "    )\n",
    "\n",
    "    resp = openai_client.responses.parse(\n",
    "        model=LLM_MODEL_STRUCTURED,\n",
    "        input=messages,\n",
    "        text_format=RQOutputNotion,\n",
    "        temperature=temperature,\n",
    "        max_output_tokens=MAX_TOKENS,  # MAX_TOKENS„ÅØ 1500„Äú2500 Êé®Â•®\n",
    "    )\n",
    "\n",
    "    parsed: RQOutputNotion = resp.output_parsed\n",
    "    return evidence, context_str, parsed\n",
    "\n",
    "# ----------------------------\n",
    "# UI Widgets\n",
    "# ----------------------------\n",
    "seed_input = widgets.Textarea(\n",
    "    value=\"\",\n",
    "    placeholder=\"Á†îÁ©∂„Ç¢„Ç§„Éá„Ç¢„ÇíÂÖ•ÂäõÔºà2„Äú6Êñá„Åå„Åä„Åô„Åô„ÇÅÔºâ...\",\n",
    "    description=\"Seed:\",\n",
    "    layout=widgets.Layout(width=\"900px\", height=\"120px\")\n",
    ")\n",
    "\n",
    "topk_slider = widgets.IntSlider(\n",
    "    value=int(globals().get(\"TOP_K_RETRIEVAL\", 5)),\n",
    "    min=1, max=20, step=1,\n",
    "    description=\"Top-k:\",\n",
    "    continuous_update=False\n",
    ")\n",
    "\n",
    "temp_slider = widgets.FloatSlider(\n",
    "    value=float(globals().get(\"TEMPERATURE\", 0.7)),\n",
    "    min=0.0, max=1.2, step=0.1,\n",
    "    description=\"Temp:\",\n",
    "    continuous_update=False\n",
    ")\n",
    "\n",
    "n_rq_slider = widgets.IntSlider(\n",
    "    value=int(globals().get(\"N_RQ_CANDIDATES\", 5)),\n",
    "    min=3, max=12, step=1,\n",
    "    description=\"#RQs:\",\n",
    "    continuous_update=False\n",
    ")\n",
    "\n",
    "generate_btn = widgets.Button(description=\"Retrieve + Generate (Notion format)\", button_style=\"primary\")\n",
    "\n",
    "out_retrieval = widgets.Output()\n",
    "out_generation = widgets.Output()\n",
    "out_review = widgets.Output()\n",
    "\n",
    "display(seed_input)\n",
    "display(widgets.HBox([topk_slider, n_rq_slider, temp_slider, generate_btn]))\n",
    "display(out_retrieval, out_generation, out_review)\n",
    "\n",
    "# ----------------------------\n",
    "# Review UI (Go/No-Go + manual edit)\n",
    "# ----------------------------\n",
    "_go_widgets = []\n",
    "_edit_widgets = []\n",
    "\n",
    "def build_review_ui(rq_candidates: List[dict]):\n",
    "    _go_widgets.clear()\n",
    "    _edit_widgets.clear()\n",
    "    rows = []\n",
    "\n",
    "    for i, rq in enumerate(rq_candidates):\n",
    "        name = rq.get(\"Name\", \"\")\n",
    "        pr = rq.get(\"Priority\", \"\")\n",
    "        st = rq.get(\"Status\", \"\")\n",
    "        tags = rq.get(\"Tags\", [])\n",
    "\n",
    "        header = widgets.HTML(\n",
    "            value=f\"<b>RQ-{i+1:02d}</b> &nbsp; <span style='color:#666'>Priority={pr} | Status={st} | Tags={tags}</span><br/>{name}\"\n",
    "        )\n",
    "\n",
    "        go = widgets.ToggleButtons(\n",
    "            options=[(\"Go\", True), (\"No-Go\", False)],\n",
    "            value=False,\n",
    "            description=\"Decision:\"\n",
    "        )\n",
    "\n",
    "        edit = widgets.Textarea(\n",
    "            value=json.dumps(rq, ensure_ascii=False, indent=2),\n",
    "            description=\"Edit JSON:\",\n",
    "            layout=widgets.Layout(width=\"900px\", height=\"220px\")\n",
    "        )\n",
    "\n",
    "        _go_widgets.append(go)\n",
    "        _edit_widgets.append(edit)\n",
    "        rows.append(widgets.VBox([header, go, edit]))\n",
    "\n",
    "    return widgets.VBox(rows)\n",
    "\n",
    "def collect_selected_rqs() -> pd.DataFrame:\n",
    "    selected = []\n",
    "    for go_w, edit_w in zip(_go_widgets, _edit_widgets):\n",
    "        if not go_w.value:\n",
    "            continue\n",
    "        selected.append(json.loads(edit_w.value))\n",
    "    return pd.DataFrame(selected)\n",
    "\n",
    "def on_generate_clicked(_):\n",
    "    with out_retrieval:\n",
    "        clear_output()\n",
    "        print(\"Running retrieval...\")\n",
    "\n",
    "    with out_generation:\n",
    "        clear_output()\n",
    "        print(\"Calling the LLM (DB-aligned structured parse)...\")\n",
    "\n",
    "    with out_review:\n",
    "        clear_output()\n",
    "\n",
    "    seed_idea = seed_input.value.strip()\n",
    "    if not seed_idea:\n",
    "        with out_retrieval:\n",
    "            clear_output()\n",
    "            print(\"‚ùå Please enter a seed idea.\")\n",
    "        return\n",
    "\n",
    "    try:\n",
    "        evidence, context_str, parsed = generate_rqs_structured(\n",
    "            seed_idea=seed_idea,\n",
    "            top_k=int(topk_slider.value),\n",
    "            n_candidates=int(n_rq_slider.value),\n",
    "            temperature=float(temp_slider.value),\n",
    "        )\n",
    "\n",
    "        with out_retrieval:\n",
    "            clear_output()\n",
    "            print(\"‚úÖ Retrieval completed.\")\n",
    "            pretty_print_results(evidence)\n",
    "\n",
    "        # Convert parsed to list[dict] using aliases (original Notion-like keys)\n",
    "        rqs_list = [c.model_dump(by_alias=True) for c in parsed.rq_candidates]\n",
    "\n",
    "        with out_generation:\n",
    "            clear_output()\n",
    "            print(\"‚úÖ Generation completed (parsed & validated).\")\n",
    "            print(f\"Candidates: {len(rqs_list)}\")\n",
    "            print(\"Preview of first candidate:\")\n",
    "            print(json.dumps(rqs_list[0], ensure_ascii=False, indent=2)[:2000])\n",
    "\n",
    "        review_ui = build_review_ui(rqs_list)\n",
    "        save_btn = widgets.Button(description=\"Save Go-approved (as rq_selected_df)\", button_style=\"success\")\n",
    "        save_out = widgets.Output()\n",
    "\n",
    "        def on_save_clicked(__):\n",
    "            with save_out:\n",
    "                clear_output()\n",
    "                df_sel = collect_selected_rqs()\n",
    "                if df_sel.empty:\n",
    "                    print(\"No Go-approved RQs selected.\")\n",
    "                    return\n",
    "\n",
    "                global rq_selected_df, rq_df\n",
    "                rq_df = pd.DataFrame(rqs_list)\n",
    "                rq_selected_df = df_sel\n",
    "\n",
    "                ts = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "                path = OUTPUT_DIR / f\"rq_selected_{ts}.csv\"\n",
    "                df_sel.to_csv(path, index=False)\n",
    "                print(f\"‚úÖ Saved {len(df_sel)} selected RQs to: {path}\")\n",
    "                display(df_sel)\n",
    "\n",
    "        save_btn.on_click(on_save_clicked)\n",
    "\n",
    "        with out_review:\n",
    "            clear_output()\n",
    "            display(review_ui)\n",
    "            display(save_btn, save_out)\n",
    "\n",
    "    except Exception as e:\n",
    "        with out_generation:\n",
    "            clear_output()\n",
    "            print(\"‚ùå Error during generation:\", type(e).__name__, str(e))\n",
    "\n",
    "generate_btn.on_click(on_generate_clicked)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "269043e9-0eab-45b2-9dd8-93d0dcd4da3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Post-processing complete.\n",
      "Candidates (raw): 5\n",
      "After filters:    5\n",
      "After dedup:      5\n",
      "Removed:          0\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>Rationale / Background</th>\n",
       "      <th>Gap Identified</th>\n",
       "      <th>Proposed Approach</th>\n",
       "      <th>Priority</th>\n",
       "      <th>Status</th>\n",
       "      <th>Tags</th>\n",
       "      <th>evidence_used</th>\n",
       "      <th>evidence_count</th>\n",
       "      <th>tag_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CVC„Å®IVC„ÅÆÊäïË≥áË°åÂãï„ÅÆÈÅï„ÅÑ„Åå‰ºÅÊ•≠„ÅÆÂ≠¶ÁøíÊàêÊûú„Å´‰∏é„Åà„ÇãÂΩ±Èüø„ÅØ‰Ωï„ÅãÔºü</td>\n",
       "      <td>CVC„Å®IVC„ÅÆÊäïË≥áË°åÂãï„Å´„ÅØÊòéÁ¢∫„Å™ÈÅï„ÅÑ„Åå„ÅÇ„Çä„ÄÅ„Åù„Çå„Åå‰ºÅÊ•≠„ÅÆÂ≠¶Áøí„ÇÑÊàêÈï∑„Å´„Å©„ÅÆ„Çà„ÅÜ„Å´ÂΩ±Èüø„Åô„Çã„Åã„ÇíÁêÜËß£„Åô„Çã„Åì„Å®„ÅØÈáçË¶Å„Åß„ÅÇ„Çã„ÄÇÁâπ„Å´„ÄÅÂ≠¶Áøí„ÅÆË¶≥ÁÇπ„Åã„Çâ‰∏°ËÄÖ„ÅÆÈÅï„ÅÑ„ÇíÊòéÁ¢∫„Å´„Åô„Çã„Åì„Å®„Åß„ÄÅ„Çà„ÇäÂäπÊûúÁöÑ„Å™ÊäïË≥áÊà¶Áï•„ÅåË¶ã„Åà„Å¶„Åè„Çã</td>\n",
       "      <td>CVC„Å®IVC„ÅÆÊäïË≥áË°åÂãï„ÅÆÈÅï„ÅÑ„Åå‰ºÅÊ•≠„ÅÆÂ≠¶ÁøíÊàêÊûú„Å´‰∏é„Åà„ÇãÂΩ±Èüø„Å´Èñ¢„Åô„ÇãÂÆüË®ºÁ†îÁ©∂„ÅØÈôê„Çâ„Çå„Å¶„Åä„Çä„ÄÅ„Åù„ÅÆ„É°„Ç´„Éã„Ç∫„É†„Åå‰∏çÊòé„Åß„ÅÇ„Çã</td>\n",
       "      <td>CVC„Åä„Çà„Å≥IVC„Åã„ÇâË≥áÈáë„ÇíÂèó„Åë„Åü‰ºÅÊ•≠„ÅÆ„Ç§„Éé„Éô„Éº„Ç∑„Éß„É≥ÊàêÊûú„ÇíÊØîËºÉ„Åô„Çã„Åì„Å®„Åß„ÄÅ‰∏°ËÄÖ„ÅÆÊäïË≥áË°åÂãï„ÅÆÈÅï„ÅÑ„Åå‰ºÅÊ•≠„ÅÆÂ≠¶ÁøíÊàêÊûú„Å´‰∏é„Åà„ÇãÂΩ±Èüø„ÇíÂàÜÊûê„Åô„Çã</td>\n",
       "      <td>Medium</td>\n",
       "      <td>New</td>\n",
       "      <td>[CVC, IVC, Learning Outcomes, Investment Behavior, Innovation]</td>\n",
       "      <td>[1, 2]</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CVC„ÅÆÂÆüÊñΩ„Åå‰ºÅÊ•≠„ÅÆÈï∑ÊúüÁöÑ„Å™ÊàêÈï∑„Å´‰∏é„Åà„ÇãÂΩ±Èüø„ÅØ‰Ωï„ÅãÔºü</td>\n",
       "      <td>CVC„ÅÆÂΩπÂâ≤„ÅØÁü≠ÊúüÁöÑ„Å™ÊäïË≥áÂà©Áõä„Å†„Åë„Åß„Å™„Åè„ÄÅ‰ºÅÊ•≠„ÅÆÈï∑ÊúüÁöÑ„Å™ÊàêÈï∑„Å´„ÇÇÂΩ±Èüø„Çí‰∏é„Åà„Çã„ÄÇÊú¨Á†îÁ©∂„ÅØ„Åù„ÅÆ„É°„Ç´„Éã„Ç∫„É†„ÇíÊòé„Çâ„Åã„Å´„Åô„Çã„Åì„Å®„Åß„ÄÅCVC„ÅÆ‰æ°ÂÄ§„ÇíÂÜçË©ï‰æ°„Åô„Çã„Åì„Å®„ÇíÁõÆÁöÑ„Å®„Åô„Çã</td>\n",
       "      <td>CVC„ÅÆÂÆüÊñΩ„Åå‰ºÅÊ•≠„ÅÆÈï∑ÊúüÁöÑ„Å™ÊàêÈï∑„Å´‰∏é„Åà„ÇãÂΩ±Èüø„Å´„Å§„ÅÑ„Å¶„ÅÆÂÆüË®ºÁ†îÁ©∂„ÅØÈôê„Çâ„Çå„Å¶„Åä„Çä„ÄÅÁâπ„Å´„Åù„ÅÆ„É°„Ç´„Éã„Ç∫„É†„Åå‰∏çÊòé„Åß„ÅÇ„Çã</td>\n",
       "      <td>CVC„ÇíÂÆüÊñΩ„Åó„Åü‰ºÅÊ•≠„ÅÆÊàêÈï∑ÊåáÊ®ô„ÇíËøΩË∑°„Åó„ÄÅCVC„ÅÆÂÆüÊñΩ„ÅåÈï∑ÊúüÁöÑ„Å™ÊàêÈï∑„Å´„Å©„ÅÆ„Çà„ÅÜ„Å´ÂØÑ‰∏é„Åô„Çã„Åã„ÇíÂàÜÊûê„Åô„Çã„ÄÇÂÆöÈáèÁöÑ„Éá„Éº„Çø„Å®Ë≥™ÁöÑ„Ç§„É≥„Çø„Éì„É•„Éº„Çí‰ΩµÁî®„Åô„Çã</td>\n",
       "      <td>Medium</td>\n",
       "      <td>New</td>\n",
       "      <td>[CVC, Long-term Growth, Investment Impact, Qualitative Research, Quantitative Research]</td>\n",
       "      <td>[1, 2]</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CVC„ÅÆÊàêÈï∑ÈÅéÁ®ã„Å´„Åä„Åë„ÇãÂ≠¶ÁøíÂäπÊûú„ÅØ„ÄÅ„Éï„Ç°„Ç§„Éä„É≥„Ç∑„É£„É´„É™„Çø„Éº„É≥ÈáçË¶ñ„ÅÆÂßøÂã¢„Å´„Çà„Å£„Å¶„Å©„ÅÆ„Çà„ÅÜ„Å´Â§âÂåñ„Åô„Çã„ÅãÔºü</td>\n",
       "      <td>CVC„ÅØÂçò„Å™„ÇãË≥áÈáëÊèê‰æõ„Å´Áïô„Åæ„Çâ„Åö„ÄÅ‰ºÅÊ•≠„ÅÆÂ≠¶Áøí„ÇÑÊàêÈï∑„Å´ÂØÑ‰∏é„Åô„ÇãÈáçË¶Å„Å™Ë¶ÅÁ¥†„Åß„ÅÇ„Çã„ÄÇÊú¨Á†îÁ©∂„ÅØ„Éï„Ç°„Ç§„Éä„É≥„Ç∑„É£„É´„É™„Çø„Éº„É≥ÈáçË¶ñ„ÅåCVC„ÅÆÊàêÈï∑ÈÅéÁ®ã„Å´‰∏é„Åà„ÇãÂΩ±Èüø„ÇíÊòé„Çâ„Åã„Å´„Åô„Çã„Åì„Å®„Åß„ÄÅÂÆüÂãôËÄÖ„Å´„Å®„Å£„Å¶„ÅÆÁ§∫ÂîÜ„ÇíÊèê‰æõ„Åô„Çã„Åì„Å®„ÇíÁõÆÊåá„Åô</td>\n",
       "      <td>ÂæìÊù•„ÅÆÁ†îÁ©∂„ÅØCVC„ÅÆÂ≠¶ÁøíÈÅéÁ®ã„Å´„Åä„Åë„Çã„Éï„Ç°„Ç§„Éä„É≥„Ç∑„É£„É´„É™„Çø„Éº„É≥„ÅÆÂΩ±Èüø„ÇíÂçÅÂàÜ„Å´ËÄÉÂØü„Åó„Å¶„Åä„Çâ„Åö„ÄÅ„Åù„ÅÆ„É°„Ç´„Éã„Ç∫„É†„Åå‰∏çÊòéÁ¢∫„Åß„ÅÇ„Çã„ÄÇÁâπ„Å´„ÄÅ„Éï„Ç°„Ç§„Éä„É≥„Ç∑„É£„É´„É™„Çø„Éº„É≥ÈáçË¶ñ„Åå‰ºÅÊ•≠„ÅÆÊé¢Á¥¢Ë°åÂãï„Å´‰∏é„Åà„ÇãÂΩ±Èüø„Å´Èñ¢„Åô„ÇãÂÆüË®ºÁ†îÁ©∂„ÅØ‰∏çË∂≥„Åó„Å¶„ÅÑ„Çã</td>\n",
       "      <td>CVC„ÇíÂà©Áî®„Åô„Çã‰ºÅÊ•≠„ÇíÂØæË±°„Å´„ÄÅ„Éï„Ç°„Ç§„Éä„É≥„Ç∑„É£„É´„É™„Çø„Éº„É≥ÈáçË¶ñÂ∫¶„Å®Â≠¶ÁøíÊàêÊûú„ÅÆÁõ∏Èñ¢„ÇíÂ§öÂ§âÈáèÂàÜÊûê„ÇíÁî®„ÅÑ„Å¶Ê§úË®º„Åô„Çã„ÄÇ„Éá„Éº„Çø„Å®„Åó„Å¶„ÅØ„ÄÅCVCÊäïË≥áÂæå„ÅÆ„Ç§„Éé„Éô„Éº„Ç∑„Éß„É≥ÊàêÊûú„ÇÑÊàêÈï∑ÊåáÊ®ô„ÇíÁî®„ÅÑ„Çã‰∫àÂÆö„Åß„ÅÇ„Çã</td>\n",
       "      <td>Medium</td>\n",
       "      <td>New</td>\n",
       "      <td>[CVC, Learning, Financial Returns, Innovation, Growth]</td>\n",
       "      <td>[1, 2]</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>CVC„ÅÆÊäïË≥áÊàêÂäü„Å´„Åä„Åë„ÇãÂà∂Â∫¶Áí∞Â¢É„ÅÆÂΩπÂâ≤„ÅØ„Å©„ÅÆ„Çà„ÅÜ„Å™„ÇÇ„ÅÆ„ÅãÔºü</td>\n",
       "      <td>Âà∂Â∫¶Áí∞Â¢É„ÅØCVC„ÅÆÊäïË≥áÊàêÂäü„Å´ÂΩ±Èüø„ÇíÂèä„Åº„ÅôÈáçË¶Å„Å™Ë¶ÅÂõ†„Åß„ÅÇ„Çã„ÄÇÊú¨Á†îÁ©∂„ÅØÂà∂Â∫¶Áí∞Â¢É„ÅåCVC„ÅÆÊäïË≥áÊàêÊûú„Å´‰∏é„Åà„ÇãÂΩ±Èüø„ÇíÊòé„Çâ„Åã„Å´„Åô„Çã„Åì„Å®„Åß„ÄÅÊîøÁ≠ñÊèêË®Ä„Å´ÂØÑ‰∏é„Åô„Çã„Åì„Å®„ÇíÁõÆÊåá„Åô</td>\n",
       "      <td>Âà∂Â∫¶Áí∞Â¢É„ÅåCVC„ÅÆÊäïË≥áÊàêÂäü„Å´‰∏é„Åà„ÇãÂΩ±Èüø„Å´„Å§„ÅÑ„Å¶„ÅÆÂÆüË®ºÁ†îÁ©∂„Åå‰∏çË∂≥„Åó„Å¶„Åä„Çä„ÄÅÁâπ„Å´Âà∂Â∫¶„ÅÆË≥™„Å®ÊäïË≥áÊàêÊûú„ÅÆÈñ¢‰øÇ„Åå‰∏çÊòé„Åß„ÅÇ„Çã</td>\n",
       "      <td>ÂõΩ„ÇÑÂú∞Âüü„ÅÆÂà∂Â∫¶Áí∞Â¢ÉÊåáÊ®ô„ÇíÁî®„ÅÑ„Å¶„ÄÅCVC„ÅÆÊäïË≥áÊàêÂäü„Å®„ÅÆÈñ¢‰øÇ„ÇíÂ§öÂ§âÈáèÂõûÂ∏∞ÂàÜÊûê„ÅßÊ§úË®º„Åô„Çã„ÄÇÂà∂Â∫¶Áí∞Â¢É„ÅÆË≥™„ÇíÊìç‰ΩúÂ§âÊï∞„Å®„Åó„Å¶ËÄÉÊÖÆ„Åô„Çã</td>\n",
       "      <td>Medium</td>\n",
       "      <td>New</td>\n",
       "      <td>[CVC, Institutional Environment, Investment Success, Policy Implications, Quantitative Analysis]</td>\n",
       "      <td>[1, 2]</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>CVC„ÅÆÊäïË≥áÊàêÊûú„Å´„Åä„Åë„Çã„Éï„Ç°„Ç§„Éä„É≥„Ç∑„É£„É´„É™„Çø„Éº„É≥ÈáçË¶ñ„ÅÆÂΩ±Èüø„ÇíÂú∞ÂüüÂ∑Æ„ÅßÂàÜÊûê„Åô„Çã„Åì„Å®„ÅØÂèØËÉΩ„ÅãÔºü</td>\n",
       "      <td>Âú∞Âüü„Å´„Çà„Å£„Å¶Ëµ∑Ê•≠Áí∞Â¢É„ÇÑÊñáÂåñ„ÅåÁï∞„Å™„Çã„Åü„ÇÅ„ÄÅCVC„ÅÆÊäïË≥áÊàêÊûú„Å´„ÇÇÂΩ±Èüø„Çí‰∏é„Åà„Çã„ÄÇÂú∞ÂüüÂ∑Æ„ÇíËÄÉÊÖÆ„Åô„Çã„Åì„Å®„Åß„ÄÅ„Çà„ÇäÂÖ∑‰ΩìÁöÑ„Å™Êà¶Áï•„ÇÑÊîøÁ≠ñÊèêË®Ä„ÅåÂèØËÉΩ„Å´„Å™„Çã</td>\n",
       "      <td>CVC„ÅÆÊäïË≥áÊàêÊûú„Å®Âú∞ÂüüÁâπÊÄß„ÅÆÈñ¢ÈÄ£ÊÄß„Å´„Å§„ÅÑ„Å¶„ÅÆÂÆüË®ºÁ†îÁ©∂„Åå‰∏çË∂≥„Åó„Å¶„Åä„Çä„ÄÅÁâπ„Å´„Éï„Ç°„Ç§„Éä„É≥„Ç∑„É£„É´„É™„Çø„Éº„É≥ÈáçË¶ñ„ÅÆÂΩ±Èüø„ÇíÂú∞Âüü„Åî„Å®„Å´ÂàÜÊûê„Åó„ÅüÁ†îÁ©∂„ÅØÁöÜÁÑ°„Åß„ÅÇ„Çã</td>\n",
       "      <td>Âú∞ÂüüÂà•„ÅÆCVCÊäïË≥á„Éá„Éº„Çø„ÇíÁî®„ÅÑ„Å¶„ÄÅ„Éï„Ç°„Ç§„Éä„É≥„Ç∑„É£„É´„É™„Çø„Éº„É≥ÈáçË¶ñ„ÅÆÁ®ãÂ∫¶„Å®ÊäïË≥áÊàêÊûú„Å®„ÅÆÈñ¢‰øÇ„ÇíÂ§öÂ§âÈáèÂõûÂ∏∞ÂàÜÊûê„ÅßÊ§úË®º„Åô„Çã„ÄÇÂú∞ÂüüÁâπÊÄßÊåáÊ®ô„Çí‰∫§Áµ°Âõ†Â≠ê„Å®„Åó„Å¶ËÄÉÊÖÆ„Åô„Çã</td>\n",
       "      <td>Medium</td>\n",
       "      <td>New</td>\n",
       "      <td>[CVC, Regional Differences, Financial Returns, Investment Outcomes, Policy Analysis]</td>\n",
       "      <td>[1, 2]</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 Name  \\\n",
       "0                   CVC„Å®IVC„ÅÆÊäïË≥áË°åÂãï„ÅÆÈÅï„ÅÑ„Åå‰ºÅÊ•≠„ÅÆÂ≠¶ÁøíÊàêÊûú„Å´‰∏é„Åà„ÇãÂΩ±Èüø„ÅØ‰Ωï„ÅãÔºü   \n",
       "1                          CVC„ÅÆÂÆüÊñΩ„Åå‰ºÅÊ•≠„ÅÆÈï∑ÊúüÁöÑ„Å™ÊàêÈï∑„Å´‰∏é„Åà„ÇãÂΩ±Èüø„ÅØ‰Ωï„ÅãÔºü   \n",
       "2  CVC„ÅÆÊàêÈï∑ÈÅéÁ®ã„Å´„Åä„Åë„ÇãÂ≠¶ÁøíÂäπÊûú„ÅØ„ÄÅ„Éï„Ç°„Ç§„Éä„É≥„Ç∑„É£„É´„É™„Çø„Éº„É≥ÈáçË¶ñ„ÅÆÂßøÂã¢„Å´„Çà„Å£„Å¶„Å©„ÅÆ„Çà„ÅÜ„Å´Â§âÂåñ„Åô„Çã„ÅãÔºü   \n",
       "3                       CVC„ÅÆÊäïË≥áÊàêÂäü„Å´„Åä„Åë„ÇãÂà∂Â∫¶Áí∞Â¢É„ÅÆÂΩπÂâ≤„ÅØ„Å©„ÅÆ„Çà„ÅÜ„Å™„ÇÇ„ÅÆ„ÅãÔºü   \n",
       "4       CVC„ÅÆÊäïË≥áÊàêÊûú„Å´„Åä„Åë„Çã„Éï„Ç°„Ç§„Éä„É≥„Ç∑„É£„É´„É™„Çø„Éº„É≥ÈáçË¶ñ„ÅÆÂΩ±Èüø„ÇíÂú∞ÂüüÂ∑Æ„ÅßÂàÜÊûê„Åô„Çã„Åì„Å®„ÅØÂèØËÉΩ„ÅãÔºü   \n",
       "\n",
       "                                                                                     Rationale / Background  \\\n",
       "0      CVC„Å®IVC„ÅÆÊäïË≥áË°åÂãï„Å´„ÅØÊòéÁ¢∫„Å™ÈÅï„ÅÑ„Åå„ÅÇ„Çä„ÄÅ„Åù„Çå„Åå‰ºÅÊ•≠„ÅÆÂ≠¶Áøí„ÇÑÊàêÈï∑„Å´„Å©„ÅÆ„Çà„ÅÜ„Å´ÂΩ±Èüø„Åô„Çã„Åã„ÇíÁêÜËß£„Åô„Çã„Åì„Å®„ÅØÈáçË¶Å„Åß„ÅÇ„Çã„ÄÇÁâπ„Å´„ÄÅÂ≠¶Áøí„ÅÆË¶≥ÁÇπ„Åã„Çâ‰∏°ËÄÖ„ÅÆÈÅï„ÅÑ„ÇíÊòéÁ¢∫„Å´„Åô„Çã„Åì„Å®„Åß„ÄÅ„Çà„ÇäÂäπÊûúÁöÑ„Å™ÊäïË≥áÊà¶Áï•„ÅåË¶ã„Åà„Å¶„Åè„Çã   \n",
       "1                         CVC„ÅÆÂΩπÂâ≤„ÅØÁü≠ÊúüÁöÑ„Å™ÊäïË≥áÂà©Áõä„Å†„Åë„Åß„Å™„Åè„ÄÅ‰ºÅÊ•≠„ÅÆÈï∑ÊúüÁöÑ„Å™ÊàêÈï∑„Å´„ÇÇÂΩ±Èüø„Çí‰∏é„Åà„Çã„ÄÇÊú¨Á†îÁ©∂„ÅØ„Åù„ÅÆ„É°„Ç´„Éã„Ç∫„É†„ÇíÊòé„Çâ„Åã„Å´„Åô„Çã„Åì„Å®„Åß„ÄÅCVC„ÅÆ‰æ°ÂÄ§„ÇíÂÜçË©ï‰æ°„Åô„Çã„Åì„Å®„ÇíÁõÆÁöÑ„Å®„Åô„Çã   \n",
       "2  CVC„ÅØÂçò„Å™„ÇãË≥áÈáëÊèê‰æõ„Å´Áïô„Åæ„Çâ„Åö„ÄÅ‰ºÅÊ•≠„ÅÆÂ≠¶Áøí„ÇÑÊàêÈï∑„Å´ÂØÑ‰∏é„Åô„ÇãÈáçË¶Å„Å™Ë¶ÅÁ¥†„Åß„ÅÇ„Çã„ÄÇÊú¨Á†îÁ©∂„ÅØ„Éï„Ç°„Ç§„Éä„É≥„Ç∑„É£„É´„É™„Çø„Éº„É≥ÈáçË¶ñ„ÅåCVC„ÅÆÊàêÈï∑ÈÅéÁ®ã„Å´‰∏é„Åà„ÇãÂΩ±Èüø„ÇíÊòé„Çâ„Åã„Å´„Åô„Çã„Åì„Å®„Åß„ÄÅÂÆüÂãôËÄÖ„Å´„Å®„Å£„Å¶„ÅÆÁ§∫ÂîÜ„ÇíÊèê‰æõ„Åô„Çã„Åì„Å®„ÇíÁõÆÊåá„Åô   \n",
       "3                            Âà∂Â∫¶Áí∞Â¢É„ÅØCVC„ÅÆÊäïË≥áÊàêÂäü„Å´ÂΩ±Èüø„ÇíÂèä„Åº„ÅôÈáçË¶Å„Å™Ë¶ÅÂõ†„Åß„ÅÇ„Çã„ÄÇÊú¨Á†îÁ©∂„ÅØÂà∂Â∫¶Áí∞Â¢É„ÅåCVC„ÅÆÊäïË≥áÊàêÊûú„Å´‰∏é„Åà„ÇãÂΩ±Èüø„ÇíÊòé„Çâ„Åã„Å´„Åô„Çã„Åì„Å®„Åß„ÄÅÊîøÁ≠ñÊèêË®Ä„Å´ÂØÑ‰∏é„Åô„Çã„Åì„Å®„ÇíÁõÆÊåá„Åô   \n",
       "4                                      Âú∞Âüü„Å´„Çà„Å£„Å¶Ëµ∑Ê•≠Áí∞Â¢É„ÇÑÊñáÂåñ„ÅåÁï∞„Å™„Çã„Åü„ÇÅ„ÄÅCVC„ÅÆÊäïË≥áÊàêÊûú„Å´„ÇÇÂΩ±Èüø„Çí‰∏é„Åà„Çã„ÄÇÂú∞ÂüüÂ∑Æ„ÇíËÄÉÊÖÆ„Åô„Çã„Åì„Å®„Åß„ÄÅ„Çà„ÇäÂÖ∑‰ΩìÁöÑ„Å™Êà¶Áï•„ÇÑÊîøÁ≠ñÊèêË®Ä„ÅåÂèØËÉΩ„Å´„Å™„Çã   \n",
       "\n",
       "                                                                                               Gap Identified  \\\n",
       "0                                                  CVC„Å®IVC„ÅÆÊäïË≥áË°åÂãï„ÅÆÈÅï„ÅÑ„Åå‰ºÅÊ•≠„ÅÆÂ≠¶ÁøíÊàêÊûú„Å´‰∏é„Åà„ÇãÂΩ±Èüø„Å´Èñ¢„Åô„ÇãÂÆüË®ºÁ†îÁ©∂„ÅØÈôê„Çâ„Çå„Å¶„Åä„Çä„ÄÅ„Åù„ÅÆ„É°„Ç´„Éã„Ç∫„É†„Åå‰∏çÊòé„Åß„ÅÇ„Çã   \n",
       "1                                                      CVC„ÅÆÂÆüÊñΩ„Åå‰ºÅÊ•≠„ÅÆÈï∑ÊúüÁöÑ„Å™ÊàêÈï∑„Å´‰∏é„Åà„ÇãÂΩ±Èüø„Å´„Å§„ÅÑ„Å¶„ÅÆÂÆüË®ºÁ†îÁ©∂„ÅØÈôê„Çâ„Çå„Å¶„Åä„Çä„ÄÅÁâπ„Å´„Åù„ÅÆ„É°„Ç´„Éã„Ç∫„É†„Åå‰∏çÊòé„Åß„ÅÇ„Çã   \n",
       "2  ÂæìÊù•„ÅÆÁ†îÁ©∂„ÅØCVC„ÅÆÂ≠¶ÁøíÈÅéÁ®ã„Å´„Åä„Åë„Çã„Éï„Ç°„Ç§„Éä„É≥„Ç∑„É£„É´„É™„Çø„Éº„É≥„ÅÆÂΩ±Èüø„ÇíÂçÅÂàÜ„Å´ËÄÉÂØü„Åó„Å¶„Åä„Çâ„Åö„ÄÅ„Åù„ÅÆ„É°„Ç´„Éã„Ç∫„É†„Åå‰∏çÊòéÁ¢∫„Åß„ÅÇ„Çã„ÄÇÁâπ„Å´„ÄÅ„Éï„Ç°„Ç§„Éä„É≥„Ç∑„É£„É´„É™„Çø„Éº„É≥ÈáçË¶ñ„Åå‰ºÅÊ•≠„ÅÆÊé¢Á¥¢Ë°åÂãï„Å´‰∏é„Åà„ÇãÂΩ±Èüø„Å´Èñ¢„Åô„ÇãÂÆüË®ºÁ†îÁ©∂„ÅØ‰∏çË∂≥„Åó„Å¶„ÅÑ„Çã   \n",
       "3                                                    Âà∂Â∫¶Áí∞Â¢É„ÅåCVC„ÅÆÊäïË≥áÊàêÂäü„Å´‰∏é„Åà„ÇãÂΩ±Èüø„Å´„Å§„ÅÑ„Å¶„ÅÆÂÆüË®ºÁ†îÁ©∂„Åå‰∏çË∂≥„Åó„Å¶„Åä„Çä„ÄÅÁâπ„Å´Âà∂Â∫¶„ÅÆË≥™„Å®ÊäïË≥áÊàêÊûú„ÅÆÈñ¢‰øÇ„Åå‰∏çÊòé„Åß„ÅÇ„Çã   \n",
       "4                                     CVC„ÅÆÊäïË≥áÊàêÊûú„Å®Âú∞ÂüüÁâπÊÄß„ÅÆÈñ¢ÈÄ£ÊÄß„Å´„Å§„ÅÑ„Å¶„ÅÆÂÆüË®ºÁ†îÁ©∂„Åå‰∏çË∂≥„Åó„Å¶„Åä„Çä„ÄÅÁâπ„Å´„Éï„Ç°„Ç§„Éä„É≥„Ç∑„É£„É´„É™„Çø„Éº„É≥ÈáçË¶ñ„ÅÆÂΩ±Èüø„ÇíÂú∞Âüü„Åî„Å®„Å´ÂàÜÊûê„Åó„ÅüÁ†îÁ©∂„ÅØÁöÜÁÑ°„Åß„ÅÇ„Çã   \n",
       "\n",
       "                                                                             Proposed Approach  \\\n",
       "0                          CVC„Åä„Çà„Å≥IVC„Åã„ÇâË≥áÈáë„ÇíÂèó„Åë„Åü‰ºÅÊ•≠„ÅÆ„Ç§„Éé„Éô„Éº„Ç∑„Éß„É≥ÊàêÊûú„ÇíÊØîËºÉ„Åô„Çã„Åì„Å®„Åß„ÄÅ‰∏°ËÄÖ„ÅÆÊäïË≥áË°åÂãï„ÅÆÈÅï„ÅÑ„Åå‰ºÅÊ•≠„ÅÆÂ≠¶ÁøíÊàêÊûú„Å´‰∏é„Åà„ÇãÂΩ±Èüø„ÇíÂàÜÊûê„Åô„Çã   \n",
       "1                       CVC„ÇíÂÆüÊñΩ„Åó„Åü‰ºÅÊ•≠„ÅÆÊàêÈï∑ÊåáÊ®ô„ÇíËøΩË∑°„Åó„ÄÅCVC„ÅÆÂÆüÊñΩ„ÅåÈï∑ÊúüÁöÑ„Å™ÊàêÈï∑„Å´„Å©„ÅÆ„Çà„ÅÜ„Å´ÂØÑ‰∏é„Åô„Çã„Åã„ÇíÂàÜÊûê„Åô„Çã„ÄÇÂÆöÈáèÁöÑ„Éá„Éº„Çø„Å®Ë≥™ÁöÑ„Ç§„É≥„Çø„Éì„É•„Éº„Çí‰ΩµÁî®„Åô„Çã   \n",
       "2  CVC„ÇíÂà©Áî®„Åô„Çã‰ºÅÊ•≠„ÇíÂØæË±°„Å´„ÄÅ„Éï„Ç°„Ç§„Éä„É≥„Ç∑„É£„É´„É™„Çø„Éº„É≥ÈáçË¶ñÂ∫¶„Å®Â≠¶ÁøíÊàêÊûú„ÅÆÁõ∏Èñ¢„ÇíÂ§öÂ§âÈáèÂàÜÊûê„ÇíÁî®„ÅÑ„Å¶Ê§úË®º„Åô„Çã„ÄÇ„Éá„Éº„Çø„Å®„Åó„Å¶„ÅØ„ÄÅCVCÊäïË≥áÂæå„ÅÆ„Ç§„Éé„Éô„Éº„Ç∑„Éß„É≥ÊàêÊûú„ÇÑÊàêÈï∑ÊåáÊ®ô„ÇíÁî®„ÅÑ„Çã‰∫àÂÆö„Åß„ÅÇ„Çã   \n",
       "3                                 ÂõΩ„ÇÑÂú∞Âüü„ÅÆÂà∂Â∫¶Áí∞Â¢ÉÊåáÊ®ô„ÇíÁî®„ÅÑ„Å¶„ÄÅCVC„ÅÆÊäïË≥áÊàêÂäü„Å®„ÅÆÈñ¢‰øÇ„ÇíÂ§öÂ§âÈáèÂõûÂ∏∞ÂàÜÊûê„ÅßÊ§úË®º„Åô„Çã„ÄÇÂà∂Â∫¶Áí∞Â¢É„ÅÆË≥™„ÇíÊìç‰ΩúÂ§âÊï∞„Å®„Åó„Å¶ËÄÉÊÖÆ„Åô„Çã   \n",
       "4                  Âú∞ÂüüÂà•„ÅÆCVCÊäïË≥á„Éá„Éº„Çø„ÇíÁî®„ÅÑ„Å¶„ÄÅ„Éï„Ç°„Ç§„Éä„É≥„Ç∑„É£„É´„É™„Çø„Éº„É≥ÈáçË¶ñ„ÅÆÁ®ãÂ∫¶„Å®ÊäïË≥áÊàêÊûú„Å®„ÅÆÈñ¢‰øÇ„ÇíÂ§öÂ§âÈáèÂõûÂ∏∞ÂàÜÊûê„ÅßÊ§úË®º„Åô„Çã„ÄÇÂú∞ÂüüÁâπÊÄßÊåáÊ®ô„Çí‰∫§Áµ°Âõ†Â≠ê„Å®„Åó„Å¶ËÄÉÊÖÆ„Åô„Çã   \n",
       "\n",
       "  Priority Status  \\\n",
       "0   Medium    New   \n",
       "1   Medium    New   \n",
       "2   Medium    New   \n",
       "3   Medium    New   \n",
       "4   Medium    New   \n",
       "\n",
       "                                                                                               Tags  \\\n",
       "0                                    [CVC, IVC, Learning Outcomes, Investment Behavior, Innovation]   \n",
       "1           [CVC, Long-term Growth, Investment Impact, Qualitative Research, Quantitative Research]   \n",
       "2                                            [CVC, Learning, Financial Returns, Innovation, Growth]   \n",
       "3  [CVC, Institutional Environment, Investment Success, Policy Implications, Quantitative Analysis]   \n",
       "4              [CVC, Regional Differences, Financial Returns, Investment Outcomes, Policy Analysis]   \n",
       "\n",
       "  evidence_used  evidence_count  tag_count  \n",
       "0        [1, 2]               2          5  \n",
       "1        [1, 2]               2          5  \n",
       "2        [1, 2]               2          5  \n",
       "3        [1, 2]               2          5  \n",
       "4        [1, 2]               2          5  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ Selected RQs post-processed.\n",
      "Selected (raw):   1\n",
      "Selected (clean): 1\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>Rationale / Background</th>\n",
       "      <th>Gap Identified</th>\n",
       "      <th>Proposed Approach</th>\n",
       "      <th>Priority</th>\n",
       "      <th>Status</th>\n",
       "      <th>Tags</th>\n",
       "      <th>evidence_used</th>\n",
       "      <th>evidence_count</th>\n",
       "      <th>tag_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CVC„ÅÆÊàêÈï∑ÈÅéÁ®ã„Å´„Åä„Åë„ÇãÂ≠¶ÁøíÂäπÊûú„ÅØ„ÄÅ„Éï„Ç°„Ç§„Éä„É≥„Ç∑„É£„É´„É™„Çø„Éº„É≥ÈáçË¶ñ„ÅÆÂßøÂã¢„Å´„Çà„Å£„Å¶„Å©„ÅÆ„Çà„ÅÜ„Å´Â§âÂåñ„Åô„Çã„ÅãÔºü</td>\n",
       "      <td>CVC„ÅØÂçò„Å™„ÇãË≥áÈáëÊèê‰æõ„Å´Áïô„Åæ„Çâ„Åö„ÄÅ‰ºÅÊ•≠„ÅÆÂ≠¶Áøí„ÇÑÊàêÈï∑„Å´ÂØÑ‰∏é„Åô„ÇãÈáçË¶Å„Å™Ë¶ÅÁ¥†„Åß„ÅÇ„Çã„ÄÇÊú¨Á†îÁ©∂„ÅØ„Éï„Ç°„Ç§„Éä„É≥„Ç∑„É£„É´„É™„Çø„Éº„É≥ÈáçË¶ñ„ÅåCVC„ÅÆÊàêÈï∑ÈÅéÁ®ã„Å´‰∏é„Åà„ÇãÂΩ±Èüø„ÇíÊòé„Çâ„Åã„Å´„Åô„Çã„Åì„Å®„Åß„ÄÅÂÆüÂãôËÄÖ„Å´„Å®„Å£„Å¶„ÅÆÁ§∫ÂîÜ„ÇíÊèê‰æõ„Åô„Çã„Åì„Å®„ÇíÁõÆÊåá„Åô</td>\n",
       "      <td>ÂæìÊù•„ÅÆÁ†îÁ©∂„ÅØCVC„ÅÆÂ≠¶ÁøíÈÅéÁ®ã„Å´„Åä„Åë„Çã„Éï„Ç°„Ç§„Éä„É≥„Ç∑„É£„É´„É™„Çø„Éº„É≥„ÅÆÂΩ±Èüø„ÇíÂçÅÂàÜ„Å´ËÄÉÂØü„Åó„Å¶„Åä„Çâ„Åö„ÄÅ„Åù„ÅÆ„É°„Ç´„Éã„Ç∫„É†„Åå‰∏çÊòéÁ¢∫„Åß„ÅÇ„Çã„ÄÇÁâπ„Å´„ÄÅ„Éï„Ç°„Ç§„Éä„É≥„Ç∑„É£„É´„É™„Çø„Éº„É≥ÈáçË¶ñ„Åå‰ºÅÊ•≠„ÅÆÊé¢Á¥¢Ë°åÂãï„Å´‰∏é„Åà„ÇãÂΩ±Èüø„Å´Èñ¢„Åô„ÇãÂÆüË®ºÁ†îÁ©∂„ÅØ‰∏çË∂≥„Åó„Å¶„ÅÑ„Çã</td>\n",
       "      <td>CVC„ÇíÂà©Áî®„Åô„Çã‰ºÅÊ•≠„ÇíÂØæË±°„Å´„ÄÅ„Éï„Ç°„Ç§„Éä„É≥„Ç∑„É£„É´„É™„Çø„Éº„É≥ÈáçË¶ñÂ∫¶„Å®Â≠¶ÁøíÊàêÊûú„ÅÆÁõ∏Èñ¢„ÇíÂ§öÂ§âÈáèÂàÜÊûê„ÇíÁî®„ÅÑ„Å¶Ê§úË®º„Åô„Çã„ÄÇ„Éá„Éº„Çø„Å®„Åó„Å¶„ÅØ„ÄÅCVCÊäïË≥áÂæå„ÅÆ„Ç§„Éé„Éô„Éº„Ç∑„Éß„É≥ÊàêÊûú„ÇÑÊàêÈï∑ÊåáÊ®ô„ÇíÁî®„ÅÑ„Çã‰∫àÂÆö„Åß„ÅÇ„Çã</td>\n",
       "      <td>Medium</td>\n",
       "      <td>New</td>\n",
       "      <td>[CVC, Learning, Financial Returns, Innovation, Growth]</td>\n",
       "      <td>[1, 2]</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 Name  \\\n",
       "0  CVC„ÅÆÊàêÈï∑ÈÅéÁ®ã„Å´„Åä„Åë„ÇãÂ≠¶ÁøíÂäπÊûú„ÅØ„ÄÅ„Éï„Ç°„Ç§„Éä„É≥„Ç∑„É£„É´„É™„Çø„Éº„É≥ÈáçË¶ñ„ÅÆÂßøÂã¢„Å´„Çà„Å£„Å¶„Å©„ÅÆ„Çà„ÅÜ„Å´Â§âÂåñ„Åô„Çã„ÅãÔºü   \n",
       "\n",
       "                                                                                     Rationale / Background  \\\n",
       "0  CVC„ÅØÂçò„Å™„ÇãË≥áÈáëÊèê‰æõ„Å´Áïô„Åæ„Çâ„Åö„ÄÅ‰ºÅÊ•≠„ÅÆÂ≠¶Áøí„ÇÑÊàêÈï∑„Å´ÂØÑ‰∏é„Åô„ÇãÈáçË¶Å„Å™Ë¶ÅÁ¥†„Åß„ÅÇ„Çã„ÄÇÊú¨Á†îÁ©∂„ÅØ„Éï„Ç°„Ç§„Éä„É≥„Ç∑„É£„É´„É™„Çø„Éº„É≥ÈáçË¶ñ„ÅåCVC„ÅÆÊàêÈï∑ÈÅéÁ®ã„Å´‰∏é„Åà„ÇãÂΩ±Èüø„ÇíÊòé„Çâ„Åã„Å´„Åô„Çã„Åì„Å®„Åß„ÄÅÂÆüÂãôËÄÖ„Å´„Å®„Å£„Å¶„ÅÆÁ§∫ÂîÜ„ÇíÊèê‰æõ„Åô„Çã„Åì„Å®„ÇíÁõÆÊåá„Åô   \n",
       "\n",
       "                                                                                               Gap Identified  \\\n",
       "0  ÂæìÊù•„ÅÆÁ†îÁ©∂„ÅØCVC„ÅÆÂ≠¶ÁøíÈÅéÁ®ã„Å´„Åä„Åë„Çã„Éï„Ç°„Ç§„Éä„É≥„Ç∑„É£„É´„É™„Çø„Éº„É≥„ÅÆÂΩ±Èüø„ÇíÂçÅÂàÜ„Å´ËÄÉÂØü„Åó„Å¶„Åä„Çâ„Åö„ÄÅ„Åù„ÅÆ„É°„Ç´„Éã„Ç∫„É†„Åå‰∏çÊòéÁ¢∫„Åß„ÅÇ„Çã„ÄÇÁâπ„Å´„ÄÅ„Éï„Ç°„Ç§„Éä„É≥„Ç∑„É£„É´„É™„Çø„Éº„É≥ÈáçË¶ñ„Åå‰ºÅÊ•≠„ÅÆÊé¢Á¥¢Ë°åÂãï„Å´‰∏é„Åà„ÇãÂΩ±Èüø„Å´Èñ¢„Åô„ÇãÂÆüË®ºÁ†îÁ©∂„ÅØ‰∏çË∂≥„Åó„Å¶„ÅÑ„Çã   \n",
       "\n",
       "                                                                             Proposed Approach  \\\n",
       "0  CVC„ÇíÂà©Áî®„Åô„Çã‰ºÅÊ•≠„ÇíÂØæË±°„Å´„ÄÅ„Éï„Ç°„Ç§„Éä„É≥„Ç∑„É£„É´„É™„Çø„Éº„É≥ÈáçË¶ñÂ∫¶„Å®Â≠¶ÁøíÊàêÊûú„ÅÆÁõ∏Èñ¢„ÇíÂ§öÂ§âÈáèÂàÜÊûê„ÇíÁî®„ÅÑ„Å¶Ê§úË®º„Åô„Çã„ÄÇ„Éá„Éº„Çø„Å®„Åó„Å¶„ÅØ„ÄÅCVCÊäïË≥áÂæå„ÅÆ„Ç§„Éé„Éô„Éº„Ç∑„Éß„É≥ÊàêÊûú„ÇÑÊàêÈï∑ÊåáÊ®ô„ÇíÁî®„ÅÑ„Çã‰∫àÂÆö„Åß„ÅÇ„Çã   \n",
       "\n",
       "  Priority Status                                                    Tags  \\\n",
       "0   Medium    New  [CVC, Learning, Financial Returns, Innovation, Growth]   \n",
       "\n",
       "  evidence_used  evidence_count  tag_count  \n",
       "0        [1, 2]               2          5  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ============================================================\n",
    "# Cell 7 : Post-processing (DB-aligned: Name/Tags/Priority/Status)\n",
    "# ============================================================\n",
    "\n",
    "import re\n",
    "import json\n",
    "from difflib import SequenceMatcher\n",
    "from typing import Tuple\n",
    "\n",
    "# ----------------------------\n",
    "# Config\n",
    "# ----------------------------\n",
    "MIN_NAME_CHARS = 12\n",
    "MAX_NAME_CHARS = 220\n",
    "FUZZY_DUP_THRESHOLD = 0.92\n",
    "\n",
    "REQUIRE_EVIDENCE = True\n",
    "MIN_EVIDENCE_IDS = 1\n",
    "\n",
    "# ----------------------------\n",
    "# Helpers\n",
    "# ----------------------------\n",
    "def normalize_text(s: str) -> str:\n",
    "    if s is None:\n",
    "        return \"\"\n",
    "    s = str(s).strip()\n",
    "    s = re.sub(r\"\\s+\", \" \", s)\n",
    "    s = s.replace(\"‚Äú\", '\"').replace(\"‚Äù\", '\"').replace(\"‚Äô\", \"'\")\n",
    "    s = re.sub(r\"[„ÄÇÔºé\\.]+$\", \"\", s)\n",
    "    return s\n",
    "\n",
    "def similarity(a: str, b: str) -> float:\n",
    "    return SequenceMatcher(None, a, b).ratio()\n",
    "\n",
    "def safe_list(x):\n",
    "    \"\"\"Parse list-like fields that may be stored as JSON string.\"\"\"\n",
    "    if x is None:\n",
    "        return []\n",
    "    if isinstance(x, list):\n",
    "        return x\n",
    "    if isinstance(x, str):\n",
    "        try:\n",
    "            v = json.loads(x)\n",
    "            return v if isinstance(v, list) else []\n",
    "        except Exception:\n",
    "            # fallback: comma split\n",
    "            return [t.strip() for t in x.split(\",\") if t.strip()]\n",
    "    return []\n",
    "\n",
    "def safe_evidence_count(ev) -> int:\n",
    "    ev_list = safe_list(ev)\n",
    "    return len([x for x in ev_list if isinstance(x, int)])\n",
    "\n",
    "def safe_tag_count(tags) -> int:\n",
    "    tags_list = safe_list(tags)\n",
    "    return len([x for x in tags_list if isinstance(x, str) and x.strip()])\n",
    "\n",
    "def deduplicate_questions(df, col: str = \"Name\") -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    if col not in df.columns:\n",
    "        raise ValueError(f\"Missing column: {col}\")\n",
    "\n",
    "    df = df.copy()\n",
    "    df[\"_norm\"] = df[col].map(normalize_text)\n",
    "\n",
    "    # exact dedup\n",
    "    df_exact = df.drop_duplicates(subset=[\"_norm\"], keep=\"first\").copy()\n",
    "    removed_exact = df[~df.index.isin(df_exact.index)].copy()\n",
    "\n",
    "    # fuzzy dedup (greedy)\n",
    "    kept, removed = [], []\n",
    "    norms = df_exact[\"_norm\"].tolist()\n",
    "    idxs = df_exact.index.tolist()\n",
    "\n",
    "    for i, idx in enumerate(idxs):\n",
    "        q = norms[i]\n",
    "        is_dup = False\n",
    "        for kept_idx in kept:\n",
    "            q2 = df_exact.loc[kept_idx, \"_norm\"]\n",
    "            if similarity(q, q2) >= FUZZY_DUP_THRESHOLD:\n",
    "                is_dup = True\n",
    "                removed.append(idx)\n",
    "                break\n",
    "        if not is_dup:\n",
    "            kept.append(idx)\n",
    "\n",
    "    df_dedup = df_exact.loc[kept].copy().reset_index(drop=True)\n",
    "    removed_fuzzy = df_exact.loc[removed].copy().reset_index(drop=True)\n",
    "\n",
    "    removed_all = pd.concat([removed_exact, removed_fuzzy], ignore_index=True)\n",
    "    df_dedup = df_dedup.drop(columns=[\"_norm\"])\n",
    "    removed_all = removed_all.drop(columns=[\"_norm\"], errors=\"ignore\")\n",
    "    return df_dedup, removed_all\n",
    "\n",
    "def apply_basic_filters(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = df.copy()\n",
    "\n",
    "    # Required columns for the DB-aligned pipeline\n",
    "    required_cols = [\"Name\", \"Rationale / Background\", \"Gap Identified\", \"Proposed Approach\", \"Priority\", \"Status\", \"Tags\"]\n",
    "    missing = [c for c in required_cols if c not in df.columns]\n",
    "    if missing:\n",
    "        raise ValueError(f\"rq_df is not DB-aligned. Missing columns: {missing}\")\n",
    "\n",
    "    # normalize text fields\n",
    "    for c in [\"Name\", \"Rationale / Background\", \"Gap Identified\", \"Proposed Approach\", \"Priority\", \"Status\"]:\n",
    "        df[c] = df[c].map(normalize_text)\n",
    "\n",
    "    # normalize list-like fields\n",
    "    df[\"Tags\"] = df[\"Tags\"].map(safe_list)\n",
    "    if \"evidence_used\" in df.columns:\n",
    "        df[\"evidence_used\"] = df[\"evidence_used\"].map(safe_list)\n",
    "    else:\n",
    "        df[\"evidence_used\"] = [[] for _ in range(len(df))]\n",
    "\n",
    "    # length filter on Name\n",
    "    df = df[df[\"Name\"].str.len().between(MIN_NAME_CHARS, MAX_NAME_CHARS)]\n",
    "\n",
    "    # evidence requirement\n",
    "    df[\"evidence_count\"] = df[\"evidence_used\"].map(safe_evidence_count)\n",
    "    if REQUIRE_EVIDENCE:\n",
    "        df = df[df[\"evidence_count\"] >= MIN_EVIDENCE_IDS]\n",
    "\n",
    "    # tag sanity\n",
    "    df[\"tag_count\"] = df[\"Tags\"].map(safe_tag_count)\n",
    "\n",
    "    return df.reset_index(drop=True)\n",
    "\n",
    "# ----------------------------\n",
    "# Run post-processing for all candidates\n",
    "# ----------------------------\n",
    "if \"rq_df\" not in globals() or rq_df is None or len(rq_df) == 0:\n",
    "    raise ValueError(\"rq_df is empty. Run Cell 6 first.\")\n",
    "\n",
    "rq_df_filtered = apply_basic_filters(rq_df)\n",
    "rq_df_clean, rq_removed_df = deduplicate_questions(rq_df_filtered, col=\"Name\")\n",
    "\n",
    "# Ranking for triage:\n",
    "# Priority/Status are fixed, so sort by evidence_count desc, then tag_count desc\n",
    "rq_ranked_df = rq_df_clean.sort_values(\n",
    "    by=[\"evidence_count\", \"tag_count\", \"Name\"],\n",
    "    ascending=[False, False, True],\n",
    ").reset_index(drop=True)\n",
    "\n",
    "print(\"‚úÖ Post-processing complete.\")\n",
    "print(f\"Candidates (raw): {len(rq_df)}\")\n",
    "print(f\"After filters:    {len(rq_df_filtered)}\")\n",
    "print(f\"After dedup:      {len(rq_df_clean)}\")\n",
    "print(f\"Removed:          {len(rq_removed_df)}\")\n",
    "\n",
    "display(rq_ranked_df.head(10))\n",
    "\n",
    "# ----------------------------\n",
    "# Optional: post-process selected (Go-approved) candidates\n",
    "# ----------------------------\n",
    "if \"rq_selected_df\" in globals() and rq_selected_df is not None and len(rq_selected_df) > 0:\n",
    "    rq_selected_filtered = apply_basic_filters(rq_selected_df)\n",
    "    rq_selected_clean_df, rq_selected_removed_df = deduplicate_questions(rq_selected_filtered, col=\"Name\")\n",
    "\n",
    "    rq_selected_clean_df = rq_selected_clean_df.sort_values(\n",
    "        by=[\"evidence_count\", \"tag_count\", \"Name\"],\n",
    "        ascending=[False, False, True],\n",
    "    ).reset_index(drop=True)\n",
    "\n",
    "    print(\"\\n‚úÖ Selected RQs post-processed.\")\n",
    "    print(f\"Selected (raw):   {len(rq_selected_df)}\")\n",
    "    print(f\"Selected (clean): {len(rq_selected_clean_df)}\")\n",
    "\n",
    "    display(rq_selected_clean_df)\n",
    "else:\n",
    "    rq_selected_clean_df = None\n",
    "    print(\"\\n(No Go-approved candidates found yet; skipping selected post-processing.)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "fbb8e4d8-94e6-4e85-bc98-e038e614b45d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üì¶ Export directory: outputs/rq_run_20260105_151558\n",
      "‚úÖ Exported ranked candidates: rq_candidates_ranked.csv, rq_candidates_ranked.json\n",
      "‚ÑπÔ∏è No removed candidates to export.\n",
      "‚úÖ Exported selected candidates: rq_selected.csv, rq_selected.json\n",
      "üßæ Saved run manifest: run_manifest.json\n",
      "\n",
      "--- Quick inspection ---\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>Rationale / Background</th>\n",
       "      <th>Gap Identified</th>\n",
       "      <th>Proposed Approach</th>\n",
       "      <th>Priority</th>\n",
       "      <th>Status</th>\n",
       "      <th>Tags</th>\n",
       "      <th>evidence_used</th>\n",
       "      <th>evidence_count</th>\n",
       "      <th>tag_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CVC„Å®IVC„ÅÆÊäïË≥áË°åÂãï„ÅÆÈÅï„ÅÑ„Åå‰ºÅÊ•≠„ÅÆÂ≠¶ÁøíÊàêÊûú„Å´‰∏é„Åà„ÇãÂΩ±Èüø„ÅØ‰Ωï„ÅãÔºü</td>\n",
       "      <td>CVC„Å®IVC„ÅÆÊäïË≥áË°åÂãï„Å´„ÅØÊòéÁ¢∫„Å™ÈÅï„ÅÑ„Åå„ÅÇ„Çä„ÄÅ„Åù„Çå„Åå‰ºÅÊ•≠„ÅÆÂ≠¶Áøí„ÇÑÊàêÈï∑„Å´„Å©„ÅÆ„Çà„ÅÜ„Å´ÂΩ±Èüø„Åô„Çã„Åã„ÇíÁêÜËß£„Åô„Çã„Åì„Å®„ÅØÈáçË¶Å„Åß„ÅÇ„Çã„ÄÇÁâπ„Å´„ÄÅÂ≠¶Áøí„ÅÆË¶≥ÁÇπ„Åã„Çâ‰∏°ËÄÖ„ÅÆÈÅï„ÅÑ„ÇíÊòéÁ¢∫„Å´„Åô„Çã„Åì„Å®„Åß„ÄÅ„Çà„ÇäÂäπÊûúÁöÑ„Å™ÊäïË≥áÊà¶Áï•„ÅåË¶ã„Åà„Å¶„Åè„Çã</td>\n",
       "      <td>CVC„Å®IVC„ÅÆÊäïË≥áË°åÂãï„ÅÆÈÅï„ÅÑ„Åå‰ºÅÊ•≠„ÅÆÂ≠¶ÁøíÊàêÊûú„Å´‰∏é„Åà„ÇãÂΩ±Èüø„Å´Èñ¢„Åô„ÇãÂÆüË®ºÁ†îÁ©∂„ÅØÈôê„Çâ„Çå„Å¶„Åä„Çä„ÄÅ„Åù„ÅÆ„É°„Ç´„Éã„Ç∫„É†„Åå‰∏çÊòé„Åß„ÅÇ„Çã</td>\n",
       "      <td>CVC„Åä„Çà„Å≥IVC„Åã„ÇâË≥áÈáë„ÇíÂèó„Åë„Åü‰ºÅÊ•≠„ÅÆ„Ç§„Éé„Éô„Éº„Ç∑„Éß„É≥ÊàêÊûú„ÇíÊØîËºÉ„Åô„Çã„Åì„Å®„Åß„ÄÅ‰∏°ËÄÖ„ÅÆÊäïË≥áË°åÂãï„ÅÆÈÅï„ÅÑ„Åå‰ºÅÊ•≠„ÅÆÂ≠¶ÁøíÊàêÊûú„Å´‰∏é„Åà„ÇãÂΩ±Èüø„ÇíÂàÜÊûê„Åô„Çã</td>\n",
       "      <td>Medium</td>\n",
       "      <td>New</td>\n",
       "      <td>[CVC, IVC, Learning Outcomes, Investment Behavior, Innovation]</td>\n",
       "      <td>[1, 2]</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CVC„ÅÆÂÆüÊñΩ„Åå‰ºÅÊ•≠„ÅÆÈï∑ÊúüÁöÑ„Å™ÊàêÈï∑„Å´‰∏é„Åà„ÇãÂΩ±Èüø„ÅØ‰Ωï„ÅãÔºü</td>\n",
       "      <td>CVC„ÅÆÂΩπÂâ≤„ÅØÁü≠ÊúüÁöÑ„Å™ÊäïË≥áÂà©Áõä„Å†„Åë„Åß„Å™„Åè„ÄÅ‰ºÅÊ•≠„ÅÆÈï∑ÊúüÁöÑ„Å™ÊàêÈï∑„Å´„ÇÇÂΩ±Èüø„Çí‰∏é„Åà„Çã„ÄÇÊú¨Á†îÁ©∂„ÅØ„Åù„ÅÆ„É°„Ç´„Éã„Ç∫„É†„ÇíÊòé„Çâ„Åã„Å´„Åô„Çã„Åì„Å®„Åß„ÄÅCVC„ÅÆ‰æ°ÂÄ§„ÇíÂÜçË©ï‰æ°„Åô„Çã„Åì„Å®„ÇíÁõÆÁöÑ„Å®„Åô„Çã</td>\n",
       "      <td>CVC„ÅÆÂÆüÊñΩ„Åå‰ºÅÊ•≠„ÅÆÈï∑ÊúüÁöÑ„Å™ÊàêÈï∑„Å´‰∏é„Åà„ÇãÂΩ±Èüø„Å´„Å§„ÅÑ„Å¶„ÅÆÂÆüË®ºÁ†îÁ©∂„ÅØÈôê„Çâ„Çå„Å¶„Åä„Çä„ÄÅÁâπ„Å´„Åù„ÅÆ„É°„Ç´„Éã„Ç∫„É†„Åå‰∏çÊòé„Åß„ÅÇ„Çã</td>\n",
       "      <td>CVC„ÇíÂÆüÊñΩ„Åó„Åü‰ºÅÊ•≠„ÅÆÊàêÈï∑ÊåáÊ®ô„ÇíËøΩË∑°„Åó„ÄÅCVC„ÅÆÂÆüÊñΩ„ÅåÈï∑ÊúüÁöÑ„Å™ÊàêÈï∑„Å´„Å©„ÅÆ„Çà„ÅÜ„Å´ÂØÑ‰∏é„Åô„Çã„Åã„ÇíÂàÜÊûê„Åô„Çã„ÄÇÂÆöÈáèÁöÑ„Éá„Éº„Çø„Å®Ë≥™ÁöÑ„Ç§„É≥„Çø„Éì„É•„Éº„Çí‰ΩµÁî®„Åô„Çã</td>\n",
       "      <td>Medium</td>\n",
       "      <td>New</td>\n",
       "      <td>[CVC, Long-term Growth, Investment Impact, Qualitative Research, Quantitative Research]</td>\n",
       "      <td>[1, 2]</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CVC„ÅÆÊàêÈï∑ÈÅéÁ®ã„Å´„Åä„Åë„ÇãÂ≠¶ÁøíÂäπÊûú„ÅØ„ÄÅ„Éï„Ç°„Ç§„Éä„É≥„Ç∑„É£„É´„É™„Çø„Éº„É≥ÈáçË¶ñ„ÅÆÂßøÂã¢„Å´„Çà„Å£„Å¶„Å©„ÅÆ„Çà„ÅÜ„Å´Â§âÂåñ„Åô„Çã„ÅãÔºü</td>\n",
       "      <td>CVC„ÅØÂçò„Å™„ÇãË≥áÈáëÊèê‰æõ„Å´Áïô„Åæ„Çâ„Åö„ÄÅ‰ºÅÊ•≠„ÅÆÂ≠¶Áøí„ÇÑÊàêÈï∑„Å´ÂØÑ‰∏é„Åô„ÇãÈáçË¶Å„Å™Ë¶ÅÁ¥†„Åß„ÅÇ„Çã„ÄÇÊú¨Á†îÁ©∂„ÅØ„Éï„Ç°„Ç§„Éä„É≥„Ç∑„É£„É´„É™„Çø„Éº„É≥ÈáçË¶ñ„ÅåCVC„ÅÆÊàêÈï∑ÈÅéÁ®ã„Å´‰∏é„Åà„ÇãÂΩ±Èüø„ÇíÊòé„Çâ„Åã„Å´„Åô„Çã„Åì„Å®„Åß„ÄÅÂÆüÂãôËÄÖ„Å´„Å®„Å£„Å¶„ÅÆÁ§∫ÂîÜ„ÇíÊèê‰æõ„Åô„Çã„Åì„Å®„ÇíÁõÆÊåá„Åô</td>\n",
       "      <td>ÂæìÊù•„ÅÆÁ†îÁ©∂„ÅØCVC„ÅÆÂ≠¶ÁøíÈÅéÁ®ã„Å´„Åä„Åë„Çã„Éï„Ç°„Ç§„Éä„É≥„Ç∑„É£„É´„É™„Çø„Éº„É≥„ÅÆÂΩ±Èüø„ÇíÂçÅÂàÜ„Å´ËÄÉÂØü„Åó„Å¶„Åä„Çâ„Åö„ÄÅ„Åù„ÅÆ„É°„Ç´„Éã„Ç∫„É†„Åå‰∏çÊòéÁ¢∫„Åß„ÅÇ„Çã„ÄÇÁâπ„Å´„ÄÅ„Éï„Ç°„Ç§„Éä„É≥„Ç∑„É£„É´„É™„Çø„Éº„É≥ÈáçË¶ñ„Åå‰ºÅÊ•≠„ÅÆÊé¢Á¥¢Ë°åÂãï„Å´‰∏é„Åà„ÇãÂΩ±Èüø„Å´Èñ¢„Åô„ÇãÂÆüË®ºÁ†îÁ©∂„ÅØ‰∏çË∂≥„Åó„Å¶„ÅÑ„Çã</td>\n",
       "      <td>CVC„ÇíÂà©Áî®„Åô„Çã‰ºÅÊ•≠„ÇíÂØæË±°„Å´„ÄÅ„Éï„Ç°„Ç§„Éä„É≥„Ç∑„É£„É´„É™„Çø„Éº„É≥ÈáçË¶ñÂ∫¶„Å®Â≠¶ÁøíÊàêÊûú„ÅÆÁõ∏Èñ¢„ÇíÂ§öÂ§âÈáèÂàÜÊûê„ÇíÁî®„ÅÑ„Å¶Ê§úË®º„Åô„Çã„ÄÇ„Éá„Éº„Çø„Å®„Åó„Å¶„ÅØ„ÄÅCVCÊäïË≥áÂæå„ÅÆ„Ç§„Éé„Éô„Éº„Ç∑„Éß„É≥ÊàêÊûú„ÇÑÊàêÈï∑ÊåáÊ®ô„ÇíÁî®„ÅÑ„Çã‰∫àÂÆö„Åß„ÅÇ„Çã</td>\n",
       "      <td>Medium</td>\n",
       "      <td>New</td>\n",
       "      <td>[CVC, Learning, Financial Returns, Innovation, Growth]</td>\n",
       "      <td>[1, 2]</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>CVC„ÅÆÊäïË≥áÊàêÂäü„Å´„Åä„Åë„ÇãÂà∂Â∫¶Áí∞Â¢É„ÅÆÂΩπÂâ≤„ÅØ„Å©„ÅÆ„Çà„ÅÜ„Å™„ÇÇ„ÅÆ„ÅãÔºü</td>\n",
       "      <td>Âà∂Â∫¶Áí∞Â¢É„ÅØCVC„ÅÆÊäïË≥áÊàêÂäü„Å´ÂΩ±Èüø„ÇíÂèä„Åº„ÅôÈáçË¶Å„Å™Ë¶ÅÂõ†„Åß„ÅÇ„Çã„ÄÇÊú¨Á†îÁ©∂„ÅØÂà∂Â∫¶Áí∞Â¢É„ÅåCVC„ÅÆÊäïË≥áÊàêÊûú„Å´‰∏é„Åà„ÇãÂΩ±Èüø„ÇíÊòé„Çâ„Åã„Å´„Åô„Çã„Åì„Å®„Åß„ÄÅÊîøÁ≠ñÊèêË®Ä„Å´ÂØÑ‰∏é„Åô„Çã„Åì„Å®„ÇíÁõÆÊåá„Åô</td>\n",
       "      <td>Âà∂Â∫¶Áí∞Â¢É„ÅåCVC„ÅÆÊäïË≥áÊàêÂäü„Å´‰∏é„Åà„ÇãÂΩ±Èüø„Å´„Å§„ÅÑ„Å¶„ÅÆÂÆüË®ºÁ†îÁ©∂„Åå‰∏çË∂≥„Åó„Å¶„Åä„Çä„ÄÅÁâπ„Å´Âà∂Â∫¶„ÅÆË≥™„Å®ÊäïË≥áÊàêÊûú„ÅÆÈñ¢‰øÇ„Åå‰∏çÊòé„Åß„ÅÇ„Çã</td>\n",
       "      <td>ÂõΩ„ÇÑÂú∞Âüü„ÅÆÂà∂Â∫¶Áí∞Â¢ÉÊåáÊ®ô„ÇíÁî®„ÅÑ„Å¶„ÄÅCVC„ÅÆÊäïË≥áÊàêÂäü„Å®„ÅÆÈñ¢‰øÇ„ÇíÂ§öÂ§âÈáèÂõûÂ∏∞ÂàÜÊûê„ÅßÊ§úË®º„Åô„Çã„ÄÇÂà∂Â∫¶Áí∞Â¢É„ÅÆË≥™„ÇíÊìç‰ΩúÂ§âÊï∞„Å®„Åó„Å¶ËÄÉÊÖÆ„Åô„Çã</td>\n",
       "      <td>Medium</td>\n",
       "      <td>New</td>\n",
       "      <td>[CVC, Institutional Environment, Investment Success, Policy Implications, Quantitative Analysis]</td>\n",
       "      <td>[1, 2]</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>CVC„ÅÆÊäïË≥áÊàêÊûú„Å´„Åä„Åë„Çã„Éï„Ç°„Ç§„Éä„É≥„Ç∑„É£„É´„É™„Çø„Éº„É≥ÈáçË¶ñ„ÅÆÂΩ±Èüø„ÇíÂú∞ÂüüÂ∑Æ„ÅßÂàÜÊûê„Åô„Çã„Åì„Å®„ÅØÂèØËÉΩ„ÅãÔºü</td>\n",
       "      <td>Âú∞Âüü„Å´„Çà„Å£„Å¶Ëµ∑Ê•≠Áí∞Â¢É„ÇÑÊñáÂåñ„ÅåÁï∞„Å™„Çã„Åü„ÇÅ„ÄÅCVC„ÅÆÊäïË≥áÊàêÊûú„Å´„ÇÇÂΩ±Èüø„Çí‰∏é„Åà„Çã„ÄÇÂú∞ÂüüÂ∑Æ„ÇíËÄÉÊÖÆ„Åô„Çã„Åì„Å®„Åß„ÄÅ„Çà„ÇäÂÖ∑‰ΩìÁöÑ„Å™Êà¶Áï•„ÇÑÊîøÁ≠ñÊèêË®Ä„ÅåÂèØËÉΩ„Å´„Å™„Çã</td>\n",
       "      <td>CVC„ÅÆÊäïË≥áÊàêÊûú„Å®Âú∞ÂüüÁâπÊÄß„ÅÆÈñ¢ÈÄ£ÊÄß„Å´„Å§„ÅÑ„Å¶„ÅÆÂÆüË®ºÁ†îÁ©∂„Åå‰∏çË∂≥„Åó„Å¶„Åä„Çä„ÄÅÁâπ„Å´„Éï„Ç°„Ç§„Éä„É≥„Ç∑„É£„É´„É™„Çø„Éº„É≥ÈáçË¶ñ„ÅÆÂΩ±Èüø„ÇíÂú∞Âüü„Åî„Å®„Å´ÂàÜÊûê„Åó„ÅüÁ†îÁ©∂„ÅØÁöÜÁÑ°„Åß„ÅÇ„Çã</td>\n",
       "      <td>Âú∞ÂüüÂà•„ÅÆCVCÊäïË≥á„Éá„Éº„Çø„ÇíÁî®„ÅÑ„Å¶„ÄÅ„Éï„Ç°„Ç§„Éä„É≥„Ç∑„É£„É´„É™„Çø„Éº„É≥ÈáçË¶ñ„ÅÆÁ®ãÂ∫¶„Å®ÊäïË≥áÊàêÊûú„Å®„ÅÆÈñ¢‰øÇ„ÇíÂ§öÂ§âÈáèÂõûÂ∏∞ÂàÜÊûê„ÅßÊ§úË®º„Åô„Çã„ÄÇÂú∞ÂüüÁâπÊÄßÊåáÊ®ô„Çí‰∫§Áµ°Âõ†Â≠ê„Å®„Åó„Å¶ËÄÉÊÖÆ„Åô„Çã</td>\n",
       "      <td>Medium</td>\n",
       "      <td>New</td>\n",
       "      <td>[CVC, Regional Differences, Financial Returns, Investment Outcomes, Policy Analysis]</td>\n",
       "      <td>[1, 2]</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 Name  \\\n",
       "0                   CVC„Å®IVC„ÅÆÊäïË≥áË°åÂãï„ÅÆÈÅï„ÅÑ„Åå‰ºÅÊ•≠„ÅÆÂ≠¶ÁøíÊàêÊûú„Å´‰∏é„Åà„ÇãÂΩ±Èüø„ÅØ‰Ωï„ÅãÔºü   \n",
       "1                          CVC„ÅÆÂÆüÊñΩ„Åå‰ºÅÊ•≠„ÅÆÈï∑ÊúüÁöÑ„Å™ÊàêÈï∑„Å´‰∏é„Åà„ÇãÂΩ±Èüø„ÅØ‰Ωï„ÅãÔºü   \n",
       "2  CVC„ÅÆÊàêÈï∑ÈÅéÁ®ã„Å´„Åä„Åë„ÇãÂ≠¶ÁøíÂäπÊûú„ÅØ„ÄÅ„Éï„Ç°„Ç§„Éä„É≥„Ç∑„É£„É´„É™„Çø„Éº„É≥ÈáçË¶ñ„ÅÆÂßøÂã¢„Å´„Çà„Å£„Å¶„Å©„ÅÆ„Çà„ÅÜ„Å´Â§âÂåñ„Åô„Çã„ÅãÔºü   \n",
       "3                       CVC„ÅÆÊäïË≥áÊàêÂäü„Å´„Åä„Åë„ÇãÂà∂Â∫¶Áí∞Â¢É„ÅÆÂΩπÂâ≤„ÅØ„Å©„ÅÆ„Çà„ÅÜ„Å™„ÇÇ„ÅÆ„ÅãÔºü   \n",
       "4       CVC„ÅÆÊäïË≥áÊàêÊûú„Å´„Åä„Åë„Çã„Éï„Ç°„Ç§„Éä„É≥„Ç∑„É£„É´„É™„Çø„Éº„É≥ÈáçË¶ñ„ÅÆÂΩ±Èüø„ÇíÂú∞ÂüüÂ∑Æ„ÅßÂàÜÊûê„Åô„Çã„Åì„Å®„ÅØÂèØËÉΩ„ÅãÔºü   \n",
       "\n",
       "                                                                                     Rationale / Background  \\\n",
       "0      CVC„Å®IVC„ÅÆÊäïË≥áË°åÂãï„Å´„ÅØÊòéÁ¢∫„Å™ÈÅï„ÅÑ„Åå„ÅÇ„Çä„ÄÅ„Åù„Çå„Åå‰ºÅÊ•≠„ÅÆÂ≠¶Áøí„ÇÑÊàêÈï∑„Å´„Å©„ÅÆ„Çà„ÅÜ„Å´ÂΩ±Èüø„Åô„Çã„Åã„ÇíÁêÜËß£„Åô„Çã„Åì„Å®„ÅØÈáçË¶Å„Åß„ÅÇ„Çã„ÄÇÁâπ„Å´„ÄÅÂ≠¶Áøí„ÅÆË¶≥ÁÇπ„Åã„Çâ‰∏°ËÄÖ„ÅÆÈÅï„ÅÑ„ÇíÊòéÁ¢∫„Å´„Åô„Çã„Åì„Å®„Åß„ÄÅ„Çà„ÇäÂäπÊûúÁöÑ„Å™ÊäïË≥áÊà¶Áï•„ÅåË¶ã„Åà„Å¶„Åè„Çã   \n",
       "1                         CVC„ÅÆÂΩπÂâ≤„ÅØÁü≠ÊúüÁöÑ„Å™ÊäïË≥áÂà©Áõä„Å†„Åë„Åß„Å™„Åè„ÄÅ‰ºÅÊ•≠„ÅÆÈï∑ÊúüÁöÑ„Å™ÊàêÈï∑„Å´„ÇÇÂΩ±Èüø„Çí‰∏é„Åà„Çã„ÄÇÊú¨Á†îÁ©∂„ÅØ„Åù„ÅÆ„É°„Ç´„Éã„Ç∫„É†„ÇíÊòé„Çâ„Åã„Å´„Åô„Çã„Åì„Å®„Åß„ÄÅCVC„ÅÆ‰æ°ÂÄ§„ÇíÂÜçË©ï‰æ°„Åô„Çã„Åì„Å®„ÇíÁõÆÁöÑ„Å®„Åô„Çã   \n",
       "2  CVC„ÅØÂçò„Å™„ÇãË≥áÈáëÊèê‰æõ„Å´Áïô„Åæ„Çâ„Åö„ÄÅ‰ºÅÊ•≠„ÅÆÂ≠¶Áøí„ÇÑÊàêÈï∑„Å´ÂØÑ‰∏é„Åô„ÇãÈáçË¶Å„Å™Ë¶ÅÁ¥†„Åß„ÅÇ„Çã„ÄÇÊú¨Á†îÁ©∂„ÅØ„Éï„Ç°„Ç§„Éä„É≥„Ç∑„É£„É´„É™„Çø„Éº„É≥ÈáçË¶ñ„ÅåCVC„ÅÆÊàêÈï∑ÈÅéÁ®ã„Å´‰∏é„Åà„ÇãÂΩ±Èüø„ÇíÊòé„Çâ„Åã„Å´„Åô„Çã„Åì„Å®„Åß„ÄÅÂÆüÂãôËÄÖ„Å´„Å®„Å£„Å¶„ÅÆÁ§∫ÂîÜ„ÇíÊèê‰æõ„Åô„Çã„Åì„Å®„ÇíÁõÆÊåá„Åô   \n",
       "3                            Âà∂Â∫¶Áí∞Â¢É„ÅØCVC„ÅÆÊäïË≥áÊàêÂäü„Å´ÂΩ±Èüø„ÇíÂèä„Åº„ÅôÈáçË¶Å„Å™Ë¶ÅÂõ†„Åß„ÅÇ„Çã„ÄÇÊú¨Á†îÁ©∂„ÅØÂà∂Â∫¶Áí∞Â¢É„ÅåCVC„ÅÆÊäïË≥áÊàêÊûú„Å´‰∏é„Åà„ÇãÂΩ±Èüø„ÇíÊòé„Çâ„Åã„Å´„Åô„Çã„Åì„Å®„Åß„ÄÅÊîøÁ≠ñÊèêË®Ä„Å´ÂØÑ‰∏é„Åô„Çã„Åì„Å®„ÇíÁõÆÊåá„Åô   \n",
       "4                                      Âú∞Âüü„Å´„Çà„Å£„Å¶Ëµ∑Ê•≠Áí∞Â¢É„ÇÑÊñáÂåñ„ÅåÁï∞„Å™„Çã„Åü„ÇÅ„ÄÅCVC„ÅÆÊäïË≥áÊàêÊûú„Å´„ÇÇÂΩ±Èüø„Çí‰∏é„Åà„Çã„ÄÇÂú∞ÂüüÂ∑Æ„ÇíËÄÉÊÖÆ„Åô„Çã„Åì„Å®„Åß„ÄÅ„Çà„ÇäÂÖ∑‰ΩìÁöÑ„Å™Êà¶Áï•„ÇÑÊîøÁ≠ñÊèêË®Ä„ÅåÂèØËÉΩ„Å´„Å™„Çã   \n",
       "\n",
       "                                                                                               Gap Identified  \\\n",
       "0                                                  CVC„Å®IVC„ÅÆÊäïË≥áË°åÂãï„ÅÆÈÅï„ÅÑ„Åå‰ºÅÊ•≠„ÅÆÂ≠¶ÁøíÊàêÊûú„Å´‰∏é„Åà„ÇãÂΩ±Èüø„Å´Èñ¢„Åô„ÇãÂÆüË®ºÁ†îÁ©∂„ÅØÈôê„Çâ„Çå„Å¶„Åä„Çä„ÄÅ„Åù„ÅÆ„É°„Ç´„Éã„Ç∫„É†„Åå‰∏çÊòé„Åß„ÅÇ„Çã   \n",
       "1                                                      CVC„ÅÆÂÆüÊñΩ„Åå‰ºÅÊ•≠„ÅÆÈï∑ÊúüÁöÑ„Å™ÊàêÈï∑„Å´‰∏é„Åà„ÇãÂΩ±Èüø„Å´„Å§„ÅÑ„Å¶„ÅÆÂÆüË®ºÁ†îÁ©∂„ÅØÈôê„Çâ„Çå„Å¶„Åä„Çä„ÄÅÁâπ„Å´„Åù„ÅÆ„É°„Ç´„Éã„Ç∫„É†„Åå‰∏çÊòé„Åß„ÅÇ„Çã   \n",
       "2  ÂæìÊù•„ÅÆÁ†îÁ©∂„ÅØCVC„ÅÆÂ≠¶ÁøíÈÅéÁ®ã„Å´„Åä„Åë„Çã„Éï„Ç°„Ç§„Éä„É≥„Ç∑„É£„É´„É™„Çø„Éº„É≥„ÅÆÂΩ±Èüø„ÇíÂçÅÂàÜ„Å´ËÄÉÂØü„Åó„Å¶„Åä„Çâ„Åö„ÄÅ„Åù„ÅÆ„É°„Ç´„Éã„Ç∫„É†„Åå‰∏çÊòéÁ¢∫„Åß„ÅÇ„Çã„ÄÇÁâπ„Å´„ÄÅ„Éï„Ç°„Ç§„Éä„É≥„Ç∑„É£„É´„É™„Çø„Éº„É≥ÈáçË¶ñ„Åå‰ºÅÊ•≠„ÅÆÊé¢Á¥¢Ë°åÂãï„Å´‰∏é„Åà„ÇãÂΩ±Èüø„Å´Èñ¢„Åô„ÇãÂÆüË®ºÁ†îÁ©∂„ÅØ‰∏çË∂≥„Åó„Å¶„ÅÑ„Çã   \n",
       "3                                                    Âà∂Â∫¶Áí∞Â¢É„ÅåCVC„ÅÆÊäïË≥áÊàêÂäü„Å´‰∏é„Åà„ÇãÂΩ±Èüø„Å´„Å§„ÅÑ„Å¶„ÅÆÂÆüË®ºÁ†îÁ©∂„Åå‰∏çË∂≥„Åó„Å¶„Åä„Çä„ÄÅÁâπ„Å´Âà∂Â∫¶„ÅÆË≥™„Å®ÊäïË≥áÊàêÊûú„ÅÆÈñ¢‰øÇ„Åå‰∏çÊòé„Åß„ÅÇ„Çã   \n",
       "4                                     CVC„ÅÆÊäïË≥áÊàêÊûú„Å®Âú∞ÂüüÁâπÊÄß„ÅÆÈñ¢ÈÄ£ÊÄß„Å´„Å§„ÅÑ„Å¶„ÅÆÂÆüË®ºÁ†îÁ©∂„Åå‰∏çË∂≥„Åó„Å¶„Åä„Çä„ÄÅÁâπ„Å´„Éï„Ç°„Ç§„Éä„É≥„Ç∑„É£„É´„É™„Çø„Éº„É≥ÈáçË¶ñ„ÅÆÂΩ±Èüø„ÇíÂú∞Âüü„Åî„Å®„Å´ÂàÜÊûê„Åó„ÅüÁ†îÁ©∂„ÅØÁöÜÁÑ°„Åß„ÅÇ„Çã   \n",
       "\n",
       "                                                                             Proposed Approach  \\\n",
       "0                          CVC„Åä„Çà„Å≥IVC„Åã„ÇâË≥áÈáë„ÇíÂèó„Åë„Åü‰ºÅÊ•≠„ÅÆ„Ç§„Éé„Éô„Éº„Ç∑„Éß„É≥ÊàêÊûú„ÇíÊØîËºÉ„Åô„Çã„Åì„Å®„Åß„ÄÅ‰∏°ËÄÖ„ÅÆÊäïË≥áË°åÂãï„ÅÆÈÅï„ÅÑ„Åå‰ºÅÊ•≠„ÅÆÂ≠¶ÁøíÊàêÊûú„Å´‰∏é„Åà„ÇãÂΩ±Èüø„ÇíÂàÜÊûê„Åô„Çã   \n",
       "1                       CVC„ÇíÂÆüÊñΩ„Åó„Åü‰ºÅÊ•≠„ÅÆÊàêÈï∑ÊåáÊ®ô„ÇíËøΩË∑°„Åó„ÄÅCVC„ÅÆÂÆüÊñΩ„ÅåÈï∑ÊúüÁöÑ„Å™ÊàêÈï∑„Å´„Å©„ÅÆ„Çà„ÅÜ„Å´ÂØÑ‰∏é„Åô„Çã„Åã„ÇíÂàÜÊûê„Åô„Çã„ÄÇÂÆöÈáèÁöÑ„Éá„Éº„Çø„Å®Ë≥™ÁöÑ„Ç§„É≥„Çø„Éì„É•„Éº„Çí‰ΩµÁî®„Åô„Çã   \n",
       "2  CVC„ÇíÂà©Áî®„Åô„Çã‰ºÅÊ•≠„ÇíÂØæË±°„Å´„ÄÅ„Éï„Ç°„Ç§„Éä„É≥„Ç∑„É£„É´„É™„Çø„Éº„É≥ÈáçË¶ñÂ∫¶„Å®Â≠¶ÁøíÊàêÊûú„ÅÆÁõ∏Èñ¢„ÇíÂ§öÂ§âÈáèÂàÜÊûê„ÇíÁî®„ÅÑ„Å¶Ê§úË®º„Åô„Çã„ÄÇ„Éá„Éº„Çø„Å®„Åó„Å¶„ÅØ„ÄÅCVCÊäïË≥áÂæå„ÅÆ„Ç§„Éé„Éô„Éº„Ç∑„Éß„É≥ÊàêÊûú„ÇÑÊàêÈï∑ÊåáÊ®ô„ÇíÁî®„ÅÑ„Çã‰∫àÂÆö„Åß„ÅÇ„Çã   \n",
       "3                                 ÂõΩ„ÇÑÂú∞Âüü„ÅÆÂà∂Â∫¶Áí∞Â¢ÉÊåáÊ®ô„ÇíÁî®„ÅÑ„Å¶„ÄÅCVC„ÅÆÊäïË≥áÊàêÂäü„Å®„ÅÆÈñ¢‰øÇ„ÇíÂ§öÂ§âÈáèÂõûÂ∏∞ÂàÜÊûê„ÅßÊ§úË®º„Åô„Çã„ÄÇÂà∂Â∫¶Áí∞Â¢É„ÅÆË≥™„ÇíÊìç‰ΩúÂ§âÊï∞„Å®„Åó„Å¶ËÄÉÊÖÆ„Åô„Çã   \n",
       "4                  Âú∞ÂüüÂà•„ÅÆCVCÊäïË≥á„Éá„Éº„Çø„ÇíÁî®„ÅÑ„Å¶„ÄÅ„Éï„Ç°„Ç§„Éä„É≥„Ç∑„É£„É´„É™„Çø„Éº„É≥ÈáçË¶ñ„ÅÆÁ®ãÂ∫¶„Å®ÊäïË≥áÊàêÊûú„Å®„ÅÆÈñ¢‰øÇ„ÇíÂ§öÂ§âÈáèÂõûÂ∏∞ÂàÜÊûê„ÅßÊ§úË®º„Åô„Çã„ÄÇÂú∞ÂüüÁâπÊÄßÊåáÊ®ô„Çí‰∫§Áµ°Âõ†Â≠ê„Å®„Åó„Å¶ËÄÉÊÖÆ„Åô„Çã   \n",
       "\n",
       "  Priority Status  \\\n",
       "0   Medium    New   \n",
       "1   Medium    New   \n",
       "2   Medium    New   \n",
       "3   Medium    New   \n",
       "4   Medium    New   \n",
       "\n",
       "                                                                                               Tags  \\\n",
       "0                                    [CVC, IVC, Learning Outcomes, Investment Behavior, Innovation]   \n",
       "1           [CVC, Long-term Growth, Investment Impact, Qualitative Research, Quantitative Research]   \n",
       "2                                            [CVC, Learning, Financial Returns, Innovation, Growth]   \n",
       "3  [CVC, Institutional Environment, Investment Success, Policy Implications, Quantitative Analysis]   \n",
       "4              [CVC, Regional Differences, Financial Returns, Investment Outcomes, Policy Analysis]   \n",
       "\n",
       "  evidence_used  evidence_count  tag_count  \n",
       "0        [1, 2]               2          5  \n",
       "1        [1, 2]               2          5  \n",
       "2        [1, 2]               2          5  \n",
       "3        [1, 2]               2          5  \n",
       "4        [1, 2]               2          5  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Selected (Go-approved) ---\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>Rationale / Background</th>\n",
       "      <th>Gap Identified</th>\n",
       "      <th>Proposed Approach</th>\n",
       "      <th>Priority</th>\n",
       "      <th>Status</th>\n",
       "      <th>Tags</th>\n",
       "      <th>evidence_used</th>\n",
       "      <th>evidence_count</th>\n",
       "      <th>tag_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CVC„ÅÆÊàêÈï∑ÈÅéÁ®ã„Å´„Åä„Åë„ÇãÂ≠¶ÁøíÂäπÊûú„ÅØ„ÄÅ„Éï„Ç°„Ç§„Éä„É≥„Ç∑„É£„É´„É™„Çø„Éº„É≥ÈáçË¶ñ„ÅÆÂßøÂã¢„Å´„Çà„Å£„Å¶„Å©„ÅÆ„Çà„ÅÜ„Å´Â§âÂåñ„Åô„Çã„ÅãÔºü</td>\n",
       "      <td>CVC„ÅØÂçò„Å™„ÇãË≥áÈáëÊèê‰æõ„Å´Áïô„Åæ„Çâ„Åö„ÄÅ‰ºÅÊ•≠„ÅÆÂ≠¶Áøí„ÇÑÊàêÈï∑„Å´ÂØÑ‰∏é„Åô„ÇãÈáçË¶Å„Å™Ë¶ÅÁ¥†„Åß„ÅÇ„Çã„ÄÇÊú¨Á†îÁ©∂„ÅØ„Éï„Ç°„Ç§„Éä„É≥„Ç∑„É£„É´„É™„Çø„Éº„É≥ÈáçË¶ñ„ÅåCVC„ÅÆÊàêÈï∑ÈÅéÁ®ã„Å´‰∏é„Åà„ÇãÂΩ±Èüø„ÇíÊòé„Çâ„Åã„Å´„Åô„Çã„Åì„Å®„Åß„ÄÅÂÆüÂãôËÄÖ„Å´„Å®„Å£„Å¶„ÅÆÁ§∫ÂîÜ„ÇíÊèê‰æõ„Åô„Çã„Åì„Å®„ÇíÁõÆÊåá„Åô</td>\n",
       "      <td>ÂæìÊù•„ÅÆÁ†îÁ©∂„ÅØCVC„ÅÆÂ≠¶ÁøíÈÅéÁ®ã„Å´„Åä„Åë„Çã„Éï„Ç°„Ç§„Éä„É≥„Ç∑„É£„É´„É™„Çø„Éº„É≥„ÅÆÂΩ±Èüø„ÇíÂçÅÂàÜ„Å´ËÄÉÂØü„Åó„Å¶„Åä„Çâ„Åö„ÄÅ„Åù„ÅÆ„É°„Ç´„Éã„Ç∫„É†„Åå‰∏çÊòéÁ¢∫„Åß„ÅÇ„Çã„ÄÇÁâπ„Å´„ÄÅ„Éï„Ç°„Ç§„Éä„É≥„Ç∑„É£„É´„É™„Çø„Éº„É≥ÈáçË¶ñ„Åå‰ºÅÊ•≠„ÅÆÊé¢Á¥¢Ë°åÂãï„Å´‰∏é„Åà„ÇãÂΩ±Èüø„Å´Èñ¢„Åô„ÇãÂÆüË®ºÁ†îÁ©∂„ÅØ‰∏çË∂≥„Åó„Å¶„ÅÑ„Çã</td>\n",
       "      <td>CVC„ÇíÂà©Áî®„Åô„Çã‰ºÅÊ•≠„ÇíÂØæË±°„Å´„ÄÅ„Éï„Ç°„Ç§„Éä„É≥„Ç∑„É£„É´„É™„Çø„Éº„É≥ÈáçË¶ñÂ∫¶„Å®Â≠¶ÁøíÊàêÊûú„ÅÆÁõ∏Èñ¢„ÇíÂ§öÂ§âÈáèÂàÜÊûê„ÇíÁî®„ÅÑ„Å¶Ê§úË®º„Åô„Çã„ÄÇ„Éá„Éº„Çø„Å®„Åó„Å¶„ÅØ„ÄÅCVCÊäïË≥áÂæå„ÅÆ„Ç§„Éé„Éô„Éº„Ç∑„Éß„É≥ÊàêÊûú„ÇÑÊàêÈï∑ÊåáÊ®ô„ÇíÁî®„ÅÑ„Çã‰∫àÂÆö„Åß„ÅÇ„Çã</td>\n",
       "      <td>Medium</td>\n",
       "      <td>New</td>\n",
       "      <td>[CVC, Learning, Financial Returns, Innovation, Growth]</td>\n",
       "      <td>[1, 2]</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 Name  \\\n",
       "0  CVC„ÅÆÊàêÈï∑ÈÅéÁ®ã„Å´„Åä„Åë„ÇãÂ≠¶ÁøíÂäπÊûú„ÅØ„ÄÅ„Éï„Ç°„Ç§„Éä„É≥„Ç∑„É£„É´„É™„Çø„Éº„É≥ÈáçË¶ñ„ÅÆÂßøÂã¢„Å´„Çà„Å£„Å¶„Å©„ÅÆ„Çà„ÅÜ„Å´Â§âÂåñ„Åô„Çã„ÅãÔºü   \n",
       "\n",
       "                                                                                     Rationale / Background  \\\n",
       "0  CVC„ÅØÂçò„Å™„ÇãË≥áÈáëÊèê‰æõ„Å´Áïô„Åæ„Çâ„Åö„ÄÅ‰ºÅÊ•≠„ÅÆÂ≠¶Áøí„ÇÑÊàêÈï∑„Å´ÂØÑ‰∏é„Åô„ÇãÈáçË¶Å„Å™Ë¶ÅÁ¥†„Åß„ÅÇ„Çã„ÄÇÊú¨Á†îÁ©∂„ÅØ„Éï„Ç°„Ç§„Éä„É≥„Ç∑„É£„É´„É™„Çø„Éº„É≥ÈáçË¶ñ„ÅåCVC„ÅÆÊàêÈï∑ÈÅéÁ®ã„Å´‰∏é„Åà„ÇãÂΩ±Èüø„ÇíÊòé„Çâ„Åã„Å´„Åô„Çã„Åì„Å®„Åß„ÄÅÂÆüÂãôËÄÖ„Å´„Å®„Å£„Å¶„ÅÆÁ§∫ÂîÜ„ÇíÊèê‰æõ„Åô„Çã„Åì„Å®„ÇíÁõÆÊåá„Åô   \n",
       "\n",
       "                                                                                               Gap Identified  \\\n",
       "0  ÂæìÊù•„ÅÆÁ†îÁ©∂„ÅØCVC„ÅÆÂ≠¶ÁøíÈÅéÁ®ã„Å´„Åä„Åë„Çã„Éï„Ç°„Ç§„Éä„É≥„Ç∑„É£„É´„É™„Çø„Éº„É≥„ÅÆÂΩ±Èüø„ÇíÂçÅÂàÜ„Å´ËÄÉÂØü„Åó„Å¶„Åä„Çâ„Åö„ÄÅ„Åù„ÅÆ„É°„Ç´„Éã„Ç∫„É†„Åå‰∏çÊòéÁ¢∫„Åß„ÅÇ„Çã„ÄÇÁâπ„Å´„ÄÅ„Éï„Ç°„Ç§„Éä„É≥„Ç∑„É£„É´„É™„Çø„Éº„É≥ÈáçË¶ñ„Åå‰ºÅÊ•≠„ÅÆÊé¢Á¥¢Ë°åÂãï„Å´‰∏é„Åà„ÇãÂΩ±Èüø„Å´Èñ¢„Åô„ÇãÂÆüË®ºÁ†îÁ©∂„ÅØ‰∏çË∂≥„Åó„Å¶„ÅÑ„Çã   \n",
       "\n",
       "                                                                             Proposed Approach  \\\n",
       "0  CVC„ÇíÂà©Áî®„Åô„Çã‰ºÅÊ•≠„ÇíÂØæË±°„Å´„ÄÅ„Éï„Ç°„Ç§„Éä„É≥„Ç∑„É£„É´„É™„Çø„Éº„É≥ÈáçË¶ñÂ∫¶„Å®Â≠¶ÁøíÊàêÊûú„ÅÆÁõ∏Èñ¢„ÇíÂ§öÂ§âÈáèÂàÜÊûê„ÇíÁî®„ÅÑ„Å¶Ê§úË®º„Åô„Çã„ÄÇ„Éá„Éº„Çø„Å®„Åó„Å¶„ÅØ„ÄÅCVCÊäïË≥áÂæå„ÅÆ„Ç§„Éé„Éô„Éº„Ç∑„Éß„É≥ÊàêÊûú„ÇÑÊàêÈï∑ÊåáÊ®ô„ÇíÁî®„ÅÑ„Çã‰∫àÂÆö„Åß„ÅÇ„Çã   \n",
       "\n",
       "  Priority Status                                                    Tags  \\\n",
       "0   Medium    New  [CVC, Learning, Financial Returns, Innovation, Growth]   \n",
       "\n",
       "  evidence_used  evidence_count  tag_count  \n",
       "0        [1, 2]               2          5  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# Cell 8 : Export Results and Basic Inspection\n",
    "# ============================================================\n",
    "#\n",
    "# This cell exports post-processed RQ candidates to disk and provides\n",
    "# lightweight inspection helpers for quick review.\n",
    "#\n",
    "# Exports:\n",
    "# - ranked candidates (rq_ranked_df)          -> CSV + JSON\n",
    "# - removed/duplicate candidates (rq_removed_df) -> CSV\n",
    "# - Go-approved candidates (rq_selected_clean_df, if available) -> CSV + JSON\n",
    "#\n",
    "# It also creates a small \"run manifest\" JSON file to make results traceable:\n",
    "# - timestamp\n",
    "# - vector_store_id\n",
    "# - retrieval / generation settings\n",
    "# - input source counts\n",
    "#\n",
    "# Requirements (from previous cells):\n",
    "# - rq_ranked_df, rq_removed_df, rq_df_clean (Cell 7)\n",
    "# - optionally rq_selected_clean_df (Cell 7)\n",
    "# - OUTPUT_DIR (Cell 1)\n",
    "#\n",
    "import pandas as pd\n",
    "import json\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "\n",
    "# ----------------------------\n",
    "# Safety checks\n",
    "# ----------------------------\n",
    "if \"rq_ranked_df\" not in globals() or rq_ranked_df is None or len(rq_ranked_df) == 0:\n",
    "    raise ValueError(\"rq_ranked_df is empty. Run Cell 6 and Cell 7 first.\")\n",
    "\n",
    "# ----------------------------\n",
    "# Output paths\n",
    "# ----------------------------\n",
    "RUN_TS = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "EXPORT_DIR = OUTPUT_DIR / f\"rq_run_{RUN_TS}\"\n",
    "EXPORT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "CANDIDATES_CSV = EXPORT_DIR / \"rq_candidates_ranked.csv\"\n",
    "CANDIDATES_JSON = EXPORT_DIR / \"rq_candidates_ranked.json\"\n",
    "REMOVED_CSV = EXPORT_DIR / \"rq_candidates_removed.csv\"\n",
    "\n",
    "SELECTED_CSV = EXPORT_DIR / \"rq_selected.csv\"\n",
    "SELECTED_JSON = EXPORT_DIR / \"rq_selected.json\"\n",
    "\n",
    "MANIFEST_JSON = EXPORT_DIR / \"run_manifest.json\"\n",
    "\n",
    "print(f\"üì¶ Export directory: {EXPORT_DIR}\")\n",
    "\n",
    "# ----------------------------\n",
    "# Helper: DataFrame -> JSON (records)\n",
    "# ----------------------------\n",
    "def df_to_records(df: pd.DataFrame) -> list:\n",
    "    \"\"\"\n",
    "    Convert a DataFrame to JSON-serializable records.\n",
    "    Safely handles numpy types.\n",
    "    \"\"\"\n",
    "    records = json.loads(df.to_json(orient=\"records\", force_ascii=False))\n",
    "    return records\n",
    "\n",
    "# ----------------------------\n",
    "# Export: ranked candidates\n",
    "# ----------------------------\n",
    "rq_ranked_df.to_csv(CANDIDATES_CSV, index=False)\n",
    "with open(CANDIDATES_JSON, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(df_to_records(rq_ranked_df), f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(f\"‚úÖ Exported ranked candidates: {CANDIDATES_CSV.name}, {CANDIDATES_JSON.name}\")\n",
    "\n",
    "# ----------------------------\n",
    "# Export: removed candidates\n",
    "# ----------------------------\n",
    "if \"rq_removed_df\" in globals() and rq_removed_df is not None and len(rq_removed_df) > 0:\n",
    "    rq_removed_df.to_csv(REMOVED_CSV, index=False)\n",
    "    print(f\"‚úÖ Exported removed candidates: {REMOVED_CSV.name}\")\n",
    "else:\n",
    "    print(\"‚ÑπÔ∏è No removed candidates to export.\")\n",
    "\n",
    "# ----------------------------\n",
    "# Export: selected candidates (if any)\n",
    "# ----------------------------\n",
    "if \"rq_selected_clean_df\" in globals() and rq_selected_clean_df is not None and len(rq_selected_clean_df) > 0:\n",
    "    rq_selected_clean_df.to_csv(SELECTED_CSV, index=False)\n",
    "    with open(SELECTED_JSON, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(df_to_records(rq_selected_clean_df), f, ensure_ascii=False, indent=2)\n",
    "    print(f\"‚úÖ Exported selected candidates: {SELECTED_CSV.name}, {SELECTED_JSON.name}\")\n",
    "else:\n",
    "    print(\"‚ÑπÔ∏è No selected candidates to export.\")\n",
    "\n",
    "# ----------------------------\n",
    "# Run manifest (traceability)\n",
    "# ----------------------------\n",
    "manifest = {\n",
    "    \"run_timestamp\": RUN_TS,\n",
    "    \"export_dir\": str(EXPORT_DIR),\n",
    "    \"vector_store_id\": globals().get(\"VECTOR_STORE_ID\"),\n",
    "    \"llm_model\": globals().get(\"LLM_MODEL_STRUCTURED\", globals().get(\"LLM_MODEL\", None)),\n",
    "    \"embedding_model\": globals().get(\"EMBEDDING_MODEL\", None),\n",
    "    \"top_k_retrieval\": globals().get(\"TOP_K_RETRIEVAL\", None),\n",
    "    \"min_similarity\": globals().get(\"MIN_SIMILARITY\", None),\n",
    "    \"temperature\": globals().get(\"TEMPERATURE\", None),\n",
    "    \"max_tokens\": globals().get(\"MAX_TOKENS\", None),\n",
    "    \"n_candidates_generated\": int(len(rq_df)) if \"rq_df\" in globals() and rq_df is not None else None,\n",
    "    \"n_candidates_ranked\": int(len(rq_ranked_df)),\n",
    "    \"n_candidates_selected\": int(len(rq_selected_clean_df)) if \"rq_selected_clean_df\" in globals() and rq_selected_clean_df is not None else 0,\n",
    "    \"source_file_counts\": {\n",
    "        \"drive_pdfs\": int(len(globals().get(\"drive_downloaded_paths\", []))),\n",
    "        \"notion_rq_txt\": int(len(globals().get(\"notion_text_paths\", []))),\n",
    "        \"all_rag_files\": int(len(globals().get(\"ALL_RAG_FILE_PATHS\", []))),\n",
    "    },\n",
    "}\n",
    "\n",
    "with open(MANIFEST_JSON, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(manifest, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(f\"üßæ Saved run manifest: {MANIFEST_JSON.name}\")\n",
    "\n",
    "# ----------------------------\n",
    "# Basic inspection\n",
    "# ----------------------------\n",
    "print(\"\\n--- Quick inspection ---\")\n",
    "display(rq_ranked_df.head(15))\n",
    "\n",
    "if \"rq_selected_clean_df\" in globals() and rq_selected_clean_df is not None and len(rq_selected_clean_df) > 0:\n",
    "    print(\"\\n--- Selected (Go-approved) ---\")\n",
    "    display(rq_selected_clean_df)\n",
    "\n",
    "print(\"\\nDone.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "441a5ea6-bf68-485d-8e2f-211cd22f447c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Notion DB schema + options look OK.\n",
      "‚úÖ Created 1 Notion pages.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>idx</th>\n",
       "      <th>Name</th>\n",
       "      <th>notion_page_id</th>\n",
       "      <th>notion_url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>CVC„ÅÆÊàêÈï∑ÈÅéÁ®ã„Å´„Åä„Åë„ÇãÂ≠¶ÁøíÂäπÊûú„ÅØ„ÄÅ„Éï„Ç°„Ç§„Éä„É≥„Ç∑„É£„É´„É™„Çø„Éº„É≥ÈáçË¶ñ„ÅÆÂßøÂã¢„Å´„Çà„Å£„Å¶„Å©„ÅÆ„Çà„ÅÜ„Å´Â§âÂåñ„Åô„Çã„ÅãÔºü</td>\n",
       "      <td>2df8e0e4-d162-81e3-8476-fb523bf51aed</td>\n",
       "      <td>https://www.notion.so/CVC-2df8e0e4d16281e38476fb523bf51aed</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   idx                                                Name  \\\n",
       "0    1  CVC„ÅÆÊàêÈï∑ÈÅéÁ®ã„Å´„Åä„Åë„ÇãÂ≠¶ÁøíÂäπÊûú„ÅØ„ÄÅ„Éï„Ç°„Ç§„Éä„É≥„Ç∑„É£„É´„É™„Çø„Éº„É≥ÈáçË¶ñ„ÅÆÂßøÂã¢„Å´„Çà„Å£„Å¶„Å©„ÅÆ„Çà„ÅÜ„Å´Â§âÂåñ„Åô„Çã„ÅãÔºü   \n",
       "\n",
       "                         notion_page_id  \\\n",
       "0  2df8e0e4-d162-81e3-8476-fb523bf51aed   \n",
       "\n",
       "                                                   notion_url  \n",
       "0  https://www.notion.so/CVC-2df8e0e4d16281e38476fb523bf51aed  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ============================================================\n",
    "# Cell 9 : Write Selected RQs to Notion (DB-aligned) [FULL VERSION]\n",
    "# ============================================================\n",
    "#\n",
    "# This cell writes Go-approved (selected) RQs into your Notion database\n",
    "# with the EXACT property structure:\n",
    "# - Name (title)                 : Japanese Research Question\n",
    "# - Rationale / Background       : Japanese\n",
    "# - Gap Identified               : Japanese\n",
    "# - Proposed Approach            : Japanese\n",
    "# - Priority (select)            : \"Medium\" (fixed)\n",
    "# - Status (status)              : \"New\" (fixed)\n",
    "# - Tags (multi_select)          : ~5 English keywords\n",
    "#\n",
    "# It also performs:\n",
    "# - DB schema validation (property existence + type match)\n",
    "# - Option existence validation (Priority option \"Medium\", Status option \"New\")\n",
    "# - Safe parsing for Tags (list or JSON string or comma-separated string)\n",
    "#\n",
    "\n",
    "import time\n",
    "import json\n",
    "import requests\n",
    "import pandas as pd\n",
    "\n",
    "NOTION_API_BASE = \"https://api.notion.com/v1\"\n",
    "NOTION_WRITE_SLEEP_SEC = 0.35\n",
    "\n",
    "PRIORITY_VALUE = \"Medium\"\n",
    "STATUS_VALUE = \"New\"\n",
    "\n",
    "# ----------------------------\n",
    "# Safety checks\n",
    "# ----------------------------\n",
    "if \"rq_selected_clean_df\" not in globals() or rq_selected_clean_df is None or len(rq_selected_clean_df) == 0:\n",
    "    raise ValueError(\"rq_selected_clean_df is empty. Please select (Go) some RQs first.\")\n",
    "\n",
    "if \"NOTION_HEADERS\" not in globals() or not isinstance(NOTION_HEADERS, dict):\n",
    "    raise ValueError(\"NOTION_HEADERS is missing. Ensure Cell 1 (Notion config) ran successfully.\")\n",
    "\n",
    "if \"NOTION_RQ_DB_ID\" not in globals() or not NOTION_RQ_DB_ID:\n",
    "    raise ValueError(\"NOTION_RQ_DB_ID is missing. Set it in env.txt and run Cell 1.\")\n",
    "\n",
    "# ----------------------------\n",
    "# Notion property builders\n",
    "# ----------------------------\n",
    "def prop_title(text: str):\n",
    "    text = \"\" if text is None else str(text)\n",
    "    return {\"title\": [{\"type\": \"text\", \"text\": {\"content\": text}}]}\n",
    "\n",
    "def prop_rich_text(text: str):\n",
    "    text = \"\" if text is None else str(text)\n",
    "    return {\"rich_text\": [{\"type\": \"text\", \"text\": {\"content\": text}}]} if text.strip() else {\"rich_text\": []}\n",
    "\n",
    "def prop_select(name: str):\n",
    "    name = \"\" if name is None else str(name).strip()\n",
    "    return {\"select\": {\"name\": name}} if name else {\"select\": None}\n",
    "\n",
    "def prop_status(name: str):\n",
    "    name = \"\" if name is None else str(name).strip()\n",
    "    return {\"status\": {\"name\": name}} if name else {\"status\": None}\n",
    "\n",
    "def prop_multi_select(values):\n",
    "    values = values or []\n",
    "    return {\"multi_select\": [{\"name\": str(v).strip()} for v in values if str(v).strip()]}\n",
    "\n",
    "# ----------------------------\n",
    "# Notion API helpers\n",
    "# ----------------------------\n",
    "def notion_get_database(database_id: str):\n",
    "    url = f\"{NOTION_API_BASE}/databases/{database_id}\"\n",
    "    r = requests.get(url, headers=NOTION_HEADERS, timeout=60)\n",
    "    r.raise_for_status()\n",
    "    return r.json()\n",
    "\n",
    "def notion_create_page(database_id: str, properties: dict):\n",
    "    url = f\"{NOTION_API_BASE}/pages\"\n",
    "    payload = {\"parent\": {\"database_id\": database_id}, \"properties\": properties}\n",
    "    r = requests.post(url, headers=NOTION_HEADERS, json=payload, timeout=60)\n",
    "    r.raise_for_status()\n",
    "    return r.json()\n",
    "\n",
    "def _option_names_for_property(prop_def: dict) -> list:\n",
    "    \"\"\"\n",
    "    Extract option names for select/status properties.\n",
    "    \"\"\"\n",
    "    t = prop_def.get(\"type\")\n",
    "    if t == \"select\":\n",
    "        return [o.get(\"name\") for o in (prop_def.get(\"select\", {}).get(\"options\", []) or [])]\n",
    "    if t == \"status\":\n",
    "        return [o.get(\"name\") for o in (prop_def.get(\"status\", {}).get(\"options\", []) or [])]\n",
    "    return []\n",
    "\n",
    "def assert_notion_db_schema_and_options():\n",
    "    \"\"\"\n",
    "    Validate:\n",
    "    - required properties exist\n",
    "    - required property types match\n",
    "    - required select/status options exist\n",
    "    \"\"\"\n",
    "    db = notion_get_database(NOTION_RQ_DB_ID)\n",
    "    props = db.get(\"properties\", {})\n",
    "\n",
    "    required = {\n",
    "        \"Name\": \"title\",\n",
    "        \"Rationale / Background\": \"rich_text\",\n",
    "        \"Gap Identified\": \"rich_text\",\n",
    "        \"Proposed Approach\": \"rich_text\",\n",
    "        \"Priority\": \"select\",\n",
    "        \"Status\": \"status\",\n",
    "        \"Tags\": \"multi_select\",\n",
    "    }\n",
    "\n",
    "    missing = [k for k in required.keys() if k not in props]\n",
    "    if missing:\n",
    "        raise ValueError(f\"Notion DB is missing properties: {missing}\")\n",
    "\n",
    "    mismatched = []\n",
    "    for k, expected_type in required.items():\n",
    "        actual = props[k].get(\"type\")\n",
    "        if actual != expected_type:\n",
    "            mismatched.append((k, expected_type, actual))\n",
    "    if mismatched:\n",
    "        raise ValueError(f\"Notion DB property type mismatch: {mismatched}\")\n",
    "\n",
    "    # Validate options\n",
    "    pr_options = _option_names_for_property(props[\"Priority\"])\n",
    "    if PRIORITY_VALUE not in pr_options:\n",
    "        raise ValueError(\n",
    "            f'Notion DB \"Priority\" select is missing option \"{PRIORITY_VALUE}\". '\n",
    "            f\"Available options: {pr_options}\"\n",
    "        )\n",
    "\n",
    "    st_options = _option_names_for_property(props[\"Status\"])\n",
    "    if STATUS_VALUE not in st_options:\n",
    "        raise ValueError(\n",
    "            f'Notion DB \"Status\" status is missing option \"{STATUS_VALUE}\". '\n",
    "            f\"Available options: {st_options}\"\n",
    "        )\n",
    "\n",
    "    print(\"‚úÖ Notion DB schema + options look OK.\")\n",
    "\n",
    "# ----------------------------\n",
    "# Tag parsing helper\n",
    "# ----------------------------\n",
    "def parse_tags(tags_value):\n",
    "    \"\"\"\n",
    "    Accept:\n",
    "      - list[str]\n",
    "      - JSON string like '[\"VC\",\"...\"]'\n",
    "      - comma-separated string like 'VC, Policy Shock, ...'\n",
    "    Returns: list[str]\n",
    "    \"\"\"\n",
    "    if tags_value is None:\n",
    "        return []\n",
    "    if isinstance(tags_value, list):\n",
    "        return [str(t).strip() for t in tags_value if str(t).strip()]\n",
    "    if isinstance(tags_value, str):\n",
    "        s = tags_value.strip()\n",
    "        if not s:\n",
    "            return []\n",
    "        # JSON list?\n",
    "        try:\n",
    "            v = json.loads(s)\n",
    "            if isinstance(v, list):\n",
    "                return [str(t).strip() for t in v if str(t).strip()]\n",
    "        except Exception:\n",
    "            pass\n",
    "        # comma-separated\n",
    "        return [t.strip() for t in s.split(\",\") if t.strip()]\n",
    "    # fallback\n",
    "    return [str(tags_value).strip()] if str(tags_value).strip() else []\n",
    "\n",
    "# ----------------------------\n",
    "# Validate Notion DB before writing\n",
    "# ----------------------------\n",
    "assert_notion_db_schema_and_options()\n",
    "\n",
    "# ----------------------------\n",
    "# Convert selected rows\n",
    "# ----------------------------\n",
    "rows = json.loads(rq_selected_clean_df.to_json(orient=\"records\", force_ascii=False))\n",
    "\n",
    "created = []\n",
    "errors = []\n",
    "\n",
    "for i, row in enumerate(rows, start=1):\n",
    "    try:\n",
    "        tags = parse_tags(row.get(\"Tags\", []))\n",
    "\n",
    "        # Build Notion properties (exact names must match DB)\n",
    "        props = {\n",
    "            \"Name\": prop_title(str(row.get(\"Name\", \"\"))[:2000]),\n",
    "            \"Rationale / Background\": prop_rich_text(row.get(\"Rationale / Background\", \"\")),\n",
    "            \"Gap Identified\": prop_rich_text(row.get(\"Gap Identified\", \"\")),\n",
    "            \"Proposed Approach\": prop_rich_text(row.get(\"Proposed Approach\", \"\")),\n",
    "            \"Priority\": prop_select(PRIORITY_VALUE),  # fixed\n",
    "            \"Status\": prop_status(STATUS_VALUE),      # fixed\n",
    "            \"Tags\": prop_multi_select(tags),\n",
    "        }\n",
    "\n",
    "        page = notion_create_page(NOTION_RQ_DB_ID, props)\n",
    "\n",
    "        created.append({\n",
    "            \"idx\": i,\n",
    "            \"Name\": row.get(\"Name\"),\n",
    "            \"notion_page_id\": page.get(\"id\"),\n",
    "            \"notion_url\": page.get(\"url\"),\n",
    "        })\n",
    "\n",
    "        time.sleep(NOTION_WRITE_SLEEP_SEC)\n",
    "\n",
    "    except Exception as e:\n",
    "        errors.append({\n",
    "            \"idx\": i,\n",
    "            \"Name\": row.get(\"Name\"),\n",
    "            \"error\": f\"{type(e).__name__}: {str(e)}\"\n",
    "        })\n",
    "        time.sleep(NOTION_WRITE_SLEEP_SEC)\n",
    "\n",
    "print(f\"‚úÖ Created {len(created)} Notion pages.\")\n",
    "if errors:\n",
    "    print(f\"‚ö†Ô∏è Errors: {len(errors)} (showing up to 5)\")\n",
    "    for x in errors[:5]:\n",
    "        print(x)\n",
    "\n",
    "created_df = pd.DataFrame(created)\n",
    "errors_df = pd.DataFrame(errors)\n",
    "\n",
    "display(created_df.head(30))\n",
    "if len(errors_df) > 0:\n",
    "    display(errors_df.head(30))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
