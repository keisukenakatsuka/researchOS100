{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e95406dd-6a4b-4e14-be40-d43135e7b832",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 010 Startup Initial Meeting Deep Dive\n",
    "# ============================================================\n",
    "#\n",
    "# Overview\n",
    "# ----------------\n",
    "# This notebook implements a structured deep-dive workflow following an\n",
    "# initial meeting with a startup.\n",
    "#\n",
    "# Starting from entity identification and factual grounding, it progressively\n",
    "# organizes public information into explicit structures, layers hypotheses\n",
    "# where information is incomplete, and culminates in concrete meeting questions\n",
    "# and actionable outputs.\n",
    "#\n",
    "# The objective is to convert a first, often qualitative interaction into a\n",
    "# traceable analytical artifact that clearly separates facts, hypotheses, and\n",
    "# actions, enabling more effective follow-up meetings and internal\n",
    "# decision-making.\n",
    "#\n",
    "# Wherever possible, factual statements are grounded in publicly available\n",
    "# sources and preserved with evidence links to maintain transparency and\n",
    "# traceability.\n",
    "#\n",
    "#\n",
    "# Inputs / Outputs\n",
    "# ----------------\n",
    "# Inputs:\n",
    "# - Startup identifiers and public references (website, articles, announcements)\n",
    "# - Initial meeting participant(s), roles, and meeting context\n",
    "# - Your organization’s perspective (investment, partnership, research, etc.)\n",
    "#\n",
    "# Outputs:\n",
    "# - Fact-grounded startup and people profiles\n",
    "# - Explicit hypotheses on business, market, and competitive positioning\n",
    "# - Structured meeting questions, NG questions, and watchouts\n",
    "# - One-page, meeting-ready summary in Markdown\n",
    "# - Reusable artifacts (JSON / CSV / Markdown) for comparison and downstream workflows\n",
    "#\n",
    "#\n",
    "# Structure\n",
    "# ----------------\n",
    "# 0. Notebook Metadata\n",
    "#    - Run ID, timestamps, and global configuration\n",
    "#\n",
    "# 1. Input Widget\n",
    "#    - Startup name, meeting participants, meeting context, and your org context\n",
    "#\n",
    "# 2. Startup Identification\n",
    "#    - Entity resolution and canonical identification using official sources and CSE\n",
    "#\n",
    "# 3. Company Basics\n",
    "#    - Core facts on business, product, customers, market, and funding signals\n",
    "#\n",
    "# 4. Key People Extraction\n",
    "#    - Identification of founders, executives, and key team members\n",
    "#\n",
    "# 5. Meeting Person Deep Dive\n",
    "#    - Background, prior roles, public statements, and inferred focus areas\n",
    "#\n",
    "# 6. Business & Product Understanding\n",
    "#    - Product scope, technical approach, value proposition, and differentiation\n",
    "#\n",
    "# 7. Customer & Market Structure\n",
    "#    - Target customers, use cases, market size signals, and adoption patterns\n",
    "#\n",
    "# 8. Competitive Landscape\n",
    "#    - Direct and indirect competitors, substitutes, and positioning signals\n",
    "#\n",
    "# 9. Funding & Cap Table Signals\n",
    "#    - Fundraising history, investors, and capital structure indicators\n",
    "#\n",
    "# 10. Recent Changes Timeline\n",
    "#     - Key developments, announcements, and shifts over recent months\n",
    "#\n",
    "# 11. Integrated Insights            (LLM synthesis)\n",
    "#     - Cross-sectional synthesis of facts, hypotheses, risks, and contradictions\n",
    "#\n",
    "# 12. Meeting Context Framing        (LLM synthesis)\n",
    "#     - Meeting objectives, narrative framing, agenda, and role allocation\n",
    "#\n",
    "# 13. Top 5 Meeting Questions        (LLM synthesis)\n",
    "#     - Priority questions linked to hypotheses, risks, and evidence\n",
    "#\n",
    "# 14. NG Questions & Watchouts       (LLM synthesis)\n",
    "#     - Questions to avoid and signals to watch for during the meeting\n",
    "#\n",
    "# 15. One-Page Summary Generation    (LLM synthesis)\n",
    "#     - Scannable, evidence-linked Markdown summary for internal sharing\n",
    "#\n",
    "# 16. Export & Save Artifacts\n",
    "#     - Run-level export and manifest for reuse and downstream workflows\n",
    "#\n",
    "#\n",
    "# Notes\n",
    "# ----------------\n",
    "# - Facts, hypotheses, and actions are intentionally separated.\n",
    "# - Hypotheses should be revisited and updated as new information emerges.\n",
    "# - Missing or uncertain information should be made explicit (e.g., UNKNOWN).\n",
    "# - This notebook is designed for iterative reuse across multiple startups\n",
    "#   and for comparison across runs within researchOS.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "faf0294c-d738-4ba9-b76d-b572eb2f12eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "## 1. Input Widget"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "Fill in the fields below. Click **Save Inputs** to persist them for downstream cells."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "79bbfb9a5d9042a0b24ce60d499250ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(VBox(children=(HTML(value='<b>Startup Context</b>'), Text(value='', description='Startup:', lay…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ============================================================\n",
    "# 1. Input Widget\n",
    "# ============================================================\n",
    "# Purpose:\n",
    "# - Capture the minimum required context for the deep dive.\n",
    "# - Keep inputs structured so downstream cells can be deterministic.\n",
    "# - Separate \"startup context\", \"meeting participant\", \"meeting context\",\n",
    "#   and \"your org context\" to avoid mixing assumptions later.\n",
    "\n",
    "from datetime import datetime, timezone\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, Markdown, clear_output\n",
    "\n",
    "# ---- UI Components ----\n",
    "startup_name = widgets.Text(\n",
    "    value=\"\",\n",
    "    description=\"Startup:\",\n",
    "    placeholder=\"e.g., Acme AI\",\n",
    "    layout=widgets.Layout(width=\"600px\"),\n",
    ")\n",
    "\n",
    "startup_website = widgets.Text(\n",
    "    value=\"\",\n",
    "    description=\"Website:\",\n",
    "    placeholder=\"e.g., https://acme.ai\",\n",
    "    layout=widgets.Layout(width=\"600px\"),\n",
    ")\n",
    "\n",
    "startup_country = widgets.Text(\n",
    "    value=\"\",\n",
    "    description=\"Country:\",\n",
    "    placeholder=\"e.g., Japan\",\n",
    "    layout=widgets.Layout(width=\"300px\"),\n",
    ")\n",
    "\n",
    "startup_sector = widgets.Text(\n",
    "    value=\"\",\n",
    "    description=\"Sector:\",\n",
    "    placeholder=\"e.g., FinTech, AI Infra, Climate\",\n",
    "    layout=widgets.Layout(width=\"600px\"),\n",
    ")\n",
    "\n",
    "meeting_person_name = widgets.Text(\n",
    "    value=\"\",\n",
    "    description=\"Person:\",\n",
    "    placeholder=\"e.g., Jane Doe\",\n",
    "    layout=widgets.Layout(width=\"600px\"),\n",
    ")\n",
    "\n",
    "meeting_person_title = widgets.Text(\n",
    "    value=\"\",\n",
    "    description=\"Title:\",\n",
    "    placeholder=\"e.g., CEO / Head of Sales / BizDev\",\n",
    "    layout=widgets.Layout(width=\"600px\"),\n",
    ")\n",
    "\n",
    "meeting_person_email = widgets.Text(\n",
    "    value=\"\",\n",
    "    description=\"Email:\",\n",
    "    placeholder=\"(optional)\",\n",
    "    layout=widgets.Layout(width=\"600px\"),\n",
    ")\n",
    "\n",
    "meeting_date = widgets.DatePicker(\n",
    "    description=\"Date:\",\n",
    ")\n",
    "\n",
    "meeting_type = widgets.Dropdown(\n",
    "    options=[\"Intro\", \"Follow-up\", \"Partnering\", \"Investment\", \"Hiring\", \"Other\"],\n",
    "    value=\"Intro\",\n",
    "    description=\"Type:\",\n",
    ")\n",
    "\n",
    "meeting_goal = widgets.Textarea(\n",
    "    value=\"\",\n",
    "    description=\"Goal:\",\n",
    "    placeholder=\"What do you want to achieve from this meeting / deep dive?\",\n",
    "    layout=widgets.Layout(width=\"800px\", height=\"80px\"),\n",
    ")\n",
    "\n",
    "meeting_context = widgets.Textarea(\n",
    "    value=\"\",\n",
    "    description=\"Context:\",\n",
    "    placeholder=\"Any useful context from the initial meeting (notes, key claims, open questions).\",\n",
    "    layout=widgets.Layout(width=\"800px\", height=\"120px\"),\n",
    ")\n",
    "\n",
    "your_org_name = widgets.Text(\n",
    "    value=\"\",\n",
    "    description=\"Your org:\",\n",
    "    placeholder=\"e.g., XYZ Ventures\",\n",
    "    layout=widgets.Layout(width=\"600px\"),\n",
    ")\n",
    "\n",
    "your_org_role = widgets.Dropdown(\n",
    "    options=[\"VC\", \"CVC\", \"PE\", \"Corporate\", \"Accelerator\", \"Research\", \"Other\"],\n",
    "    value=\"VC\",\n",
    "    description=\"Role:\",\n",
    ")\n",
    "\n",
    "your_org_thesis = widgets.Textarea(\n",
    "    value=\"\",\n",
    "    description=\"Thesis:\",\n",
    "    placeholder=\"Your org’s perspective (investment thesis, partnership angle, strategic intent).\",\n",
    "    layout=widgets.Layout(width=\"800px\", height=\"100px\"),\n",
    ")\n",
    "\n",
    "confidentiality = widgets.Dropdown(\n",
    "    options=[\"Public-only\", \"Public + meeting notes\", \"Includes sensitive info\"],\n",
    "    value=\"Public + meeting notes\",\n",
    "    description=\"Data:\",\n",
    ")\n",
    "\n",
    "analyst_name = widgets.Text(\n",
    "    value=\"\",\n",
    "    description=\"Analyst:\",\n",
    "    placeholder=\"(optional) Your name\",\n",
    "    layout=widgets.Layout(width=\"400px\"),\n",
    ")\n",
    "\n",
    "run_id = widgets.Text(\n",
    "    value=datetime.now(timezone.utc).strftime(\"%Y%m%d_%H%M%S\"),\n",
    "    description=\"Run ID:\",\n",
    "    layout=widgets.Layout(width=\"300px\"),\n",
    ")\n",
    "\n",
    "save_button = widgets.Button(\n",
    "    description=\"Save Inputs\",\n",
    "    button_style=\"primary\",\n",
    "    tooltip=\"Validate and save inputs to /artifacts\",\n",
    ")\n",
    "\n",
    "status = widgets.Output()\n",
    "\n",
    "# ---- Layout ----\n",
    "display(Markdown(\"## 1. Input Widget\"))\n",
    "display(Markdown(\"Fill in the fields below. Click **Save Inputs** to persist them for downstream cells.\"))\n",
    "\n",
    "section_startup = widgets.VBox([\n",
    "    widgets.HTML(\"<b>Startup Context</b>\"),\n",
    "    startup_name, startup_website,\n",
    "    widgets.HBox([startup_country, widgets.Label(\"\")]),\n",
    "    startup_sector,\n",
    "])\n",
    "\n",
    "section_person = widgets.VBox([\n",
    "    widgets.HTML(\"<b>Meeting Participant</b>\"),\n",
    "    meeting_person_name, meeting_person_title, meeting_person_email,\n",
    "])\n",
    "\n",
    "section_meeting = widgets.VBox([\n",
    "    widgets.HTML(\"<b>Meeting Context</b>\"),\n",
    "    widgets.HBox([meeting_date, meeting_type, run_id]),\n",
    "    meeting_goal,\n",
    "    meeting_context,\n",
    "])\n",
    "\n",
    "section_org = widgets.VBox([\n",
    "    widgets.HTML(\"<b>Your Organization Context</b>\"),\n",
    "    your_org_name, your_org_role, analyst_name,\n",
    "    your_org_thesis,\n",
    "    confidentiality,\n",
    "])\n",
    "\n",
    "ui = widgets.VBox([\n",
    "    section_startup,\n",
    "    widgets.HTML(\"<hr>\"),\n",
    "    section_person,\n",
    "    widgets.HTML(\"<hr>\"),\n",
    "    section_meeting,\n",
    "    widgets.HTML(\"<hr>\"),\n",
    "    section_org,\n",
    "    widgets.HTML(\"<br>\"),\n",
    "    save_button,\n",
    "    status,\n",
    "])\n",
    "\n",
    "display(ui)\n",
    "\n",
    "# ---- Helpers ----\n",
    "def _validate_inputs():\n",
    "    errors = []\n",
    "    if not startup_name.value.strip():\n",
    "        errors.append(\"Startup name is required.\")\n",
    "    if not meeting_person_name.value.strip():\n",
    "        errors.append(\"Meeting participant name is required.\")\n",
    "    if not meeting_goal.value.strip():\n",
    "        errors.append(\"Meeting goal is required.\")\n",
    "    if not your_org_role.value.strip():\n",
    "        errors.append(\"Your org role is required.\")\n",
    "    return errors\n",
    "\n",
    "def _build_inputs_dict():\n",
    "    return {\n",
    "        \"meta\": {\n",
    "            \"run_id\": run_id.value.strip(),\n",
    "            \"created_at_utc\": datetime.now(timezone.utc).isoformat(),\n",
    "            \"analyst_name\": analyst_name.value.strip() or None,\n",
    "            \"data_classification\": confidentiality.value,\n",
    "        },\n",
    "        \"startup\": {\n",
    "            \"name\": startup_name.value.strip(),\n",
    "            \"website\": startup_website.value.strip() or None,\n",
    "            \"country\": startup_country.value.strip() or None,\n",
    "            \"sector\": startup_sector.value.strip() or None,\n",
    "        },\n",
    "        \"meeting_person\": {\n",
    "            \"name\": meeting_person_name.value.strip(),\n",
    "            \"title\": meeting_person_title.value.strip() or None,\n",
    "            \"email\": meeting_person_email.value.strip() or None,\n",
    "        },\n",
    "        \"meeting\": {\n",
    "            \"date\": str(meeting_date.value) if meeting_date.value else None,\n",
    "            \"type\": meeting_type.value,\n",
    "            \"goal\": meeting_goal.value.strip(),\n",
    "            \"context_notes\": meeting_context.value.strip() or None,\n",
    "        },\n",
    "        \"your_org\": {\n",
    "            \"name\": your_org_name.value.strip() or None,\n",
    "            \"role\": your_org_role.value,\n",
    "            \"thesis\": your_org_thesis.value.strip() or None,\n",
    "        },\n",
    "    }\n",
    "\n",
    "def _save_inputs(payload: dict, out_dir: Path):\n",
    "    out_dir.mkdir(parents=True, exist_ok=True)\n",
    "    out_path = out_dir / f\"inputs_{payload['meta']['run_id']}.json\"\n",
    "    out_path.write_text(json.dumps(payload, ensure_ascii=False, indent=2), encoding=\"utf-8\")\n",
    "    return out_path\n",
    "\n",
    "# ---- Callbacks ----\n",
    "def on_save_clicked(_):\n",
    "    with status:\n",
    "        clear_output()\n",
    "        errors = _validate_inputs()\n",
    "        if errors:\n",
    "            display(Markdown(\"### ❌ Validation errors\"))\n",
    "            for e in errors:\n",
    "                display(Markdown(f\"- {e}\"))\n",
    "            return\n",
    "\n",
    "        payload = _build_inputs_dict()\n",
    "        out_path = _save_inputs(payload, Path(\"artifacts\") / \"meeting_deep_dive\")\n",
    "        display(Markdown(\"### ✅ Saved\"))\n",
    "        display(Markdown(f\"- Run ID: `{payload['meta']['run_id']}`\"))\n",
    "        display(Markdown(f\"- Output: `{out_path.as_posix()}`\"))\n",
    "\n",
    "save_button.on_click(on_save_clicked)\n",
    "\n",
    "# ---- Final note ----\n",
    "# Downstream cells should only depend on the persisted JSON (not widget state),\n",
    "# to keep the workflow reproducible.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bc43db4a-0496-4870-9ad9-52f7686bf4d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Startup Identification complete\n",
      "- Startup: Sakana AI\n",
      "- Official website: https://sakana.ai/series-a\n",
      "- Method: heuristics_only\n",
      "- Saved: artifacts/meeting_deep_dive/entity_20260107_225628.json\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# 2. Startup Identification (Entity Resolution)\n",
    "# ============================================================\n",
    "# Goal:\n",
    "# - Confirm that the \"startup\" we analyze is the correct real-world entity.\n",
    "# - Use the provided URL (if any) as the primary anchor.\n",
    "# - Use Google CSE as a secondary check to reduce mis-identification risk.\n",
    "# - Produce a canonical entity record: official website + key reference URLs.\n",
    "#\n",
    "# Output (saved):\n",
    "# - artifacts/meeting_deep_dive/entity_<run_id>.json\n",
    "#\n",
    "# Notes:\n",
    "# - This cell does NOT attempt deep research; it only resolves identity.\n",
    "# - Downstream cells should rely on the resolved entity record, not raw inputs.\n",
    "\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "import time\n",
    "import hashlib\n",
    "import requests\n",
    "from urllib.parse import urlparse, urlunparse\n",
    "from pathlib import Path\n",
    "from datetime import datetime, timezone\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Global configuration (as provided)\n",
    "# ------------------------------------------------------------\n",
    "# Load env.txt explicitly (recommended for local + GitHub Actions parity)\n",
    "load_dotenv(\"env.txt\")\n",
    "\n",
    "# --- OpenAI API (required) ---\n",
    "import openai\n",
    "\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "if OPENAI_API_KEY is None:\n",
    "    raise EnvironmentError(\"OPENAI_API_KEY is not set in the environment variables.\")\n",
    "openai.api_key = OPENAI_API_KEY\n",
    "\n",
    "# --- Google Custom Search Engine (required for this notebook) ---\n",
    "GOOGLE_API_KEY = os.getenv(\"GOOGLE_API_KEY\")\n",
    "GOOGLE_CSE_CX = os.getenv(\"GOOGLE_CSE_CX\")\n",
    "\n",
    "if GOOGLE_API_KEY is None or GOOGLE_CSE_CX is None:\n",
    "    raise EnvironmentError(\n",
    "        \"GOOGLE_API_KEY and GOOGLE_CSE_CX must be set in the environment variables.\"\n",
    "    )\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Load inputs (from cell #1 artifacts)\n",
    "# ------------------------------------------------------------\n",
    "ART_DIR = Path(\"artifacts\") / \"meeting_deep_dive\"\n",
    "ART_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# If you already have `inputs` dict in memory, this will use it.\n",
    "# Otherwise, load the latest inputs_*.json from artifacts.\n",
    "def load_latest_inputs(art_dir: Path) -> dict:\n",
    "    candidates = sorted(art_dir.glob(\"inputs_*.json\"), key=lambda p: p.stat().st_mtime, reverse=True)\n",
    "    if not candidates:\n",
    "        raise FileNotFoundError(\"No inputs_*.json found. Please run cell #1 and save inputs first.\")\n",
    "    return json.loads(candidates[0].read_text(encoding=\"utf-8\"))\n",
    "\n",
    "try:\n",
    "    inputs  # noqa: F401\n",
    "except NameError:\n",
    "    inputs = load_latest_inputs(ART_DIR)\n",
    "\n",
    "run_id = inputs[\"meta\"][\"run_id\"]\n",
    "startup_name = (inputs.get(\"startup\", {}).get(\"name\") or \"\").strip()\n",
    "provided_website = (inputs.get(\"startup\", {}).get(\"website\") or \"\").strip()\n",
    "\n",
    "if not startup_name:\n",
    "    raise ValueError(\"Startup name is missing in inputs. Please fill it in the Input Widget.\")\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Utility helpers\n",
    "# ------------------------------------------------------------\n",
    "def normalize_url(url: str) -> str:\n",
    "    \"\"\"\n",
    "    Normalize URL to reduce trivial mismatches.\n",
    "    - Force scheme to https when missing\n",
    "    - Remove URL fragments\n",
    "    - Remove trailing slash (except root)\n",
    "    - Lowercase hostname\n",
    "    \"\"\"\n",
    "    if not url:\n",
    "        return \"\"\n",
    "    url = url.strip()\n",
    "    if not re.match(r\"^https?://\", url, flags=re.IGNORECASE):\n",
    "        url = \"https://\" + url\n",
    "\n",
    "    p = urlparse(url)\n",
    "    netloc = p.netloc.lower()\n",
    "    path = p.path or \"/\"\n",
    "    # strip fragment + query for identity checks (keep query only for some sites; but default strip)\n",
    "    norm = p._replace(scheme=\"https\", netloc=netloc, query=\"\", fragment=\"\")\n",
    "\n",
    "    # tidy path\n",
    "    if path != \"/\" and path.endswith(\"/\"):\n",
    "        path = path[:-1]\n",
    "    norm = norm._replace(path=path)\n",
    "\n",
    "    return urlunparse(norm)\n",
    "\n",
    "def domain_of(url: str) -> str:\n",
    "    if not url:\n",
    "        return \"\"\n",
    "    p = urlparse(normalize_url(url))\n",
    "    host = p.netloc\n",
    "    # drop common prefixes\n",
    "    host = re.sub(r\"^(www\\.)\", \"\", host)\n",
    "    return host\n",
    "\n",
    "def soft_domain_match(a: str, b: str) -> bool:\n",
    "    \"\"\"\n",
    "    A conservative match:\n",
    "    - exact domain match OR one is a subdomain of the other\n",
    "    \"\"\"\n",
    "    if not a or not b:\n",
    "        return False\n",
    "    a = a.lower().strip()\n",
    "    b = b.lower().strip()\n",
    "    return (a == b) or a.endswith(\".\" + b) or b.endswith(\".\" + a)\n",
    "\n",
    "def stable_hash(text: str) -> str:\n",
    "    return hashlib.sha256(text.encode(\"utf-8\")).hexdigest()[:12]\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Google CSE\n",
    "# ------------------------------------------------------------\n",
    "def google_cse_search(query: str, num: int = 5) -> list[dict]:\n",
    "    \"\"\"\n",
    "    Returns a list of items with keys:\n",
    "      - title, link, snippet, displayLink\n",
    "    \"\"\"\n",
    "    url = \"https://www.googleapis.com/customsearch/v1\"\n",
    "    params = {\n",
    "        \"key\": GOOGLE_API_KEY,\n",
    "        \"cx\": GOOGLE_CSE_CX,\n",
    "        \"q\": query,\n",
    "        \"num\": min(max(num, 1), 10),\n",
    "    }\n",
    "    r = requests.get(url, params=params, timeout=30)\n",
    "    r.raise_for_status()\n",
    "    data = r.json()\n",
    "    return data.get(\"items\", []) or []\n",
    "\n",
    "def cse_candidate_urls(startup_name: str, provided_website: str) -> dict:\n",
    "    \"\"\"\n",
    "    Run a few searches to collect candidate official URLs and reference pages.\n",
    "    \"\"\"\n",
    "    queries = []\n",
    "    if provided_website:\n",
    "        # confirm the provided domain is actually associated with the startup name\n",
    "        queries.append(f'\"{startup_name}\" {domain_of(provided_website)}')\n",
    "    queries.extend([\n",
    "        f'\"{startup_name}\" official website',\n",
    "        f'{startup_name} company',\n",
    "        f'{startup_name} Crunchbase',\n",
    "        f'{startup_name} LinkedIn',\n",
    "    ])\n",
    "\n",
    "    results = []\n",
    "    for q in queries:\n",
    "        try:\n",
    "            items = google_cse_search(q, num=5)\n",
    "            for it in items:\n",
    "                results.append({\n",
    "                    \"query\": q,\n",
    "                    \"title\": it.get(\"title\"),\n",
    "                    \"link\": it.get(\"link\"),\n",
    "                    \"snippet\": it.get(\"snippet\"),\n",
    "                    \"displayLink\": it.get(\"displayLink\"),\n",
    "                })\n",
    "            time.sleep(0.2)  # gentle pacing\n",
    "        except Exception as e:\n",
    "            results.append({\"query\": q, \"error\": str(e)})\n",
    "\n",
    "    # Extract and group links\n",
    "    links = [r[\"link\"] for r in results if isinstance(r, dict) and r.get(\"link\")]\n",
    "    # keep unique order\n",
    "    seen = set()\n",
    "    uniq_links = []\n",
    "    for l in links:\n",
    "        nl = normalize_url(l)\n",
    "        if nl and nl not in seen:\n",
    "            seen.add(nl)\n",
    "            uniq_links.append(nl)\n",
    "\n",
    "    return {\"queries\": queries, \"raw_results\": results, \"unique_links\": uniq_links}\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Candidate selection heuristics (deterministic, lightweight)\n",
    "# ------------------------------------------------------------\n",
    "KNOWN_REFERENCE_DOMAINS = {\n",
    "    \"crunchbase.com\": \"crunchbase\",\n",
    "    \"linkedin.com\": \"linkedin\",\n",
    "    \"pitchbook.com\": \"pitchbook\",\n",
    "    \"bloomberg.com\": \"bloomberg\",\n",
    "    \"prtimes.jp\": \"prtimes\",\n",
    "    \"note.com\": \"note\",\n",
    "    \"medium.com\": \"medium\",\n",
    "    \"x.com\": \"x\",\n",
    "    \"twitter.com\": \"x\",\n",
    "    \"facebook.com\": \"facebook\",\n",
    "    \"github.com\": \"github\",\n",
    "}\n",
    "\n",
    "def classify_link(url: str) -> str:\n",
    "    d = domain_of(url)\n",
    "    for dom, label in KNOWN_REFERENCE_DOMAINS.items():\n",
    "        if soft_domain_match(d, dom) or d.endswith(\".\" + dom):\n",
    "            return label\n",
    "    return \"other\"\n",
    "\n",
    "def score_official_site_candidate(url: str, startup_name: str, provided_website: str) -> float:\n",
    "    \"\"\"\n",
    "    Heuristic scoring for \"official website\" candidates.\n",
    "    We prefer:\n",
    "    - match with provided domain (if provided)\n",
    "    - non-reference domains (not Crunchbase/LinkedIn/etc.)\n",
    "    - shorter URLs (homepages)\n",
    "    \"\"\"\n",
    "    url_n = normalize_url(url)\n",
    "    d = domain_of(url_n)\n",
    "\n",
    "    score = 0.0\n",
    "    # Provided URL anchor\n",
    "    if provided_website:\n",
    "        if soft_domain_match(d, domain_of(provided_website)):\n",
    "            score += 5.0\n",
    "\n",
    "    # Penalize known reference aggregators for \"official website\" selection\n",
    "    ref = classify_link(url_n)\n",
    "    if ref != \"other\":\n",
    "        score -= 3.0\n",
    "\n",
    "    # Prefer homepage-ish paths\n",
    "    p = urlparse(url_n).path or \"/\"\n",
    "    if p in [\"/\", \"\"]:\n",
    "        score += 1.5\n",
    "    elif len(p) <= 12:\n",
    "        score += 0.5\n",
    "    else:\n",
    "        score -= 0.3\n",
    "\n",
    "    # Light keyword bias (domain contains a token from the name)\n",
    "    tokens = re.findall(r\"[a-z0-9]+\", startup_name.lower())\n",
    "    if tokens:\n",
    "        for t in tokens[:2]:\n",
    "            if t and t in d:\n",
    "                score += 0.8\n",
    "                break\n",
    "\n",
    "    return score\n",
    "\n",
    "\n",
    "def pick_top_official_site(links: list[str], startup_name: str, provided_website: str) -> dict:\n",
    "    scored = []\n",
    "    for u in links:\n",
    "        scored.append({\n",
    "            \"url\": u,\n",
    "            \"domain\": domain_of(u),\n",
    "            \"class\": classify_link(u),\n",
    "            \"score\": score_official_site_candidate(u, startup_name, provided_website),\n",
    "        })\n",
    "    scored.sort(key=lambda x: x[\"score\"], reverse=True)\n",
    "\n",
    "    top = scored[0] if scored else None\n",
    "    top_official = top[\"url\"] if top and top[\"score\"] >= 0.5 else (normalize_url(provided_website) if provided_website else None)\n",
    "\n",
    "    return {\n",
    "        \"top_official_site\": top_official,\n",
    "        \"ranked_candidates\": scored[:15],\n",
    "    }\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Optional: Use OpenAI to finalize entity resolution (conservative)\n",
    "# ------------------------------------------------------------\n",
    "def openai_finalize_entity(startup_name: str, provided_website: str, ranked_candidates: list[dict], raw_results: list[dict]) -> dict:\n",
    "    \"\"\"\n",
    "    Use OpenAI to:\n",
    "    - choose canonical official website (or keep provided)\n",
    "    - extract key reference URLs (Crunchbase / LinkedIn / X / etc.)\n",
    "    - output a structured entity record with confidence + rationale\n",
    "\n",
    "    This is intentionally conservative: it should prefer 'provided_website'\n",
    "    when it matches the candidates.\n",
    "    \"\"\"\n",
    "    # Keep prompt compact; include only top candidates and a small sample of results\n",
    "    top_candidates = ranked_candidates[:8]\n",
    "    sample_results = [r for r in raw_results if r.get(\"link\")][:10]\n",
    "\n",
    "    system = (\n",
    "        \"You are an entity-resolution assistant. \"\n",
    "        \"Given a startup name, an optionally provided website, and Google CSE results, \"\n",
    "        \"select the most likely canonical official website and key reference pages. \"\n",
    "        \"Be conservative: if the provided website domain appears consistent, keep it. \"\n",
    "        \"Return JSON only, no extra text.\"\n",
    "    )\n",
    "\n",
    "    user = {\n",
    "        \"startup_name\": startup_name,\n",
    "        \"provided_website\": provided_website or None,\n",
    "        \"top_candidates\": top_candidates,\n",
    "        \"sample_results\": sample_results,\n",
    "        \"required_output_schema\": {\n",
    "            \"canonical_name\": \"string\",\n",
    "            \"official_website\": \"string|null\",\n",
    "            \"official_website_confidence\": \"0..1\",\n",
    "            \"official_website_rationale\": \"string\",\n",
    "            \"reference_urls\": {\n",
    "                \"crunchbase\": \"string|null\",\n",
    "                \"linkedin\": \"string|null\",\n",
    "                \"x\": \"string|null\",\n",
    "                \"github\": \"string|null\",\n",
    "                \"other\": \"array of {label, url}\"\n",
    "            },\n",
    "            \"flags\": {\n",
    "                \"possible_name_collision\": \"boolean\",\n",
    "                \"needs_manual_review\": \"boolean\"\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "\n",
    "    # Compatible with older openai.ChatCompletion style\n",
    "    resp = openai.ChatCompletion.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system},\n",
    "            {\"role\": \"user\", \"content\": json.dumps(user, ensure_ascii=False)},\n",
    "        ],\n",
    "        temperature=0.0,\n",
    "    )\n",
    "    content = resp[\"choices\"][0][\"message\"][\"content\"]\n",
    "    return json.loads(content)\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Run: collect candidates and resolve\n",
    "# ------------------------------------------------------------\n",
    "provided_website_n = normalize_url(provided_website) if provided_website else \"\"\n",
    "cse_pack = cse_candidate_urls(startup_name=startup_name, provided_website=provided_website_n)\n",
    "\n",
    "picked = pick_top_official_site(\n",
    "    links=cse_pack[\"unique_links\"],\n",
    "    startup_name=startup_name,\n",
    "    provided_website=provided_website_n\n",
    ")\n",
    "\n",
    "# Try OpenAI finalization; if it fails, fall back to heuristic result\n",
    "try:\n",
    "    entity = openai_finalize_entity(\n",
    "        startup_name=startup_name,\n",
    "        provided_website=provided_website_n,\n",
    "        ranked_candidates=picked[\"ranked_candidates\"],\n",
    "        raw_results=cse_pack[\"raw_results\"],\n",
    "    )\n",
    "    entity[\"resolution_method\"] = \"openai+heuristics\"\n",
    "except Exception as e:\n",
    "    entity = {\n",
    "        \"canonical_name\": startup_name,\n",
    "        \"official_website\": picked[\"top_official_site\"],\n",
    "        \"official_website_confidence\": 0.55 if picked[\"top_official_site\"] else 0.2,\n",
    "        \"official_website_rationale\": f\"Fallback heuristic selection (OpenAI step failed): {str(e)}\",\n",
    "        \"reference_urls\": {\n",
    "            \"crunchbase\": None,\n",
    "            \"linkedin\": None,\n",
    "            \"x\": None,\n",
    "            \"github\": None,\n",
    "            \"other\": []\n",
    "        },\n",
    "        \"flags\": {\n",
    "            \"possible_name_collision\": True,\n",
    "            \"needs_manual_review\": True\n",
    "        },\n",
    "        \"resolution_method\": \"heuristics_only\",\n",
    "    }\n",
    "\n",
    "# Attach evidence (lightweight)\n",
    "entity[\"evidence\"] = {\n",
    "    \"provided_website\": provided_website_n or None,\n",
    "    \"cse_queries\": cse_pack[\"queries\"],\n",
    "    \"top_ranked_candidates\": picked[\"ranked_candidates\"][:10],\n",
    "    \"result_sample_hash\": stable_hash(json.dumps(cse_pack[\"raw_results\"][:20], ensure_ascii=False)),\n",
    "}\n",
    "\n",
    "# Persist\n",
    "out_path = ART_DIR / f\"entity_{run_id}.json\"\n",
    "out_path.write_text(json.dumps(entity, ensure_ascii=False, indent=2), encoding=\"utf-8\")\n",
    "\n",
    "print(\"✅ Startup Identification complete\")\n",
    "print(f\"- Startup: {startup_name}\")\n",
    "print(f\"- Official website: {entity.get('official_website')}\")\n",
    "print(f\"- Method: {entity.get('resolution_method')}\")\n",
    "print(f\"- Saved: {out_path.as_posix()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "90435ff7-12f8-483b-acf4-8cf021570c8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Startup: Sakana AI\n",
      "Official: https://sakana.ai/series-a\n",
      "Entity file: entity_20260107_225628.json\n",
      "Official targets (robots-allowed, capped): 30\n",
      "CSE targets (capped): 25\n",
      "✅ Raw pages saved: artifacts/meeting_deep_dive/raw_pages_20260107_225628.jsonl\n",
      "Usable pages: 22 | Shortlist to LLM: 14\n",
      "✅ Company basics (expanded) complete\n",
      "- Raw pages: artifacts/meeting_deep_dive/raw_pages_20260107_225628.jsonl\n",
      "- Extraction JSON: artifacts/meeting_deep_dive/company_extraction_20260107_225628.json\n",
      "- Company DF: artifacts/meeting_deep_dive/company_basics_20260107_225628.csv\n",
      "- Claims DF: artifacts/meeting_deep_dive/company_claims_20260107_225628.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>run_id</th>\n",
       "      <th>company_name</th>\n",
       "      <th>official_website</th>\n",
       "      <th>one_liner</th>\n",
       "      <th>business_description</th>\n",
       "      <th>products_services</th>\n",
       "      <th>customer_segments</th>\n",
       "      <th>customers_named</th>\n",
       "      <th>use_cases</th>\n",
       "      <th>industries_hiring_for</th>\n",
       "      <th>target_market</th>\n",
       "      <th>geography_focus</th>\n",
       "      <th>competitors</th>\n",
       "      <th>funding_stage_hint</th>\n",
       "      <th>investors_mentioned</th>\n",
       "      <th>funding_amounts_mentioned</th>\n",
       "      <th>funding_dates_mentioned</th>\n",
       "      <th>confidence_overall</th>\n",
       "      <th>confidence_rationale</th>\n",
       "      <th>entity_source</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20260107_225628</td>\n",
       "      <td>Sakana AI</td>\n",
       "      <td>https://sakana.ai/series-a</td>\n",
       "      <td>A new AI R&amp;D company based in Tokyo, Japan, fo...</td>\n",
       "      <td>Sakana AI aims to develop transformative AI th...</td>\n",
       "      <td>[AI Scientist, ShinkaEvolve, Evolutionary Mode...</td>\n",
       "      <td>[Enterprises, Finance, Manufacturing, Government]</td>\n",
       "      <td>[Mitsubishi UFJ Financial Group, Daiwa Securit...</td>\n",
       "      <td>[Automated scientific discovery, AI model opti...</td>\n",
       "      <td>[AI Research, Software Engineering, Cybersecur...</td>\n",
       "      <td>Japan</td>\n",
       "      <td>Tokyo, Japan</td>\n",
       "      <td>[OpenAI, Google, Anthropic]</td>\n",
       "      <td>Series B</td>\n",
       "      <td>[New Enterprise Associates, Khosla Ventures, L...</td>\n",
       "      <td>[$30M, $200M, $135M]</td>\n",
       "      <td>[January 16, 2024, September 04, 2024, Novembe...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>The information is sourced from official compa...</td>\n",
       "      <td>entity_20260107_225628.json</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            run_id company_name            official_website  \\\n",
       "0  20260107_225628    Sakana AI  https://sakana.ai/series-a   \n",
       "\n",
       "                                           one_liner  \\\n",
       "0  A new AI R&D company based in Tokyo, Japan, fo...   \n",
       "\n",
       "                                business_description  \\\n",
       "0  Sakana AI aims to develop transformative AI th...   \n",
       "\n",
       "                                   products_services  \\\n",
       "0  [AI Scientist, ShinkaEvolve, Evolutionary Mode...   \n",
       "\n",
       "                                   customer_segments  \\\n",
       "0  [Enterprises, Finance, Manufacturing, Government]   \n",
       "\n",
       "                                     customers_named  \\\n",
       "0  [Mitsubishi UFJ Financial Group, Daiwa Securit...   \n",
       "\n",
       "                                           use_cases  \\\n",
       "0  [Automated scientific discovery, AI model opti...   \n",
       "\n",
       "                               industries_hiring_for target_market  \\\n",
       "0  [AI Research, Software Engineering, Cybersecur...         Japan   \n",
       "\n",
       "  geography_focus                  competitors funding_stage_hint  \\\n",
       "0    Tokyo, Japan  [OpenAI, Google, Anthropic]           Series B   \n",
       "\n",
       "                                 investors_mentioned  \\\n",
       "0  [New Enterprise Associates, Khosla Ventures, L...   \n",
       "\n",
       "  funding_amounts_mentioned  \\\n",
       "0      [$30M, $200M, $135M]   \n",
       "\n",
       "                             funding_dates_mentioned  confidence_overall  \\\n",
       "0  [January 16, 2024, September 04, 2024, Novembe...                 1.0   \n",
       "\n",
       "                                confidence_rationale  \\\n",
       "0  The information is sourced from official compa...   \n",
       "\n",
       "                 entity_source  \n",
       "0  entity_20260107_225628.json  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>run_id</th>\n",
       "      <th>company_name</th>\n",
       "      <th>category</th>\n",
       "      <th>claim_type</th>\n",
       "      <th>claim</th>\n",
       "      <th>evidence_url</th>\n",
       "      <th>source_type</th>\n",
       "      <th>confidence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20260107_225628</td>\n",
       "      <td>Sakana AI</td>\n",
       "      <td>funding</td>\n",
       "      <td>fact</td>\n",
       "      <td>Sakana AI raised approximately $200M in its Se...</td>\n",
       "      <td>https://sakana.ai/series-a/</td>\n",
       "      <td>official</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20260107_225628</td>\n",
       "      <td>Sakana AI</td>\n",
       "      <td>funding</td>\n",
       "      <td>fact</td>\n",
       "      <td>Sakana AI raised $135M in its Series B funding...</td>\n",
       "      <td>https://siliconangle.com/2025/11/17/sakana-ai-...</td>\n",
       "      <td>cse</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20260107_225628</td>\n",
       "      <td>Sakana AI</td>\n",
       "      <td>product</td>\n",
       "      <td>fact</td>\n",
       "      <td>Sakana AI developed the AI Scientist, a system...</td>\n",
       "      <td>https://sakana.ai/ai-scientist/</td>\n",
       "      <td>cse</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>20260107_225628</td>\n",
       "      <td>Sakana AI</td>\n",
       "      <td>product</td>\n",
       "      <td>fact</td>\n",
       "      <td>Sakana AI's ShinkaEvolve framework optimizes c...</td>\n",
       "      <td>https://sakana.ai/icfp-2025/</td>\n",
       "      <td>cse</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20260107_225628</td>\n",
       "      <td>Sakana AI</td>\n",
       "      <td>market</td>\n",
       "      <td>fact</td>\n",
       "      <td>Sakana AI focuses on developing AI models tail...</td>\n",
       "      <td>https://siliconangle.com/2025/11/17/sakana-ai-...</td>\n",
       "      <td>cse</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            run_id company_name category claim_type  \\\n",
       "0  20260107_225628    Sakana AI  funding       fact   \n",
       "1  20260107_225628    Sakana AI  funding       fact   \n",
       "2  20260107_225628    Sakana AI  product       fact   \n",
       "3  20260107_225628    Sakana AI  product       fact   \n",
       "4  20260107_225628    Sakana AI   market       fact   \n",
       "\n",
       "                                               claim  \\\n",
       "0  Sakana AI raised approximately $200M in its Se...   \n",
       "1  Sakana AI raised $135M in its Series B funding...   \n",
       "2  Sakana AI developed the AI Scientist, a system...   \n",
       "3  Sakana AI's ShinkaEvolve framework optimizes c...   \n",
       "4  Sakana AI focuses on developing AI models tail...   \n",
       "\n",
       "                                        evidence_url source_type  confidence  \n",
       "0                        https://sakana.ai/series-a/    official         1.0  \n",
       "1  https://siliconangle.com/2025/11/17/sakana-ai-...         cse         1.0  \n",
       "2                    https://sakana.ai/ai-scientist/         cse         1.0  \n",
       "3                       https://sakana.ai/icfp-2025/         cse         1.0  \n",
       "4  https://siliconangle.com/2025/11/17/sakana-ai-...         cse         1.0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ============================================================\n",
    "# 3. Company Basics (Expanded version)\n",
    "# ============================================================\n",
    "# What this cell does (expanded, still lightweight & compliant):\n",
    "# 1) Official-site first: collect richer facts from key pages + internal links + sitemap hints (robots-respecting).\n",
    "# 2) CSE enrichment: collect more third-party references (robots-respecting per-domain).\n",
    "# 3) Store raw page text as JSONL for traceability/reproducibility.\n",
    "# 4) Use OpenAI (SDK v1+) to:\n",
    "#    - generate a 1-row \"company basics\" table\n",
    "#    - generate a \"claims\" long-form table with evidence_url + confidence\n",
    "#\n",
    "# Outputs (saved under artifacts/meeting_deep_dive/):\n",
    "# - raw_pages_<run_id>.jsonl\n",
    "# - company_basics_<run_id>.csv\n",
    "# - company_claims_<run_id>.csv\n",
    "# - company_extraction_<run_id>.json     (full model output)\n",
    "#\n",
    "# Requirements:\n",
    "# - pip install beautifulsoup4 lxml pandas python-dotenv requests openai\n",
    "\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "import time\n",
    "import hashlib\n",
    "import requests\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "from pathlib import Path\n",
    "from urllib.parse import urlparse, urljoin\n",
    "from datetime import datetime, timezone\n",
    "from urllib.robotparser import RobotFileParser\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Global configuration\n",
    "# ------------------------------------------------------------\n",
    "# Load env.txt explicitly (recommended for local + GitHub Actions parity)\n",
    "load_dotenv(\"env.txt\")\n",
    "\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "if OPENAI_API_KEY is None:\n",
    "    raise EnvironmentError(\"OPENAI_API_KEY is not set in the environment variables.\")\n",
    "\n",
    "GOOGLE_API_KEY = os.getenv(\"GOOGLE_API_KEY\")\n",
    "GOOGLE_CSE_CX = os.getenv(\"GOOGLE_CSE_CX\")\n",
    "if GOOGLE_API_KEY is None or GOOGLE_CSE_CX is None:\n",
    "    raise EnvironmentError(\"GOOGLE_API_KEY and GOOGLE_CSE_CX must be set in the environment variables.\")\n",
    "\n",
    "client = OpenAI(api_key=OPENAI_API_KEY)\n",
    "\n",
    "ART_DIR = Path(\"artifacts\") / \"meeting_deep_dive\"\n",
    "ART_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Load resolved entity (from cell #2)\n",
    "# ------------------------------------------------------------\n",
    "def load_latest_entity(art_dir: Path) -> tuple[dict, Path]:\n",
    "    files = sorted(art_dir.glob(\"entity_*.json\"), key=lambda p: p.stat().st_mtime, reverse=True)\n",
    "    if not files:\n",
    "        raise FileNotFoundError(\"No entity_*.json found. Please run cell #2 first.\")\n",
    "    p = files[0]\n",
    "    return json.loads(p.read_text(encoding=\"utf-8\")), p\n",
    "\n",
    "entity, entity_path = load_latest_entity(ART_DIR)\n",
    "\n",
    "startup_name = (entity.get(\"canonical_name\") or \"\").strip() or \"Unknown Startup\"\n",
    "official_website = (entity.get(\"official_website\") or \"\").strip()\n",
    "if not official_website:\n",
    "    raise ValueError(\"official_website is missing in entity. Please resolve identity in cell #2.\")\n",
    "\n",
    "# Try to align run_id with inputs if present; else derive from time\n",
    "try:\n",
    "    run_id = inputs[\"meta\"][\"run_id\"]  # from cell #1\n",
    "except Exception:\n",
    "    run_id = datetime.now(timezone.utc).strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Knobs (expanded but still safe)\n",
    "# ------------------------------------------------------------\n",
    "USER_AGENT = \"researchOSv2-bot/0.2 (+contact: internal-research)\"\n",
    "TIMEOUT = 25\n",
    "SLEEP_SEC = 0.25\n",
    "\n",
    "MAX_OFFICIAL_PAGES = 30         # expanded from ~12\n",
    "MAX_SITEMAP_URLS = 250          # cap sitemap candidates\n",
    "MAX_CSE_ARTICLES = 25           # expanded from ~12\n",
    "MAX_TEXT_CHARS_STORE = 60000    # expanded raw text cap\n",
    "MAX_TEXT_CHARS_PROMPT = 9000    # per-page cap in LLM prompt\n",
    "MAX_PAGES_TO_LLM = 18           # keep prompt bounded; prioritize official pages\n",
    "\n",
    "EXPLICIT_PATHS = [\n",
    "    \"/about\", \"/company\", \"/mission\", \"/vision\", \"/team\",\n",
    "    \"/product\", \"/products\", \"/service\", \"/services\", \"/solutions\",\n",
    "    \"/customers\", \"/case\", \"/cases\", \"/case-studies\", \"/usecase\", \"/use-cases\",\n",
    "    \"/pricing\",\n",
    "    \"/news\", \"/press\", \"/blog\", \"/media\",\n",
    "    \"/careers\", \"/jobs\", \"/recruit\",\n",
    "    \"/investor\", \"/investors\", \"/funding\", \"/ir\"\n",
    "]\n",
    "\n",
    "OFFICIAL_KEYWORDS = [\n",
    "    \"about\", \"company\", \"mission\", \"vision\", \"team\",\n",
    "    \"product\", \"products\", \"service\", \"services\", \"solutions\",\n",
    "    \"customer\", \"customers\", \"case\", \"cases\", \"case-study\", \"usecase\", \"use-case\",\n",
    "    \"pricing\", \"security\", \"privacy\", \"compliance\",\n",
    "    \"news\", \"press\", \"blog\", \"media\",\n",
    "    \"careers\", \"jobs\", \"recruit\",\n",
    "    \"investor\", \"investors\", \"funding\", \"ir\"\n",
    "]\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# URL / domain helpers\n",
    "# ------------------------------------------------------------\n",
    "def normalize_url(url: str) -> str:\n",
    "    url = (url or \"\").strip()\n",
    "    if not url:\n",
    "        return \"\"\n",
    "    if not re.match(r\"^https?://\", url, flags=re.I):\n",
    "        url = \"https://\" + url\n",
    "    p = urlparse(url)\n",
    "    # normalize host and remove fragment\n",
    "    p = p._replace(netloc=p.netloc.lower(), fragment=\"\")\n",
    "    return p.geturl()\n",
    "\n",
    "def get_domain(url: str) -> str:\n",
    "    u = normalize_url(url)\n",
    "    return re.sub(r\"^www\\.\", \"\", urlparse(u).netloc)\n",
    "\n",
    "def same_domain(a: str, b: str) -> bool:\n",
    "    da, db = get_domain(a), get_domain(b)\n",
    "    return da == db or da.endswith(\".\" + db) or db.endswith(\".\" + da)\n",
    "\n",
    "def is_asset_url(url: str) -> bool:\n",
    "    path = (urlparse(url).path or \"\").lower()\n",
    "    return bool(re.search(r\"\\.(pdf|jpg|jpeg|png|gif|svg|zip|mp4|mov)$\", path))\n",
    "\n",
    "def stable_hash(text: str) -> str:\n",
    "    return hashlib.sha256(text.encode(\"utf-8\")).hexdigest()[:12]\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Robots helpers (conservative by design)\n",
    "# ------------------------------------------------------------\n",
    "def build_robot_parser(base_url: str) -> RobotFileParser | None:\n",
    "    base = normalize_url(base_url)\n",
    "    rp = RobotFileParser()\n",
    "    robots_url = urljoin(base, \"/robots.txt\")\n",
    "    rp.set_url(robots_url)\n",
    "    try:\n",
    "        rp.read()\n",
    "        return rp\n",
    "    except Exception:\n",
    "        # If robots can't be fetched, be conservative later\n",
    "        return None\n",
    "\n",
    "def robots_allows(rp: RobotFileParser | None, url: str, user_agent: str = USER_AGENT) -> bool:\n",
    "    if rp is None:\n",
    "        # Conservative fallback: allow only homepage & obvious public info paths\n",
    "        p = (urlparse(url).path or \"/\").lower()\n",
    "        return p in [\"/\", \"\"] or any(k in p for k in [\n",
    "            \"about\", \"company\", \"product\", \"service\", \"solutions\",\n",
    "            \"customers\", \"case\", \"pricing\", \"news\", \"press\", \"blog\",\n",
    "            \"careers\", \"jobs\", \"recruit\", \"investor\", \"funding\", \"ir\"\n",
    "        ])\n",
    "    try:\n",
    "        return rp.can_fetch(user_agent, url)\n",
    "    except Exception:\n",
    "        return False\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Fetch + extraction helpers\n",
    "# ------------------------------------------------------------\n",
    "def fetch_html(url: str) -> tuple[int, str]:\n",
    "    headers = {\"User-Agent\": USER_AGENT}\n",
    "    r = requests.get(url, headers=headers, timeout=TIMEOUT, allow_redirects=True)\n",
    "    return r.status_code, (r.text if r.ok else \"\")\n",
    "\n",
    "def html_to_text(html: str) -> str:\n",
    "    \"\"\"\n",
    "    Higher-density extraction:\n",
    "    - Remove scripts/styles\n",
    "    - Prefer <main> or <article> when present to reduce nav/footer noise\n",
    "    \"\"\"\n",
    "    soup = BeautifulSoup(html, \"lxml\")\n",
    "    for tag in soup([\"script\", \"style\", \"noscript\", \"svg\"]):\n",
    "        tag.decompose()\n",
    "\n",
    "    node = soup.find(\"main\") or soup.find(\"article\") or (soup.body if soup.body else soup)\n",
    "    text = node.get_text(\"\\n\", strip=True)\n",
    "    text = re.sub(r\"\\n{3,}\", \"\\n\\n\", text)\n",
    "    return text.strip()\n",
    "\n",
    "def extract_links(base_url: str, html: str) -> list[str]:\n",
    "    soup = BeautifulSoup(html, \"lxml\")\n",
    "    links = []\n",
    "    for a in soup.find_all(\"a\", href=True):\n",
    "        href = a[\"href\"].strip()\n",
    "        if href.startswith(\"#\") or href.lower().startswith(\"mailto:\") or href.lower().startswith(\"tel:\"):\n",
    "            continue\n",
    "        abs_url = normalize_url(urljoin(base_url, href))\n",
    "        if abs_url.startswith((\"http://\", \"https://\")):\n",
    "            links.append(abs_url)\n",
    "    # de-dup preserve order\n",
    "    seen, out = set(), []\n",
    "    for l in links:\n",
    "        if l not in seen:\n",
    "            seen.add(l)\n",
    "            out.append(l)\n",
    "    return out\n",
    "\n",
    "def score_official_link(url: str) -> float:\n",
    "    \"\"\"\n",
    "    Prefer likely \"company basics\" pages; de-prioritize assets/long tracking URLs.\n",
    "    \"\"\"\n",
    "    u = normalize_url(url)\n",
    "    p = (urlparse(u).path or \"/\").lower()\n",
    "\n",
    "    score = 0.0\n",
    "    if p in [\"/\", \"\"]:\n",
    "        score += 2.0\n",
    "\n",
    "    for k in OFFICIAL_KEYWORDS:\n",
    "        if k in p:\n",
    "            score += 1.0\n",
    "\n",
    "    if is_asset_url(u):\n",
    "        score -= 4.0\n",
    "\n",
    "    # penalize extremely deep paths\n",
    "    if p.count(\"/\") >= 4:\n",
    "        score -= 0.5\n",
    "\n",
    "    # mild penalty for long query strings\n",
    "    if len(urlparse(u).query) > 30:\n",
    "        score -= 0.4\n",
    "\n",
    "    return score\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Sitemap discovery (lightweight)\n",
    "# ------------------------------------------------------------\n",
    "def discover_from_sitemap(site_url: str, rp: RobotFileParser | None, limit: int = MAX_SITEMAP_URLS) -> list[str]:\n",
    "    import xml.etree.ElementTree as ET\n",
    "\n",
    "    root = normalize_url(site_url)\n",
    "    sm_url = urljoin(root, \"/sitemap.xml\")\n",
    "    if not robots_allows(rp, sm_url):\n",
    "        return []\n",
    "\n",
    "    try:\n",
    "        r = requests.get(sm_url, headers={\"User-Agent\": USER_AGENT}, timeout=TIMEOUT)\n",
    "        if not r.ok:\n",
    "            return []\n",
    "        tree = ET.fromstring(r.text)\n",
    "    except Exception:\n",
    "        return []\n",
    "\n",
    "    ns = {\"sm\": \"http://www.sitemaps.org/schemas/sitemap/0.9\"}\n",
    "    urls = []\n",
    "\n",
    "    # sitemap index?\n",
    "    sitemap_locs = [loc.text for loc in tree.findall(\".//sm:sitemap/sm:loc\", ns) if loc.text]\n",
    "    if sitemap_locs:\n",
    "        for loc in sitemap_locs[:5]:\n",
    "            try:\n",
    "                if not robots_allows(rp, loc):\n",
    "                    continue\n",
    "                r2 = requests.get(loc, headers={\"User-Agent\": USER_AGENT}, timeout=TIMEOUT)\n",
    "                if not r2.ok:\n",
    "                    continue\n",
    "                t2 = ET.fromstring(r2.text)\n",
    "                locs2 = [x.text for x in t2.findall(\".//sm:url/sm:loc\", ns) if x.text]\n",
    "                urls.extend(locs2)\n",
    "                if len(urls) >= limit:\n",
    "                    break\n",
    "            except Exception:\n",
    "                continue\n",
    "    else:\n",
    "        locs = [loc.text for loc in tree.findall(\".//sm:url/sm:loc\", ns) if loc.text]\n",
    "        urls.extend(locs)\n",
    "\n",
    "    base_dom = get_domain(root)\n",
    "    out, seen = [], set()\n",
    "    for u in urls:\n",
    "        nu = normalize_url(u)\n",
    "        if not nu:\n",
    "            continue\n",
    "        if not same_domain(nu, root):\n",
    "            continue\n",
    "        if nu not in seen:\n",
    "            seen.add(nu)\n",
    "            out.append(nu)\n",
    "        if len(out) >= limit:\n",
    "            break\n",
    "    return out\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Google CSE enrichment\n",
    "# ------------------------------------------------------------\n",
    "def google_cse_search(query: str, num: int = 5) -> list[dict]:\n",
    "    url = \"https://www.googleapis.com/customsearch/v1\"\n",
    "    params = {\"key\": GOOGLE_API_KEY, \"cx\": GOOGLE_CSE_CX, \"q\": query, \"num\": min(max(num, 1), 10)}\n",
    "    r = requests.get(url, params=params, timeout=30)\n",
    "    r.raise_for_status()\n",
    "    data = r.json()\n",
    "    items = data.get(\"items\", []) or []\n",
    "    out = []\n",
    "    for it in items:\n",
    "        out.append({\n",
    "            \"title\": it.get(\"title\"),\n",
    "            \"link\": normalize_url(it.get(\"link\") or \"\"),\n",
    "            \"snippet\": it.get(\"snippet\"),\n",
    "            \"displayLink\": it.get(\"displayLink\"),\n",
    "        })\n",
    "    return out\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Build official targets (homepage + explicit paths + internal links + sitemap)\n",
    "# ------------------------------------------------------------\n",
    "official_home = normalize_url(official_website)\n",
    "official_domain = get_domain(official_home)\n",
    "\n",
    "rp_official = build_robot_parser(official_home)\n",
    "print(f\"Startup: {startup_name}\")\n",
    "print(f\"Official: {official_home}\")\n",
    "print(f\"Entity file: {entity_path.name}\")\n",
    "\n",
    "# Fetch homepage\n",
    "home_status, home_html = fetch_html(official_home)\n",
    "time.sleep(SLEEP_SEC)\n",
    "if home_status >= 400 or not home_html:\n",
    "    raise RuntimeError(f\"Failed to fetch official homepage: HTTP {home_status}\")\n",
    "\n",
    "home_links = extract_links(official_home, home_html)\n",
    "internal_links = [l for l in home_links if same_domain(l, official_home) and not is_asset_url(l)]\n",
    "\n",
    "explicit_candidates = []\n",
    "for p in EXPLICIT_PATHS:\n",
    "    u = normalize_url(urljoin(official_home, p))\n",
    "    if not is_asset_url(u) and robots_allows(rp_official, u):\n",
    "        explicit_candidates.append(u)\n",
    "\n",
    "sitemap_links = discover_from_sitemap(official_home, rp_official, limit=MAX_SITEMAP_URLS)\n",
    "\n",
    "# Merge candidates\n",
    "official_candidates = list(dict.fromkeys([official_home] + explicit_candidates + internal_links + sitemap_links))\n",
    "\n",
    "# Score + pick\n",
    "scored = [{\"url\": u, \"score\": score_official_link(u)} for u in official_candidates]\n",
    "scored.sort(key=lambda x: x[\"score\"], reverse=True)\n",
    "\n",
    "official_targets = []\n",
    "for it in scored:\n",
    "    if len(official_targets) >= MAX_OFFICIAL_PAGES:\n",
    "        break\n",
    "    u = it[\"url\"]\n",
    "    if it[\"score\"] < 0.2:\n",
    "        continue\n",
    "    if robots_allows(rp_official, u):\n",
    "        official_targets.append(u)\n",
    "\n",
    "official_targets = list(dict.fromkeys(official_targets))\n",
    "print(f\"Official targets (robots-allowed, capped): {len(official_targets)}\")\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Build CSE targets (expanded query set, bilingual-friendly)\n",
    "# ------------------------------------------------------------\n",
    "queries = [\n",
    "    f'\"{startup_name}\" {official_domain}',\n",
    "    f'\"{startup_name}\" funding OR raised OR Series OR seed OR 資金調達',\n",
    "    f'\"{startup_name}\" customer OR case study OR 導入事例 OR 事例',\n",
    "    f'\"{startup_name}\" partnership OR 提携',\n",
    "    f'\"{startup_name}\" pricing OR 料金',\n",
    "    f'\"{startup_name}\" competitor OR alternative OR 競合',\n",
    "    f'\"{startup_name}\" market OR TAM OR 市場',\n",
    "    f'\"{startup_name}\" ARR OR revenue OR 売上',\n",
    "    f'\"{startup_name}\" security OR SOC2 OR ISO27001 OR セキュリティ',\n",
    "    f'\"{startup_name}\" hiring OR careers OR 採用 OR 募集',\n",
    "]\n",
    "\n",
    "cse_items = []\n",
    "for q in queries:\n",
    "    try:\n",
    "        cse_items.extend(google_cse_search(q, num=5))\n",
    "        time.sleep(0.2)\n",
    "    except Exception as e:\n",
    "        cse_items.append({\"title\": None, \"link\": None, \"snippet\": f\"CSE error: {e}\", \"displayLink\": None})\n",
    "\n",
    "# Dedup + cap\n",
    "seen = set()\n",
    "cse_links = []\n",
    "for it in cse_items:\n",
    "    link = it.get(\"link\")\n",
    "    if link and link not in seen and link.startswith((\"http://\", \"https://\")):\n",
    "        seen.add(link)\n",
    "        cse_links.append(link)\n",
    "cse_links = cse_links[:MAX_CSE_ARTICLES]\n",
    "print(f\"CSE targets (capped): {len(cse_links)}\")\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Robots cache for third-party domains\n",
    "# ------------------------------------------------------------\n",
    "_robot_cache: dict[str, RobotFileParser | None] = {}\n",
    "\n",
    "def get_rp_for_url(url: str) -> RobotFileParser | None:\n",
    "    dom = get_domain(url)\n",
    "    if dom in _robot_cache:\n",
    "        return _robot_cache[dom]\n",
    "    rp = build_robot_parser(url)\n",
    "    _robot_cache[dom] = rp\n",
    "    return rp\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Fetch + store raw pages (JSONL)\n",
    "# ------------------------------------------------------------\n",
    "RAW_PATH = ART_DIR / f\"raw_pages_{run_id}.jsonl\"\n",
    "\n",
    "def append_jsonl(path: Path, obj: dict):\n",
    "    with path.open(\"a\", encoding=\"utf-8\") as f:\n",
    "        f.write(json.dumps(obj, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "RAW_PATH.write_text(\"\", encoding=\"utf-8\")\n",
    "\n",
    "def collect_pages(urls: list[str], source_type: str):\n",
    "    for u in urls:\n",
    "        u = normalize_url(u)\n",
    "        if not u or is_asset_url(u):\n",
    "            continue\n",
    "\n",
    "        rp = rp_official if source_type == \"official\" else get_rp_for_url(u)\n",
    "        allowed = robots_allows(rp, u)\n",
    "\n",
    "        if not allowed:\n",
    "            append_jsonl(RAW_PATH, {\n",
    "                \"url\": u,\n",
    "                \"source_type\": source_type,\n",
    "                \"domain\": get_domain(u),\n",
    "                \"fetched_at_utc\": datetime.now(timezone.utc).isoformat(),\n",
    "                \"http_status\": None,\n",
    "                \"robots_allowed\": False,\n",
    "                \"text\": \"\",\n",
    "                \"notes\": \"Skipped due to robots.txt (or conservative fallback).\",\n",
    "            })\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            code, html = fetch_html(u)\n",
    "            time.sleep(SLEEP_SEC)\n",
    "            text = html_to_text(html) if html else \"\"\n",
    "            text = text[:MAX_TEXT_CHARS_STORE]\n",
    "\n",
    "            append_jsonl(RAW_PATH, {\n",
    "                \"url\": u,\n",
    "                \"source_type\": source_type,\n",
    "                \"domain\": get_domain(u),\n",
    "                \"fetched_at_utc\": datetime.now(timezone.utc).isoformat(),\n",
    "                \"http_status\": code,\n",
    "                \"robots_allowed\": True,\n",
    "                \"text\": text,\n",
    "                \"text_char_len\": len(text),\n",
    "            })\n",
    "        except Exception as e:\n",
    "            append_jsonl(RAW_PATH, {\n",
    "                \"url\": u,\n",
    "                \"source_type\": source_type,\n",
    "                \"domain\": get_domain(u),\n",
    "                \"fetched_at_utc\": datetime.now(timezone.utc).isoformat(),\n",
    "                \"http_status\": None,\n",
    "                \"robots_allowed\": True,\n",
    "                \"text\": \"\",\n",
    "                \"notes\": f\"Fetch error: {str(e)}\",\n",
    "            })\n",
    "\n",
    "collect_pages(official_targets, \"official\")\n",
    "collect_pages(cse_links, \"cse\")\n",
    "\n",
    "print(f\"✅ Raw pages saved: {RAW_PATH.as_posix()}\")\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Load raw pages + prepare shortlist for LLM (prioritize official)\n",
    "# ------------------------------------------------------------\n",
    "def load_raw_pages(path: Path) -> list[dict]:\n",
    "    rows = []\n",
    "    for line in path.read_text(encoding=\"utf-8\").splitlines():\n",
    "        if line.strip():\n",
    "            rows.append(json.loads(line))\n",
    "    return rows\n",
    "\n",
    "raw_pages = load_raw_pages(RAW_PATH)\n",
    "\n",
    "usable_pages = [\n",
    "    r for r in raw_pages\n",
    "    if r.get(\"robots_allowed\")\n",
    "    and isinstance(r.get(\"http_status\"), int) and r[\"http_status\"] < 400\n",
    "    and (r.get(\"text\") or \"\").strip()\n",
    "]\n",
    "\n",
    "official_usable = [r for r in usable_pages if r.get(\"source_type\") == \"official\"]\n",
    "cse_usable = [r for r in usable_pages if r.get(\"source_type\") == \"cse\"]\n",
    "\n",
    "# Shortlist: official first, then CSE\n",
    "shortlist = (official_usable[:12] + cse_usable[:10])[:MAX_PAGES_TO_LLM]\n",
    "print(f\"Usable pages: {len(usable_pages)} | Shortlist to LLM: {len(shortlist)}\")\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# OpenAI extraction (SDK v1+): summary row + evidence-linked claims\n",
    "# ------------------------------------------------------------\n",
    "def openai_extract_company_basics_and_claims(\n",
    "    startup_name: str,\n",
    "    official_website: str,\n",
    "    pages: list[dict]\n",
    ") -> dict:\n",
    "    \"\"\"\n",
    "    Return JSON only. Includes:\n",
    "    - company_summary (single object)\n",
    "    - claims (list; each claim has evidence_url and confidence)\n",
    "    \"\"\"\n",
    "    system = (\n",
    "        \"You are a research analyst assistant. \"\n",
    "        \"Given raw text from multiple webpages about a startup, extract factual company basics. \"\n",
    "        \"Prefer official sources when conflicts exist. \"\n",
    "        \"Separate facts from hypotheses. \"\n",
    "        \"For claims, ALWAYS attach an evidence_url from the provided pages; \"\n",
    "        \"if you cannot support it, omit it. Return JSON only.\"\n",
    "    )\n",
    "\n",
    "    schema = {\n",
    "        \"company_summary\": {\n",
    "            \"name\": \"string\",\n",
    "            \"official_website\": \"string|null\",\n",
    "            \"one_liner\": \"string|null\",\n",
    "            \"business_description\": \"string|null\",\n",
    "            \"products_services\": \"array of strings\",\n",
    "            \"customer_segments\": \"array of strings\",\n",
    "            \"customers_named\": \"array of strings\",\n",
    "            \"use_cases\": \"array of strings\",\n",
    "            \"industries_hiring_for\": \"array of strings\",\n",
    "            \"target_market\": \"string|null\",\n",
    "            \"geography_focus\": \"string|null\",\n",
    "            \"competitors\": \"array of strings\",\n",
    "            \"funding_signals\": {\n",
    "                \"funding_stage_hint\": \"string|null\",\n",
    "                \"investors_mentioned\": \"array of strings\",\n",
    "                \"funding_amounts_mentioned\": \"array of strings\",\n",
    "                \"funding_dates_mentioned\": \"array of strings\"\n",
    "            }\n",
    "        },\n",
    "        \"claims\": [\n",
    "            {\n",
    "                \"category\": \"product|customer|competitor|market|funding|hiring|other\",\n",
    "                \"claim_type\": \"fact|hypothesis\",\n",
    "                \"claim\": \"string\",\n",
    "                \"evidence_url\": \"string\",\n",
    "                \"source_type\": \"official|cse\",\n",
    "                \"confidence\": \"0..1\"\n",
    "            }\n",
    "        ],\n",
    "        \"notes\": {\n",
    "            \"conflicts_or_ambiguities\": \"array of strings\",\n",
    "            \"data_gaps\": \"array of strings\"\n",
    "        },\n",
    "        \"confidence\": {\n",
    "            \"overall\": \"0..1\",\n",
    "            \"rationale\": \"string\"\n",
    "        }\n",
    "    }\n",
    "\n",
    "    bundle = {\n",
    "        \"startup_name\": startup_name,\n",
    "        \"official_website\": official_website,\n",
    "        \"pages\": [\n",
    "            {\n",
    "                \"url\": p[\"url\"],\n",
    "                \"source_type\": p[\"source_type\"],\n",
    "                \"domain\": p.get(\"domain\"),\n",
    "                \"text\": (p.get(\"text\") or \"\")[:MAX_TEXT_CHARS_PROMPT],\n",
    "            } for p in pages\n",
    "        ],\n",
    "        \"schema\": schema\n",
    "    }\n",
    "\n",
    "    resp = client.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system},\n",
    "            {\"role\": \"user\", \"content\": json.dumps(bundle, ensure_ascii=False)},\n",
    "        ],\n",
    "        temperature=0.0,\n",
    "    )\n",
    "\n",
    "    content = resp.choices[0].message.content\n",
    "    return json.loads(content)\n",
    "\n",
    "extracted = openai_extract_company_basics_and_claims(startup_name, official_home, shortlist)\n",
    "\n",
    "# Save full extraction JSON\n",
    "EXTRACT_PATH = ART_DIR / f\"company_extraction_{run_id}.json\"\n",
    "EXTRACT_PATH.write_text(json.dumps(extracted, ensure_ascii=False, indent=2), encoding=\"utf-8\")\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Build DataFrames\n",
    "# ------------------------------------------------------------\n",
    "summary = extracted.get(\"company_summary\", {}) or {}\n",
    "funding = summary.get(\"funding_signals\", {}) if isinstance(summary.get(\"funding_signals\"), dict) else {}\n",
    "\n",
    "df_company = pd.DataFrame([{\n",
    "    \"run_id\": run_id,\n",
    "    \"company_name\": summary.get(\"name\") or startup_name,\n",
    "    \"official_website\": summary.get(\"official_website\") or official_home,\n",
    "    \"one_liner\": summary.get(\"one_liner\"),\n",
    "    \"business_description\": summary.get(\"business_description\"),\n",
    "    \"products_services\": summary.get(\"products_services\", []),\n",
    "    \"customer_segments\": summary.get(\"customer_segments\", []),\n",
    "    \"customers_named\": summary.get(\"customers_named\", []),\n",
    "    \"use_cases\": summary.get(\"use_cases\", []),\n",
    "    \"industries_hiring_for\": summary.get(\"industries_hiring_for\", []),\n",
    "    \"target_market\": summary.get(\"target_market\"),\n",
    "    \"geography_focus\": summary.get(\"geography_focus\"),\n",
    "    \"competitors\": summary.get(\"competitors\", []),\n",
    "    \"funding_stage_hint\": funding.get(\"funding_stage_hint\"),\n",
    "    \"investors_mentioned\": funding.get(\"investors_mentioned\", []),\n",
    "    \"funding_amounts_mentioned\": funding.get(\"funding_amounts_mentioned\", []),\n",
    "    \"funding_dates_mentioned\": funding.get(\"funding_dates_mentioned\", []),\n",
    "    \"confidence_overall\": (extracted.get(\"confidence\", {}) or {}).get(\"overall\"),\n",
    "    \"confidence_rationale\": (extracted.get(\"confidence\", {}) or {}).get(\"rationale\"),\n",
    "    \"entity_source\": entity_path.name,\n",
    "}])\n",
    "\n",
    "claims = extracted.get(\"claims\", []) or []\n",
    "df_claims = pd.DataFrame(claims)\n",
    "if not df_claims.empty:\n",
    "    df_claims.insert(0, \"run_id\", run_id)\n",
    "    df_claims.insert(1, \"company_name\", df_company.loc[0, \"company_name\"])\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Save artifacts\n",
    "# ------------------------------------------------------------\n",
    "COMPANY_CSV = ART_DIR / f\"company_basics_{run_id}.csv\"\n",
    "CLAIMS_CSV = ART_DIR / f\"company_claims_{run_id}.csv\"\n",
    "\n",
    "df_company.to_csv(COMPANY_CSV, index=False)\n",
    "df_claims.to_csv(CLAIMS_CSV, index=False)\n",
    "\n",
    "print(\"✅ Company basics (expanded) complete\")\n",
    "print(f\"- Raw pages: {RAW_PATH.as_posix()}\")\n",
    "print(f\"- Extraction JSON: {EXTRACT_PATH.as_posix()}\")\n",
    "print(f\"- Company DF: {COMPANY_CSV.as_posix()}\")\n",
    "print(f\"- Claims DF: {CLAIMS_CSV.as_posix()}\")\n",
    "\n",
    "# Show previews\n",
    "display(df_company)\n",
    "display(df_claims.head(20) if not df_claims.empty else df_claims)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7588d0c1-e601-402b-99aa-b8b299d6385d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Official people targets: 7\n",
      "CSE people targets: 20\n",
      "✅ People raw pages saved: artifacts/meeting_deep_dive/people_raw_pages_20260107_225628.jsonl\n",
      "Usable pages: 12 | Shortlist to LLM: 10\n",
      "✅ Key People Extraction complete\n",
      "- Raw pages: artifacts/meeting_deep_dive/people_raw_pages_20260107_225628.jsonl\n",
      "- Extraction JSON: artifacts/meeting_deep_dive/people_extraction_20260107_225628.json\n",
      "- People DF: artifacts/meeting_deep_dive/people_directory_20260107_225628.csv\n",
      "\n",
      "Meeting participant match:\n",
      "- input: David Ha / 代表取締役\n",
      "- matched_person_id: david-ha\n",
      "- confidence: 1\n",
      "- rationale: David Ha is identified as the CEO and co-founder of Sakana AI, matching the title of 代表取締役.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>run_id</th>\n",
       "      <th>company_name</th>\n",
       "      <th>person_id</th>\n",
       "      <th>full_name</th>\n",
       "      <th>role_title</th>\n",
       "      <th>category</th>\n",
       "      <th>bio_summary</th>\n",
       "      <th>signals</th>\n",
       "      <th>company_profile_url</th>\n",
       "      <th>linkedin_url</th>\n",
       "      <th>x_url</th>\n",
       "      <th>other_links</th>\n",
       "      <th>evidence_url</th>\n",
       "      <th>evidence_points</th>\n",
       "      <th>confidence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20260107_225628</td>\n",
       "      <td>Sakana AI</td>\n",
       "      <td>david-ha</td>\n",
       "      <td>David Ha</td>\n",
       "      <td>CEO</td>\n",
       "      <td>founder</td>\n",
       "      <td>David Ha is the co-founder and CEO of Sakana A...</td>\n",
       "      <td>[ex-Google, PhD]</td>\n",
       "      <td>https://sakana.ai/series-a</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>[]</td>\n",
       "      <td>https://time.com/collections/time100-ai-2025/7...</td>\n",
       "      <td>[Co-founder and CEO of Sakana AI, Developed AI...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20260107_225628</td>\n",
       "      <td>Sakana AI</td>\n",
       "      <td>llion-jones</td>\n",
       "      <td>Llion Jones</td>\n",
       "      <td>CTO</td>\n",
       "      <td>founder</td>\n",
       "      <td>Llion Jones is the co-founder and CTO of Sakan...</td>\n",
       "      <td>[ex-Google, PhD]</td>\n",
       "      <td>https://sakana.ai/series-a</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>[]</td>\n",
       "      <td>https://sakana.ai/seed-round/</td>\n",
       "      <td>[Co-founder and CTO of Sakana AI, Co-author of...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20260107_225628</td>\n",
       "      <td>Sakana AI</td>\n",
       "      <td>ren-ito</td>\n",
       "      <td>Ren Ito</td>\n",
       "      <td>COO</td>\n",
       "      <td>founder</td>\n",
       "      <td>Ren Ito is the co-founder and COO of Sakana AI...</td>\n",
       "      <td>[ex-Mercari, diplomat]</td>\n",
       "      <td>https://sakana.ai/mufg-bank/</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>[]</td>\n",
       "      <td>https://sakana.ai/mufg-bank/</td>\n",
       "      <td>[Co-founder and COO of Sakana AI, Former CEO o...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            run_id company_name    person_id    full_name role_title category  \\\n",
       "0  20260107_225628    Sakana AI     david-ha     David Ha        CEO  founder   \n",
       "1  20260107_225628    Sakana AI  llion-jones  Llion Jones        CTO  founder   \n",
       "2  20260107_225628    Sakana AI      ren-ito      Ren Ito        COO  founder   \n",
       "\n",
       "                                         bio_summary                 signals  \\\n",
       "0  David Ha is the co-founder and CEO of Sakana A...        [ex-Google, PhD]   \n",
       "1  Llion Jones is the co-founder and CTO of Sakan...        [ex-Google, PhD]   \n",
       "2  Ren Ito is the co-founder and COO of Sakana AI...  [ex-Mercari, diplomat]   \n",
       "\n",
       "            company_profile_url linkedin_url x_url other_links  \\\n",
       "0    https://sakana.ai/series-a         None  None          []   \n",
       "1    https://sakana.ai/series-a         None  None          []   \n",
       "2  https://sakana.ai/mufg-bank/         None  None          []   \n",
       "\n",
       "                                        evidence_url  \\\n",
       "0  https://time.com/collections/time100-ai-2025/7...   \n",
       "1                      https://sakana.ai/seed-round/   \n",
       "2                       https://sakana.ai/mufg-bank/   \n",
       "\n",
       "                                     evidence_points  confidence  \n",
       "0  [Co-founder and CEO of Sakana AI, Developed AI...           1  \n",
       "1  [Co-founder and CTO of Sakana AI, Co-author of...           1  \n",
       "2  [Co-founder and COO of Sakana AI, Former CEO o...           1  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ============================================================\n",
    "# 4. Key People Extraction (expanded, evidence-linked)\n",
    "# ============================================================\n",
    "# Goal:\n",
    "# - Identify key people at the startup:\n",
    "#   (a) the meeting participant (from inputs), and\n",
    "#   (b) other major members (founders, executives, leadership, board, etc.)\n",
    "# - Use the same pattern as Company Basics:\n",
    "#   official pages first + CSE enrichment, lightweight robots check,\n",
    "#   store raw pages, then use OpenAI to extract + normalize.\n",
    "#\n",
    "# Outputs (saved under artifacts/meeting_deep_dive/):\n",
    "# - people_raw_pages_<run_id>.jsonl\n",
    "# - people_directory_<run_id>.csv        (people table)\n",
    "# - people_extraction_<run_id>.json      (full model output)\n",
    "#\n",
    "# Notes:\n",
    "# - This cell is conservative: it only includes people supported by evidence URLs.\n",
    "# - It also tries to link the meeting participant to a matching person profile (if found).\n",
    "\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "import time\n",
    "import requests\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "from pathlib import Path\n",
    "from urllib.parse import urlparse, urljoin\n",
    "from datetime import datetime, timezone\n",
    "from urllib.robotparser import RobotFileParser\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Global configuration\n",
    "# ------------------------------------------------------------\n",
    "load_dotenv(\"env.txt\")\n",
    "\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "if not OPENAI_API_KEY:\n",
    "    raise EnvironmentError(\"OPENAI_API_KEY is not set.\")\n",
    "\n",
    "GOOGLE_API_KEY = os.getenv(\"GOOGLE_API_KEY\")\n",
    "GOOGLE_CSE_CX = os.getenv(\"GOOGLE_CSE_CX\")\n",
    "if not GOOGLE_API_KEY or not GOOGLE_CSE_CX:\n",
    "    raise EnvironmentError(\"GOOGLE_API_KEY and GOOGLE_CSE_CX must be set.\")\n",
    "\n",
    "client = OpenAI(api_key=OPENAI_API_KEY)\n",
    "\n",
    "ART_DIR = Path(\"artifacts\") / \"meeting_deep_dive\"\n",
    "ART_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Load inputs + entity (from prior cells)\n",
    "# ------------------------------------------------------------\n",
    "def load_latest_json(pattern: str) -> dict:\n",
    "    files = sorted(ART_DIR.glob(pattern), key=lambda p: p.stat().st_mtime, reverse=True)\n",
    "    if not files:\n",
    "        raise FileNotFoundError(f\"No {pattern} found. Please run the previous cells.\")\n",
    "    return json.loads(files[0].read_text(encoding=\"utf-8\"))\n",
    "\n",
    "try:\n",
    "    run_id = inputs[\"meta\"][\"run_id\"]\n",
    "    meeting_person_name = (inputs.get(\"meeting_person\", {}).get(\"name\") or \"\").strip()\n",
    "    meeting_person_title = (inputs.get(\"meeting_person\", {}).get(\"title\") or \"\").strip()\n",
    "except Exception:\n",
    "    # fallback to latest inputs file\n",
    "    latest_inputs = load_latest_json(\"inputs_*.json\")\n",
    "    run_id = latest_inputs.get(\"meta\", {}).get(\"run_id\") or datetime.now(timezone.utc).strftime(\"%Y%m%d_%H%M%S\")\n",
    "    meeting_person_name = (latest_inputs.get(\"meeting_person\", {}).get(\"name\") or \"\").strip()\n",
    "    meeting_person_title = (latest_inputs.get(\"meeting_person\", {}).get(\"title\") or \"\").strip()\n",
    "\n",
    "entity = load_latest_json(\"entity_*.json\")\n",
    "startup_name = (entity.get(\"canonical_name\") or \"Unknown Startup\").strip()\n",
    "official_website = (entity.get(\"official_website\") or \"\").strip()\n",
    "if not official_website:\n",
    "    raise ValueError(\"official_website is missing in entity. Run cell #2.\")\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Knobs\n",
    "# ------------------------------------------------------------\n",
    "USER_AGENT = \"researchOSv2-bot/0.2 (+contact: internal-research)\"\n",
    "TIMEOUT = 25\n",
    "SLEEP_SEC = 0.25\n",
    "\n",
    "MAX_OFFICIAL_PAGES = 25\n",
    "MAX_CSE_PAGES = 20\n",
    "MAX_TEXT_CHARS_STORE = 60000\n",
    "MAX_TEXT_CHARS_PROMPT = 9000\n",
    "MAX_PAGES_TO_LLM = 18\n",
    "\n",
    "PEOPLE_KEY_PATHS = [\n",
    "    \"/team\", \"/about\", \"/company\", \"/leadership\", \"/management\",\n",
    "    \"/members\", \"/people\", \"/board\", \"/governance\",\n",
    "    \"/careers\", \"/jobs\", \"/recruit\",\n",
    "    \"/news\", \"/press\", \"/blog\"\n",
    "]\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# URL / domain / robots helpers\n",
    "# ------------------------------------------------------------\n",
    "def normalize_url(url: str) -> str:\n",
    "    url = (url or \"\").strip()\n",
    "    if not url:\n",
    "        return \"\"\n",
    "    if not re.match(r\"^https?://\", url, flags=re.I):\n",
    "        url = \"https://\" + url\n",
    "    p = urlparse(url)\n",
    "    return p._replace(netloc=p.netloc.lower(), fragment=\"\").geturl()\n",
    "\n",
    "def get_domain(url: str) -> str:\n",
    "    return re.sub(r\"^www\\.\", \"\", urlparse(normalize_url(url)).netloc)\n",
    "\n",
    "def same_domain(a: str, b: str) -> bool:\n",
    "    da, db = get_domain(a), get_domain(b)\n",
    "    return da == db or da.endswith(\".\" + db) or db.endswith(\".\" + da)\n",
    "\n",
    "def is_asset_url(url: str) -> bool:\n",
    "    path = (urlparse(url).path or \"\").lower()\n",
    "    return bool(re.search(r\"\\.(pdf|jpg|jpeg|png|gif|svg|zip|mp4|mov)$\", path))\n",
    "\n",
    "def build_robot_parser(base_url: str) -> RobotFileParser | None:\n",
    "    rp = RobotFileParser()\n",
    "    rp.set_url(urljoin(normalize_url(base_url), \"/robots.txt\"))\n",
    "    try:\n",
    "        rp.read()\n",
    "        return rp\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "def robots_allows(rp: RobotFileParser | None, url: str) -> bool:\n",
    "    if rp is None:\n",
    "        # conservative fallback\n",
    "        p = (urlparse(url).path or \"/\").lower()\n",
    "        return p in [\"/\", \"\"] or any(k in p for k in [\"team\", \"about\", \"company\", \"leadership\", \"people\", \"board\", \"news\", \"press\", \"blog\"])\n",
    "    try:\n",
    "        return rp.can_fetch(USER_AGENT, url)\n",
    "    except Exception:\n",
    "        return False\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# HTML extraction helpers\n",
    "# ------------------------------------------------------------\n",
    "def fetch_html(url: str) -> tuple[int, str]:\n",
    "    r = requests.get(url, headers={\"User-Agent\": USER_AGENT}, timeout=TIMEOUT, allow_redirects=True)\n",
    "    return r.status_code, (r.text if r.ok else \"\")\n",
    "\n",
    "def html_to_text(html: str) -> str:\n",
    "    soup = BeautifulSoup(html, \"lxml\")\n",
    "    for tag in soup([\"script\", \"style\", \"noscript\", \"svg\"]):\n",
    "        tag.decompose()\n",
    "    node = soup.find(\"main\") or soup.find(\"article\") or (soup.body if soup.body else soup)\n",
    "    text = node.get_text(\"\\n\", strip=True)\n",
    "    text = re.sub(r\"\\n{3,}\", \"\\n\\n\", text)\n",
    "    return text.strip()\n",
    "\n",
    "def extract_links(base_url: str, html: str) -> list[str]:\n",
    "    soup = BeautifulSoup(html, \"lxml\")\n",
    "    out = []\n",
    "    for a in soup.find_all(\"a\", href=True):\n",
    "        href = a[\"href\"].strip()\n",
    "        if href.startswith(\"#\") or href.lower().startswith(\"mailto:\") or href.lower().startswith(\"tel:\"):\n",
    "            continue\n",
    "        u = normalize_url(urljoin(base_url, href))\n",
    "        if u.startswith((\"http://\", \"https://\")):\n",
    "            out.append(u)\n",
    "    # unique preserve order\n",
    "    seen, uniq = set(), []\n",
    "    for u in out:\n",
    "        if u not in seen:\n",
    "            seen.add(u)\n",
    "            uniq.append(u)\n",
    "    return uniq\n",
    "\n",
    "def score_people_link(url: str) -> float:\n",
    "    p = (urlparse(url).path or \"/\").lower()\n",
    "    score = 0.0\n",
    "    if p in [\"/team\", \"/people\", \"/leadership\", \"/management\", \"/board\"]:\n",
    "        score += 3.0\n",
    "    for k in [\"team\", \"people\", \"leadership\", \"management\", \"board\", \"company\", \"about\"]:\n",
    "        if k in p:\n",
    "            score += 1.0\n",
    "    if is_asset_url(url):\n",
    "        score -= 4.0\n",
    "    if p.count(\"/\") >= 4:\n",
    "        score -= 0.4\n",
    "    return score\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Google CSE\n",
    "# ------------------------------------------------------------\n",
    "def google_cse_search(query: str, num: int = 5) -> list[dict]:\n",
    "    url = \"https://www.googleapis.com/customsearch/v1\"\n",
    "    params = {\"key\": GOOGLE_API_KEY, \"cx\": GOOGLE_CSE_CX, \"q\": query, \"num\": min(max(num, 1), 10)}\n",
    "    r = requests.get(url, params=params, timeout=30)\n",
    "    r.raise_for_status()\n",
    "    data = r.json()\n",
    "    items = data.get(\"items\", []) or []\n",
    "    return [{\n",
    "        \"title\": it.get(\"title\"),\n",
    "        \"link\": normalize_url(it.get(\"link\") or \"\"),\n",
    "        \"snippet\": it.get(\"snippet\"),\n",
    "        \"displayLink\": it.get(\"displayLink\"),\n",
    "    } for it in items]\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Build targets (official + CSE)\n",
    "# ------------------------------------------------------------\n",
    "official_home = normalize_url(official_website)\n",
    "rp_official = build_robot_parser(official_home)\n",
    "\n",
    "# homepage links\n",
    "status, html = fetch_html(official_home)\n",
    "time.sleep(SLEEP_SEC)\n",
    "if status >= 400 or not html:\n",
    "    raise RuntimeError(f\"Failed to fetch official homepage: HTTP {status}\")\n",
    "\n",
    "home_links = extract_links(official_home, html)\n",
    "internal_links = [l for l in home_links if same_domain(l, official_home) and not is_asset_url(l)]\n",
    "\n",
    "# explicit paths\n",
    "explicit = []\n",
    "for p in PEOPLE_KEY_PATHS:\n",
    "    u = normalize_url(urljoin(official_home, p))\n",
    "    if robots_allows(rp_official, u) and not is_asset_url(u):\n",
    "        explicit.append(u)\n",
    "\n",
    "official_candidates = list(dict.fromkeys([official_home] + explicit + internal_links))\n",
    "scored = [{\"url\": u, \"score\": score_people_link(u)} for u in official_candidates]\n",
    "scored.sort(key=lambda x: x[\"score\"], reverse=True)\n",
    "\n",
    "official_targets = []\n",
    "for it in scored:\n",
    "    if len(official_targets) >= MAX_OFFICIAL_PAGES:\n",
    "        break\n",
    "    if it[\"score\"] < 0.2:\n",
    "        continue\n",
    "    u = it[\"url\"]\n",
    "    if robots_allows(rp_official, u):\n",
    "        official_targets.append(u)\n",
    "\n",
    "official_targets = list(dict.fromkeys(official_targets))\n",
    "print(f\"Official people targets: {len(official_targets)}\")\n",
    "\n",
    "# CSE queries (people-focused)\n",
    "official_domain = get_domain(official_home)\n",
    "queries = [\n",
    "    f'\"{startup_name}\" founder',\n",
    "    f'\"{startup_name}\" co-founder',\n",
    "    f'\"{startup_name}\" CEO',\n",
    "    f'\"{startup_name}\" leadership team',\n",
    "    f'\"{startup_name}\" management team',\n",
    "    f'\"{startup_name}\" board member',\n",
    "    f'\"{startup_name}\" {meeting_person_name}' if meeting_person_name else None,\n",
    "    f'\"{startup_name}\" 役員',\n",
    "    f'\"{startup_name}\" 創業者',\n",
    "    f'\"{startup_name}\" 経営陣',\n",
    "    f'\"{startup_name}\" {official_domain}',\n",
    "]\n",
    "queries = [q for q in queries if q]\n",
    "\n",
    "cse_items = []\n",
    "for q in queries:\n",
    "    try:\n",
    "        cse_items.extend(google_cse_search(q, num=5))\n",
    "        time.sleep(0.2)\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "# Dedup and cap\n",
    "seen = set()\n",
    "cse_links = []\n",
    "for it in cse_items:\n",
    "    link = it.get(\"link\")\n",
    "    if link and link not in seen and link.startswith((\"http://\", \"https://\")):\n",
    "        seen.add(link)\n",
    "        cse_links.append(link)\n",
    "cse_links = cse_links[:MAX_CSE_PAGES]\n",
    "print(f\"CSE people targets: {len(cse_links)}\")\n",
    "\n",
    "# Robots cache for third-party domains\n",
    "_robot_cache: dict[str, RobotFileParser | None] = {}\n",
    "\n",
    "def get_rp_for_url(url: str) -> RobotFileParser | None:\n",
    "    dom = get_domain(url)\n",
    "    if dom in _robot_cache:\n",
    "        return _robot_cache[dom]\n",
    "    rp = build_robot_parser(url)\n",
    "    _robot_cache[dom] = rp\n",
    "    return rp\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Fetch + store raw pages (JSONL)\n",
    "# ------------------------------------------------------------\n",
    "RAW_PATH = ART_DIR / f\"people_raw_pages_{run_id}.jsonl\"\n",
    "RAW_PATH.write_text(\"\", encoding=\"utf-8\")\n",
    "\n",
    "def append_jsonl(path: Path, obj: dict):\n",
    "    with path.open(\"a\", encoding=\"utf-8\") as f:\n",
    "        f.write(json.dumps(obj, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "def collect_pages(urls: list[str], source_type: str):\n",
    "    for u in urls:\n",
    "        u = normalize_url(u)\n",
    "        if not u or is_asset_url(u):\n",
    "            continue\n",
    "\n",
    "        rp = rp_official if source_type == \"official\" else get_rp_for_url(u)\n",
    "        allowed = robots_allows(rp, u)\n",
    "\n",
    "        if not allowed:\n",
    "            append_jsonl(RAW_PATH, {\n",
    "                \"url\": u,\n",
    "                \"source_type\": source_type,\n",
    "                \"domain\": get_domain(u),\n",
    "                \"fetched_at_utc\": datetime.now(timezone.utc).isoformat(),\n",
    "                \"http_status\": None,\n",
    "                \"robots_allowed\": False,\n",
    "                \"text\": \"\",\n",
    "                \"notes\": \"Skipped due to robots.txt (or conservative fallback).\",\n",
    "            })\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            code, html = fetch_html(u)\n",
    "            time.sleep(SLEEP_SEC)\n",
    "            text = html_to_text(html) if html else \"\"\n",
    "            text = text[:MAX_TEXT_CHARS_STORE]\n",
    "\n",
    "            append_jsonl(RAW_PATH, {\n",
    "                \"url\": u,\n",
    "                \"source_type\": source_type,\n",
    "                \"domain\": get_domain(u),\n",
    "                \"fetched_at_utc\": datetime.now(timezone.utc).isoformat(),\n",
    "                \"http_status\": code,\n",
    "                \"robots_allowed\": True,\n",
    "                \"text\": text,\n",
    "                \"text_char_len\": len(text),\n",
    "            })\n",
    "        except Exception as e:\n",
    "            append_jsonl(RAW_PATH, {\n",
    "                \"url\": u,\n",
    "                \"source_type\": source_type,\n",
    "                \"domain\": get_domain(u),\n",
    "                \"fetched_at_utc\": datetime.now(timezone.utc).isoformat(),\n",
    "                \"http_status\": None,\n",
    "                \"robots_allowed\": True,\n",
    "                \"text\": \"\",\n",
    "                \"notes\": f\"Fetch error: {str(e)}\",\n",
    "            })\n",
    "\n",
    "collect_pages(official_targets, \"official\")\n",
    "collect_pages(cse_links, \"cse\")\n",
    "\n",
    "print(f\"✅ People raw pages saved: {RAW_PATH.as_posix()}\")\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Load + shortlist to LLM (official first)\n",
    "# ------------------------------------------------------------\n",
    "def load_raw_pages(path: Path) -> list[dict]:\n",
    "    rows = []\n",
    "    for line in path.read_text(encoding=\"utf-8\").splitlines():\n",
    "        if line.strip():\n",
    "            rows.append(json.loads(line))\n",
    "    return rows\n",
    "\n",
    "raw_pages = load_raw_pages(RAW_PATH)\n",
    "\n",
    "usable_pages = [\n",
    "    r for r in raw_pages\n",
    "    if r.get(\"robots_allowed\")\n",
    "    and isinstance(r.get(\"http_status\"), int) and r[\"http_status\"] < 400\n",
    "    and (r.get(\"text\") or \"\").strip()\n",
    "]\n",
    "\n",
    "official_usable = [r for r in usable_pages if r.get(\"source_type\") == \"official\"]\n",
    "cse_usable = [r for r in usable_pages if r.get(\"source_type\") == \"cse\"]\n",
    "\n",
    "shortlist = (official_usable[:12] + cse_usable[:10])[:MAX_PAGES_TO_LLM]\n",
    "print(f\"Usable pages: {len(usable_pages)} | Shortlist to LLM: {len(shortlist)}\")\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# OpenAI extraction: people directory + meeting-person match\n",
    "# ------------------------------------------------------------\n",
    "# ------------------------------------------------------------\n",
    "# OpenAI extraction (robust JSON handling)\n",
    "# ------------------------------------------------------------\n",
    "import json\n",
    "import re\n",
    "\n",
    "def _extract_json_object(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Best-effort extraction of a JSON object from arbitrary text.\n",
    "    Handles:\n",
    "    - ```json ... ```\n",
    "    - leading/trailing commentary\n",
    "    - extra whitespace\n",
    "    \"\"\"\n",
    "    if not text:\n",
    "        return \"\"\n",
    "    t = text.strip()\n",
    "\n",
    "    # Remove fenced code blocks (```json ... ```)\n",
    "    fence = re.search(r\"```(?:json)?\\s*(\\{.*?\\})\\s*```\", t, flags=re.DOTALL | re.IGNORECASE)\n",
    "    if fence:\n",
    "        return fence.group(1).strip()\n",
    "\n",
    "    # Otherwise: find the first \"{\" and last \"}\" and slice\n",
    "    start = t.find(\"{\")\n",
    "    end = t.rfind(\"}\")\n",
    "    if start != -1 and end != -1 and end > start:\n",
    "        return t[start:end+1].strip()\n",
    "\n",
    "    return \"\"\n",
    "\n",
    "\n",
    "def openai_extract_people_directory(\n",
    "    startup_name: str,\n",
    "    official_website: str,\n",
    "    meeting_person_name: str,\n",
    "    meeting_person_title: str,\n",
    "    pages: list[dict]\n",
    ") -> dict:\n",
    "    \"\"\"\n",
    "    Robust extractor:\n",
    "    - First try \"strict JSON mode\" via response_format (if supported).\n",
    "    - If that still fails, attempt to extract JSON from the text.\n",
    "    - If still failing, raise a clear error and print the raw output.\n",
    "    \"\"\"\n",
    "    system = (\n",
    "        \"You are a research analyst assistant focused on extracting people/leadership information. \"\n",
    "        \"From the provided webpage text, identify key people at the startup \"\n",
    "        \"(founders, executives, leadership, board). \"\n",
    "        \"Be conservative: only include a person if supported by an evidence_url from the provided pages. \"\n",
    "        \"Also try to match the meeting participant to one of the extracted people. \"\n",
    "        \"Return JSON only.\"\n",
    "    )\n",
    "\n",
    "    schema = {\n",
    "        \"company\": {\"name\": \"string\", \"official_website\": \"string|null\"},\n",
    "        \"meeting_participant\": {\n",
    "            \"input_name\": \"string|null\",\n",
    "            \"input_title\": \"string|null\",\n",
    "            \"matched_person_id\": \"string|null\",\n",
    "            \"match_confidence\": \"0..1\",\n",
    "            \"match_rationale\": \"string\"\n",
    "        },\n",
    "        \"people\": [\n",
    "            {\n",
    "                \"person_id\": \"string (stable id, e.g., slug)\",\n",
    "                \"full_name\": \"string\",\n",
    "                \"role_title\": \"string|null\",\n",
    "                \"category\": \"founder|executive|leadership|board|other\",\n",
    "                \"bio_summary\": \"string|null\",\n",
    "                \"signals\": \"array of strings (e.g., 'ex-Google', 'PhD', etc.)\",\n",
    "                \"links\": {\n",
    "                    \"company_profile\": \"string|null\",\n",
    "                    \"linkedin\": \"string|null\",\n",
    "                    \"x\": \"string|null\",\n",
    "                    \"other\": \"array of strings\"\n",
    "                },\n",
    "                \"evidence\": {\n",
    "                    \"evidence_url\": \"string\",\n",
    "                    \"extracted_points\": \"array of strings\"\n",
    "                },\n",
    "                \"confidence\": \"0..1\"\n",
    "            }\n",
    "        ],\n",
    "        \"notes\": {\n",
    "            \"name_collisions\": \"array of strings\",\n",
    "            \"data_gaps\": \"array of strings\"\n",
    "        }\n",
    "    }\n",
    "\n",
    "    bundle = {\n",
    "        \"startup_name\": startup_name,\n",
    "        \"official_website\": official_website,\n",
    "        \"meeting_participant_input\": {\n",
    "            \"name\": meeting_person_name or None,\n",
    "            \"title\": meeting_person_title or None\n",
    "        },\n",
    "        \"pages\": [\n",
    "            {\n",
    "                \"url\": p[\"url\"],\n",
    "                \"source_type\": p[\"source_type\"],\n",
    "                \"domain\": p.get(\"domain\"),\n",
    "                \"text\": (p.get(\"text\") or \"\")[:MAX_TEXT_CHARS_PROMPT],\n",
    "            } for p in pages\n",
    "        ],\n",
    "        \"schema\": schema\n",
    "    }\n",
    "\n",
    "    # 1) Try strict JSON mode (preferred)\n",
    "    try:\n",
    "        resp = client.chat.completions.create(\n",
    "            model=\"gpt-4o-mini\",\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": system},\n",
    "                {\"role\": \"user\", \"content\": json.dumps(bundle, ensure_ascii=False)},\n",
    "            ],\n",
    "            temperature=0.0,\n",
    "            response_format={\"type\": \"json_object\"},  # <-- key change\n",
    "        )\n",
    "        content = (resp.choices[0].message.content or \"\").strip()\n",
    "        if content:\n",
    "            return json.loads(content)\n",
    "    except Exception as e:\n",
    "        # We'll fallback to lenient parsing below\n",
    "        last_error = e\n",
    "\n",
    "    # 2) Fallback: request again with explicit \"no markdown\" + then extract JSON\n",
    "    resp2 = client.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system + \" Output must be a single JSON object. No markdown. No commentary.\"},\n",
    "            {\"role\": \"user\", \"content\": json.dumps(bundle, ensure_ascii=False)},\n",
    "        ],\n",
    "        temperature=0.0,\n",
    "    )\n",
    "    raw = (resp2.choices[0].message.content or \"\")\n",
    "\n",
    "    candidate = _extract_json_object(raw)\n",
    "    if candidate:\n",
    "        try:\n",
    "            return json.loads(candidate)\n",
    "        except Exception as e:\n",
    "            # Debug-friendly error\n",
    "            print(\"---- Raw model output (truncated) ----\")\n",
    "            print(raw[:2000])\n",
    "            print(\"---- Extracted JSON candidate (truncated) ----\")\n",
    "            print(candidate[:2000])\n",
    "            raise RuntimeError(f\"Failed to parse extracted JSON candidate: {e}\") from e\n",
    "\n",
    "    # 3) If we still cannot recover, raise a clear error\n",
    "    print(\"---- Raw model output (truncated) ----\")\n",
    "    print(raw[:2000])\n",
    "    raise RuntimeError(\n",
    "        \"Model did not return a JSON object. \"\n",
    "        \"Consider reducing pages, reducing MAX_TEXT_CHARS_PROMPT, or switching the model.\"\n",
    "    )\n",
    "\n",
    "\n",
    "extracted = openai_extract_people_directory(\n",
    "    startup_name=startup_name,\n",
    "    official_website=official_home,\n",
    "    meeting_person_name=meeting_person_name,\n",
    "    meeting_person_title=meeting_person_title,\n",
    "    pages=shortlist\n",
    ")\n",
    "\n",
    "# Save extraction JSON\n",
    "EXTRACT_PATH = ART_DIR / f\"people_extraction_{run_id}.json\"\n",
    "EXTRACT_PATH.write_text(json.dumps(extracted, ensure_ascii=False, indent=2), encoding=\"utf-8\")\n",
    "\n",
    "# Build people DF\n",
    "people = extracted.get(\"people\", []) or []\n",
    "df_people = pd.DataFrame(people)\n",
    "\n",
    "if not df_people.empty:\n",
    "    # flatten some nested fields for CSV friendliness\n",
    "    def _get(d, *keys):\n",
    "        cur = d\n",
    "        for k in keys:\n",
    "            if not isinstance(cur, dict):\n",
    "                return None\n",
    "            cur = cur.get(k)\n",
    "        return cur\n",
    "\n",
    "    df_people[\"run_id\"] = run_id\n",
    "    df_people[\"company_name\"] = startup_name\n",
    "    df_people[\"evidence_url\"] = df_people[\"evidence\"].apply(lambda x: _get(x, \"evidence_url\"))\n",
    "    df_people[\"evidence_points\"] = df_people[\"evidence\"].apply(lambda x: _get(x, \"extracted_points\"))\n",
    "    df_people[\"company_profile_url\"] = df_people[\"links\"].apply(lambda x: _get(x, \"company_profile\"))\n",
    "    df_people[\"linkedin_url\"] = df_people[\"links\"].apply(lambda x: _get(x, \"linkedin\"))\n",
    "    df_people[\"x_url\"] = df_people[\"links\"].apply(lambda x: _get(x, \"x\"))\n",
    "    df_people[\"other_links\"] = df_people[\"links\"].apply(lambda x: _get(x, \"other\"))\n",
    "\n",
    "    # keep a readable column order\n",
    "    cols = [\n",
    "        \"run_id\", \"company_name\",\n",
    "        \"person_id\", \"full_name\", \"role_title\", \"category\",\n",
    "        \"bio_summary\", \"signals\",\n",
    "        \"company_profile_url\", \"linkedin_url\", \"x_url\", \"other_links\",\n",
    "        \"evidence_url\", \"evidence_points\",\n",
    "        \"confidence\"\n",
    "    ]\n",
    "    cols = [c for c in cols if c in df_people.columns]\n",
    "    df_people = df_people[cols]\n",
    "\n",
    "# Save people directory\n",
    "PEOPLE_CSV = ART_DIR / f\"people_directory_{run_id}.csv\"\n",
    "df_people.to_csv(PEOPLE_CSV, index=False)\n",
    "\n",
    "# Meeting participant matching info (print)\n",
    "mp = extracted.get(\"meeting_participant\", {}) or {}\n",
    "print(\"✅ Key People Extraction complete\")\n",
    "print(f\"- Raw pages: {RAW_PATH.as_posix()}\")\n",
    "print(f\"- Extraction JSON: {EXTRACT_PATH.as_posix()}\")\n",
    "print(f\"- People DF: {PEOPLE_CSV.as_posix()}\")\n",
    "print()\n",
    "print(\"Meeting participant match:\")\n",
    "print(f\"- input: {mp.get('input_name')} / {mp.get('input_title')}\")\n",
    "print(f\"- matched_person_id: {mp.get('matched_person_id')}\")\n",
    "print(f\"- confidence: {mp.get('match_confidence')}\")\n",
    "print(f\"- rationale: {mp.get('match_rationale')}\")\n",
    "\n",
    "display(df_people)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bf61afed-e067-49b9-aec5-3a4a22067b4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target people:\n",
      "- David Ha | 代表取締役\n",
      "- Llion Jones | CTO\n",
      "- Ren Ito | COO\n",
      "Fetching pages (robots-respecting)...\n",
      "✅ Deep dive raw pages saved: artifacts/meeting_deep_dive/meeting_deep_dive_raw_pages_20260107_225628.jsonl\n",
      "Usable pages: 21 | Shortlist to LLM: 18\n",
      "✅ Meeting Person Deep Dive complete\n",
      "- Raw pages: artifacts/meeting_deep_dive/meeting_deep_dive_raw_pages_20260107_225628.jsonl\n",
      "- JSON: artifacts/meeting_deep_dive/meeting_person_deep_dive_20260107_225628.json\n",
      "- Profiles CSV: artifacts/meeting_deep_dive/people_profiles_20260107_225628.csv\n",
      "- Content CSV: artifacts/meeting_deep_dive/people_content_index_20260107_225628.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: []\n",
       "Index: []"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>run_id</th>\n",
       "      <th>startup_name</th>\n",
       "      <th>person_name</th>\n",
       "      <th>content_type</th>\n",
       "      <th>title</th>\n",
       "      <th>publisher_or_platform</th>\n",
       "      <th>date</th>\n",
       "      <th>url</th>\n",
       "      <th>what_it_contains</th>\n",
       "      <th>notable_points</th>\n",
       "      <th>relevance</th>\n",
       "      <th>confidence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20260107_225628</td>\n",
       "      <td>Sakana AI</td>\n",
       "      <td>David Ha</td>\n",
       "      <td>profile</td>\n",
       "      <td>Sakana AI CEO</td>\n",
       "      <td>Sakana AI</td>\n",
       "      <td>None</td>\n",
       "      <td>https://sakana.ai/seed-round/</td>\n",
       "      <td>Overview of David Ha's role and background.</td>\n",
       "      <td>[Co-founder and CEO of Sakana AI., Former mana...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20260107_225628</td>\n",
       "      <td>Sakana AI</td>\n",
       "      <td>Llion Jones</td>\n",
       "      <td>profile</td>\n",
       "      <td>CTO of Sakana AI</td>\n",
       "      <td>Sakana AI</td>\n",
       "      <td>None</td>\n",
       "      <td>https://sakana.ai/seed-round/</td>\n",
       "      <td>Overview of Llion Jones's role and background.</td>\n",
       "      <td>[Co-founder and CTO of Sakana AI., Formerly wo...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20260107_225628</td>\n",
       "      <td>Sakana AI</td>\n",
       "      <td>Ren Ito</td>\n",
       "      <td>profile</td>\n",
       "      <td>COO of Sakana AI</td>\n",
       "      <td>Sakana AI</td>\n",
       "      <td>None</td>\n",
       "      <td>https://sakana.ai/seed-round/</td>\n",
       "      <td>Overview of Ren Ito's role and background.</td>\n",
       "      <td>[Co-founder and COO of Sakana AI., Former exec...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>20260107_225628</td>\n",
       "      <td>Sakana AI</td>\n",
       "      <td>David Ha</td>\n",
       "      <td>interview</td>\n",
       "      <td>Research Retrospectives: An interview with Dav...</td>\n",
       "      <td>yeschat.ai</td>\n",
       "      <td>2024-03-05</td>\n",
       "      <td>https://www.yeschat.ai/blog-Research-Retrospec...</td>\n",
       "      <td>Discussion of David Ha's journey and insights ...</td>\n",
       "      <td>[Transitioned from finance to AI research., Em...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20260107_225628</td>\n",
       "      <td>Sakana AI</td>\n",
       "      <td>Ren Ito</td>\n",
       "      <td>article</td>\n",
       "      <td>MUFG enters multiyear AI partnership with Saka...</td>\n",
       "      <td>retailbankerinternational.com</td>\n",
       "      <td>2025-05-20</td>\n",
       "      <td>https://www.retailbankerinternational.com/news...</td>\n",
       "      <td>Details on the partnership between MUFG and Sa...</td>\n",
       "      <td>[Ren Ito named AI Advisor to MUFG., Focus on i...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>20260107_225628</td>\n",
       "      <td>Sakana AI</td>\n",
       "      <td>David Ha</td>\n",
       "      <td>article</td>\n",
       "      <td>Top Japan startup Sakana AI touts nature-inspi...</td>\n",
       "      <td>Japan Times</td>\n",
       "      <td>2025-09-10</td>\n",
       "      <td>https://www.japantimes.co.jp/business/2025/09/...</td>\n",
       "      <td>Interview discussing Sakana AI's approach and ...</td>\n",
       "      <td>[Sakana AI became Japan's fastest unicorn., Fo...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            run_id startup_name  person_name content_type  \\\n",
       "0  20260107_225628    Sakana AI     David Ha      profile   \n",
       "1  20260107_225628    Sakana AI  Llion Jones      profile   \n",
       "2  20260107_225628    Sakana AI      Ren Ito      profile   \n",
       "3  20260107_225628    Sakana AI     David Ha    interview   \n",
       "4  20260107_225628    Sakana AI      Ren Ito      article   \n",
       "5  20260107_225628    Sakana AI     David Ha      article   \n",
       "\n",
       "                                               title  \\\n",
       "0                                      Sakana AI CEO   \n",
       "1                                   CTO of Sakana AI   \n",
       "2                                   COO of Sakana AI   \n",
       "3  Research Retrospectives: An interview with Dav...   \n",
       "4  MUFG enters multiyear AI partnership with Saka...   \n",
       "5  Top Japan startup Sakana AI touts nature-inspi...   \n",
       "\n",
       "           publisher_or_platform        date  \\\n",
       "0                      Sakana AI        None   \n",
       "1                      Sakana AI        None   \n",
       "2                      Sakana AI        None   \n",
       "3                     yeschat.ai  2024-03-05   \n",
       "4  retailbankerinternational.com  2025-05-20   \n",
       "5                    Japan Times  2025-09-10   \n",
       "\n",
       "                                                 url  \\\n",
       "0                      https://sakana.ai/seed-round/   \n",
       "1                      https://sakana.ai/seed-round/   \n",
       "2                      https://sakana.ai/seed-round/   \n",
       "3  https://www.yeschat.ai/blog-Research-Retrospec...   \n",
       "4  https://www.retailbankerinternational.com/news...   \n",
       "5  https://www.japantimes.co.jp/business/2025/09/...   \n",
       "\n",
       "                                    what_it_contains  \\\n",
       "0        Overview of David Ha's role and background.   \n",
       "1     Overview of Llion Jones's role and background.   \n",
       "2         Overview of Ren Ito's role and background.   \n",
       "3  Discussion of David Ha's journey and insights ...   \n",
       "4  Details on the partnership between MUFG and Sa...   \n",
       "5  Interview discussing Sakana AI's approach and ...   \n",
       "\n",
       "                                      notable_points  relevance  confidence  \n",
       "0  [Co-founder and CEO of Sakana AI., Former mana...          1           1  \n",
       "1  [Co-founder and CTO of Sakana AI., Formerly wo...          1           1  \n",
       "2  [Co-founder and COO of Sakana AI., Former exec...          1           1  \n",
       "3  [Transitioned from finance to AI research., Em...          1           1  \n",
       "4  [Ren Ito named AI Advisor to MUFG., Focus on i...          1           1  \n",
       "5  [Sakana AI became Japan's fastest unicorn., Fo...          1           1  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ============================================================\n",
    "# 5. Meeting Person Deep Dive (Facts → Hypotheses)\n",
    "# ============================================================\n",
    "# Goal:\n",
    "# - Deep dive on:\n",
    "#   (a) the meeting participant, and\n",
    "#   (b) other major members (founders/executives) if needed\n",
    "# - Collect: background/career, public posts, interviews, articles, talks, and notable quotes.\n",
    "# - Use the same pattern:\n",
    "#   official first + CSE enrichment, robots check, store raw pages,\n",
    "#   then OpenAI normalizes into:\n",
    "#     1) PeopleProfile table (one row per person)\n",
    "#     2) ContentIndex table (one row per content item, evidence URL + type + confidence)\n",
    "#\n",
    "# Outputs:\n",
    "# - meeting_deep_dive_raw_pages_<run_id>.jsonl\n",
    "# - people_profiles_<run_id>.csv\n",
    "# - people_content_index_<run_id>.csv\n",
    "# - meeting_person_deep_dive_<run_id>.json\n",
    "#\n",
    "# Notes:\n",
    "# - Conservative: only include claims supported by evidence URLs.\n",
    "# - The meeting participant is the priority; others are \"optional add-ons.\"\n",
    "# - If LinkedIn is blocked by robots / login wall, we keep only metadata from snippets.\n",
    "\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "import time\n",
    "import requests\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "from pathlib import Path\n",
    "from urllib.parse import urlparse, urljoin\n",
    "from datetime import datetime, timezone\n",
    "from urllib.robotparser import RobotFileParser\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Global configuration\n",
    "# ------------------------------------------------------------\n",
    "load_dotenv(\"env.txt\")\n",
    "\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "if not OPENAI_API_KEY:\n",
    "    raise EnvironmentError(\"OPENAI_API_KEY is not set.\")\n",
    "\n",
    "GOOGLE_API_KEY = os.getenv(\"GOOGLE_API_KEY\")\n",
    "GOOGLE_CSE_CX = os.getenv(\"GOOGLE_CSE_CX\")\n",
    "if not GOOGLE_API_KEY or not GOOGLE_CSE_CX:\n",
    "    raise EnvironmentError(\"GOOGLE_API_KEY and GOOGLE_CSE_CX must be set.\")\n",
    "\n",
    "client = OpenAI(api_key=OPENAI_API_KEY)\n",
    "\n",
    "ART_DIR = Path(\"artifacts\") / \"meeting_deep_dive\"\n",
    "ART_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Load inputs + entity + people directory\n",
    "# ------------------------------------------------------------\n",
    "def load_latest_json(pattern: str) -> tuple[dict, Path]:\n",
    "    files = sorted(ART_DIR.glob(pattern), key=lambda p: p.stat().st_mtime, reverse=True)\n",
    "    if not files:\n",
    "        raise FileNotFoundError(f\"No {pattern} found. Please run previous cells.\")\n",
    "    p = files[0]\n",
    "    return json.loads(p.read_text(encoding=\"utf-8\")), p\n",
    "\n",
    "# Inputs (meeting person)\n",
    "try:\n",
    "    run_id = inputs[\"meta\"][\"run_id\"]\n",
    "    meeting_person_name = (inputs.get(\"meeting_person\", {}).get(\"name\") or \"\").strip()\n",
    "    meeting_person_title = (inputs.get(\"meeting_person\", {}).get(\"title\") or \"\").strip()\n",
    "except Exception:\n",
    "    inp, _ = load_latest_json(\"inputs_*.json\")\n",
    "    run_id = inp.get(\"meta\", {}).get(\"run_id\") or datetime.now(timezone.utc).strftime(\"%Y%m%d_%H%M%S\")\n",
    "    meeting_person_name = (inp.get(\"meeting_person\", {}).get(\"name\") or \"\").strip()\n",
    "    meeting_person_title = (inp.get(\"meeting_person\", {}).get(\"title\") or \"\").strip()\n",
    "\n",
    "entity, _ = load_latest_json(\"entity_*.json\")\n",
    "startup_name = (entity.get(\"canonical_name\") or \"Unknown Startup\").strip()\n",
    "official_website = (entity.get(\"official_website\") or \"\").strip()\n",
    "if not official_website:\n",
    "    raise ValueError(\"official_website is missing in entity. Run cell #2.\")\n",
    "\n",
    "# People directory (from cell #4), optional but recommended\n",
    "people_df_path = sorted(ART_DIR.glob(\"people_directory_*.csv\"), key=lambda p: p.stat().st_mtime, reverse=True)\n",
    "df_people = pd.read_csv(people_df_path[0]) if people_df_path else pd.DataFrame()\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Knobs\n",
    "# ------------------------------------------------------------\n",
    "USER_AGENT = \"researchOSv2-bot/0.3 (+contact: internal-research)\"\n",
    "TIMEOUT = 25\n",
    "SLEEP_SEC = 0.25\n",
    "\n",
    "MAX_PERSONS = 6                # meeting person + top executives\n",
    "MAX_CSE_LINKS_PER_PERSON = 18\n",
    "MAX_TEXT_CHARS_STORE = 60000\n",
    "MAX_TEXT_CHARS_PROMPT = 8500\n",
    "MAX_PAGES_TO_LLM = 18\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# URL / domain / robots helpers (same as previous cells)\n",
    "# ------------------------------------------------------------\n",
    "def normalize_url(url: str) -> str:\n",
    "    url = (url or \"\").strip()\n",
    "    if not url:\n",
    "        return \"\"\n",
    "    if not re.match(r\"^https?://\", url, flags=re.I):\n",
    "        url = \"https://\" + url\n",
    "    p = urlparse(url)\n",
    "    return p._replace(netloc=p.netloc.lower(), fragment=\"\").geturl()\n",
    "\n",
    "def get_domain(url: str) -> str:\n",
    "    return re.sub(r\"^www\\.\", \"\", urlparse(normalize_url(url)).netloc)\n",
    "\n",
    "def is_asset_url(url: str) -> bool:\n",
    "    path = (urlparse(url).path or \"\").lower()\n",
    "    return bool(re.search(r\"\\.(pdf|jpg|jpeg|png|gif|svg|zip|mp4|mov)$\", path))\n",
    "\n",
    "def build_robot_parser(base_url: str) -> RobotFileParser | None:\n",
    "    rp = RobotFileParser()\n",
    "    rp.set_url(urljoin(normalize_url(base_url), \"/robots.txt\"))\n",
    "    try:\n",
    "        rp.read()\n",
    "        return rp\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "def robots_allows(rp: RobotFileParser | None, url: str) -> bool:\n",
    "    if rp is None:\n",
    "        # conservative fallback\n",
    "        p = (urlparse(url).path or \"/\").lower()\n",
    "        return p in [\"/\", \"\"] or any(k in p for k in [\"about\", \"company\", \"team\", \"people\", \"blog\", \"news\", \"press\"])\n",
    "    try:\n",
    "        return rp.can_fetch(USER_AGENT, url)\n",
    "    except Exception:\n",
    "        return False\n",
    "\n",
    "# robots cache (3rd-party)\n",
    "_robot_cache: dict[str, RobotFileParser | None] = {}\n",
    "\n",
    "def get_rp_for_url(url: str) -> RobotFileParser | None:\n",
    "    dom = get_domain(url)\n",
    "    if dom in _robot_cache:\n",
    "        return _robot_cache[dom]\n",
    "    rp = build_robot_parser(url)\n",
    "    _robot_cache[dom] = rp\n",
    "    return rp\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Fetch + text extraction\n",
    "# ------------------------------------------------------------\n",
    "def fetch_html(url: str) -> tuple[int, str]:\n",
    "    r = requests.get(url, headers={\"User-Agent\": USER_AGENT}, timeout=TIMEOUT, allow_redirects=True)\n",
    "    return r.status_code, (r.text if r.ok else \"\")\n",
    "\n",
    "def html_to_text(html: str) -> str:\n",
    "    soup = BeautifulSoup(html, \"lxml\")\n",
    "    for tag in soup([\"script\", \"style\", \"noscript\", \"svg\"]):\n",
    "        tag.decompose()\n",
    "    node = soup.find(\"main\") or soup.find(\"article\") or (soup.body if soup.body else soup)\n",
    "    text = node.get_text(\"\\n\", strip=True)\n",
    "    text = re.sub(r\"\\n{3,}\", \"\\n\\n\", text)\n",
    "    return text.strip()\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Google CSE\n",
    "# ------------------------------------------------------------\n",
    "def google_cse_search(query: str, num: int = 5) -> list[dict]:\n",
    "    url = \"https://www.googleapis.com/customsearch/v1\"\n",
    "    params = {\"key\": GOOGLE_API_KEY, \"cx\": GOOGLE_CSE_CX, \"q\": query, \"num\": min(max(num, 1), 10)}\n",
    "    r = requests.get(url, params=params, timeout=30)\n",
    "    r.raise_for_status()\n",
    "    data = r.json()\n",
    "    items = data.get(\"items\", []) or []\n",
    "    out = []\n",
    "    for it in items:\n",
    "        out.append({\n",
    "            \"title\": it.get(\"title\"),\n",
    "            \"link\": normalize_url(it.get(\"link\") or \"\"),\n",
    "            \"snippet\": it.get(\"snippet\"),\n",
    "            \"displayLink\": it.get(\"displayLink\"),\n",
    "        })\n",
    "    return out\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Select target people (meeting person first, then top members)\n",
    "# ------------------------------------------------------------\n",
    "targets = []\n",
    "\n",
    "if meeting_person_name:\n",
    "    targets.append({\"person_id\": \"meeting_person\", \"full_name\": meeting_person_name, \"role_title\": meeting_person_title or None})\n",
    "\n",
    "# Add a few key executives from df_people (if available)\n",
    "if not df_people.empty:\n",
    "    # prefer founders/executives/leadership, higher confidence if present\n",
    "    def _cat_rank(c):\n",
    "        return {\"founder\": 0, \"executive\": 1, \"leadership\": 2, \"board\": 3}.get(str(c), 9)\n",
    "\n",
    "    # if df has confidence column, use it\n",
    "    if \"confidence\" in df_people.columns:\n",
    "        ranked = df_people.copy()\n",
    "        ranked[\"cat_rank\"] = ranked[\"category\"].apply(_cat_rank) if \"category\" in ranked.columns else 9\n",
    "        ranked[\"conf\"] = pd.to_numeric(ranked[\"confidence\"], errors=\"coerce\").fillna(0.0)\n",
    "        ranked = ranked.sort_values([\"cat_rank\", \"conf\"], ascending=[True, False])\n",
    "    else:\n",
    "        ranked = df_people\n",
    "\n",
    "    for _, r in ranked.head(MAX_PERSONS - len(targets)).iterrows():\n",
    "        targets.append({\n",
    "            \"person_id\": str(r.get(\"person_id\") or r.get(\"full_name\") or f\"person_{len(targets)}\"),\n",
    "            \"full_name\": str(r.get(\"full_name\") or \"\").strip(),\n",
    "            \"role_title\": (str(r.get(\"role_title\")) if pd.notna(r.get(\"role_title\")) else None),\n",
    "        })\n",
    "\n",
    "# Dedup by name\n",
    "seen_names = set()\n",
    "dedup = []\n",
    "for t in targets:\n",
    "    name = (t.get(\"full_name\") or \"\").strip().lower()\n",
    "    if name and name not in seen_names:\n",
    "        seen_names.add(name)\n",
    "        dedup.append(t)\n",
    "targets = dedup[:MAX_PERSONS]\n",
    "\n",
    "print(\"Target people:\")\n",
    "for t in targets:\n",
    "    print(\"-\", t[\"full_name\"], \"|\", t.get(\"role_title\"))\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Build CSE queries per person to find profiles, posts, interviews, articles\n",
    "# ------------------------------------------------------------\n",
    "def build_person_queries(startup: str, person: str) -> list[str]:\n",
    "    # bilingual-friendly; keep it practical\n",
    "    return [\n",
    "        f'\"{person}\" \"{startup}\"',\n",
    "        f'\"{person}\" \"{startup}\" interview OR インタビュー',\n",
    "        f'\"{person}\" \"{startup}\" podcast OR 登壇 OR talk OR keynote',\n",
    "        f'\"{person}\" \"{startup}\" LinkedIn',\n",
    "        f'\"{person}\" \"{startup}\" X OR Twitter',\n",
    "        f'\"{person}\" \"{startup}\" note.com OR Medium',\n",
    "        f'\"{person}\" \"{startup}\" press release OR プレスリリース',\n",
    "        f'\"{person}\" \"{startup}\" funding OR 資金調達',\n",
    "    ]\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Fetch + store raw pages for deep dive\n",
    "# ------------------------------------------------------------\n",
    "RAW_PATH = ART_DIR / f\"meeting_deep_dive_raw_pages_{run_id}.jsonl\"\n",
    "RAW_PATH.write_text(\"\", encoding=\"utf-8\")\n",
    "\n",
    "def append_jsonl(path: Path, obj: dict):\n",
    "    with path.open(\"a\", encoding=\"utf-8\") as f:\n",
    "        f.write(json.dumps(obj, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "def collect_url(url: str, source_type: str, person_name: str, query: str | None = None):\n",
    "    url = normalize_url(url)\n",
    "    if not url or is_asset_url(url):\n",
    "        return\n",
    "\n",
    "    rp = get_rp_for_url(url)\n",
    "    allowed = robots_allows(rp, url)\n",
    "\n",
    "    if not allowed:\n",
    "        append_jsonl(RAW_PATH, {\n",
    "            \"url\": url,\n",
    "            \"source_type\": source_type,   # cse\n",
    "            \"person_name\": person_name,\n",
    "            \"query\": query,\n",
    "            \"domain\": get_domain(url),\n",
    "            \"fetched_at_utc\": datetime.now(timezone.utc).isoformat(),\n",
    "            \"http_status\": None,\n",
    "            \"robots_allowed\": False,\n",
    "            \"text\": \"\",\n",
    "            \"notes\": \"Skipped due to robots.txt (or conservative fallback).\",\n",
    "        })\n",
    "        return\n",
    "\n",
    "    try:\n",
    "        code, html = fetch_html(url)\n",
    "        time.sleep(SLEEP_SEC)\n",
    "        text = html_to_text(html) if html else \"\"\n",
    "        text = text[:MAX_TEXT_CHARS_STORE]\n",
    "\n",
    "        append_jsonl(RAW_PATH, {\n",
    "            \"url\": url,\n",
    "            \"source_type\": source_type,\n",
    "            \"person_name\": person_name,\n",
    "            \"query\": query,\n",
    "            \"domain\": get_domain(url),\n",
    "            \"fetched_at_utc\": datetime.now(timezone.utc).isoformat(),\n",
    "            \"http_status\": code,\n",
    "            \"robots_allowed\": True,\n",
    "            \"text\": text,\n",
    "            \"text_char_len\": len(text),\n",
    "        })\n",
    "    except Exception as e:\n",
    "        append_jsonl(RAW_PATH, {\n",
    "            \"url\": url,\n",
    "            \"source_type\": source_type,\n",
    "            \"person_name\": person_name,\n",
    "            \"query\": query,\n",
    "            \"domain\": get_domain(url),\n",
    "            \"fetched_at_utc\": datetime.now(timezone.utc).isoformat(),\n",
    "            \"http_status\": None,\n",
    "            \"robots_allowed\": True,\n",
    "            \"text\": \"\",\n",
    "            \"notes\": f\"Fetch error: {str(e)}\",\n",
    "        })\n",
    "\n",
    "# Gather CSE targets\n",
    "all_links = []\n",
    "for t in targets:\n",
    "    person = t[\"full_name\"]\n",
    "    person_queries = build_person_queries(startup_name, person)\n",
    "    person_links = []\n",
    "\n",
    "    for q in person_queries:\n",
    "        try:\n",
    "            items = google_cse_search(q, num=5)\n",
    "            for it in items:\n",
    "                link = it.get(\"link\")\n",
    "                if link:\n",
    "                    person_links.append((link, q))\n",
    "            time.sleep(0.2)\n",
    "        except Exception:\n",
    "            continue\n",
    "\n",
    "    # dedup + cap per person\n",
    "    seen = set()\n",
    "    for link, q in person_links:\n",
    "        n = normalize_url(link)\n",
    "        if n and n not in seen:\n",
    "            seen.add(n)\n",
    "            all_links.append({\"url\": n, \"query\": q, \"person_name\": person})\n",
    "\n",
    "# Per-person cap\n",
    "per_person_count = {}\n",
    "filtered = []\n",
    "for x in all_links:\n",
    "    k = x[\"person_name\"]\n",
    "    per_person_count.setdefault(k, 0)\n",
    "    if per_person_count[k] < MAX_CSE_LINKS_PER_PERSON:\n",
    "        filtered.append(x)\n",
    "        per_person_count[k] += 1\n",
    "\n",
    "print(\"Fetching pages (robots-respecting)...\")\n",
    "for x in filtered:\n",
    "    collect_url(x[\"url\"], \"cse\", x[\"person_name\"], x[\"query\"])\n",
    "\n",
    "print(f\"✅ Deep dive raw pages saved: {RAW_PATH.as_posix()}\")\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Load raw pages + shortlist to LLM (prioritize meeting person)\n",
    "# ------------------------------------------------------------\n",
    "def load_raw_pages(path: Path) -> list[dict]:\n",
    "    rows = []\n",
    "    for line in path.read_text(encoding=\"utf-8\").splitlines():\n",
    "        if line.strip():\n",
    "            rows.append(json.loads(line))\n",
    "    return rows\n",
    "\n",
    "raw_pages = load_raw_pages(RAW_PATH)\n",
    "usable = [\n",
    "    r for r in raw_pages\n",
    "    if r.get(\"robots_allowed\")\n",
    "    and isinstance(r.get(\"http_status\"), int) and r[\"http_status\"] < 400\n",
    "    and (r.get(\"text\") or \"\").strip()\n",
    "]\n",
    "\n",
    "# prioritize meeting person pages in the prompt\n",
    "def _priority(r):\n",
    "    is_meeting = (meeting_person_name or \"\").strip().lower() == (r.get(\"person_name\") or \"\").strip().lower()\n",
    "    # prefer interview-ish URLs\n",
    "    p = (urlparse(r[\"url\"]).path or \"\").lower()\n",
    "    bonus = 0\n",
    "    for k in [\"interview\", \"podcast\", \"talk\", \"note\", \"blog\", \"press\", \"media\"]:\n",
    "        if k in p:\n",
    "            bonus += 1\n",
    "    return (1 if is_meeting else 0, bonus, -len(r.get(\"text\") or \"\"))\n",
    "\n",
    "def score_profile_like(url: str) -> int:\n",
    "    p = (urlparse(url).path or \"\").lower()\n",
    "    keys = [\"speaker\", \"speakers\", \"bio\", \"profile\", \"team\", \"people\", \"leadership\", \"about\", \"faculty\", \"news\"]\n",
    "    return sum(k in p for k in keys)\n",
    "\n",
    "usable_sorted = sorted(\n",
    "    usable,\n",
    "    key=lambda r: (score_profile_like(r[\"url\"]), (r.get(\"person_name\") or \"\").lower() == (meeting_person_name or \"\").lower(), len(r.get(\"text\") or \"\")),\n",
    "    reverse=True\n",
    ")\n",
    "shortlist = usable_sorted[:MAX_PAGES_TO_LLM]\n",
    "\n",
    "print(f\"Usable pages: {len(usable)} | Shortlist to LLM: {len(shortlist)}\")\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# OpenAI: Deep dive with \"strict + loose\" outputs (anti-empty)\n",
    "# ------------------------------------------------------------\n",
    "import json\n",
    "import re\n",
    "\n",
    "def _extract_json_object(text: str) -> str:\n",
    "    if not text:\n",
    "        return \"\"\n",
    "    t = text.strip()\n",
    "    fence = re.search(r\"```(?:json)?\\s*(\\{.*?\\})\\s*```\", t, flags=re.DOTALL | re.IGNORECASE)\n",
    "    if fence:\n",
    "        return fence.group(1).strip()\n",
    "    start, end = t.find(\"{\"), t.rfind(\"}\")\n",
    "    if start != -1 and end != -1 and end > start:\n",
    "        return t[start:end+1].strip()\n",
    "    return \"\"\n",
    "\n",
    "def openai_deep_dive_people_and_content(\n",
    "    startup_name: str,\n",
    "    official_website: str,\n",
    "    meeting_person_name: str,\n",
    "    targets: list[dict],\n",
    "    pages: list[dict]\n",
    ") -> dict:\n",
    "    system = (\n",
    "        \"You are a research analyst assistant. \"\n",
    "        \"Build (1) an evidence-linked content index and (2) best-effort people profiles \"\n",
    "        \"for the meeting participant and key company members, from the provided webpage text.\\n\\n\"\n",
    "        \"CRITICAL RULES:\\n\"\n",
    "        \"- Always prioritize producing a non-empty content_index if there is any relevant material.\\n\"\n",
    "        \"- For strict profiles, include only items clearly supported by evidence URLs.\\n\"\n",
    "        \"- If strict evidence is insufficient, still produce candidate profiles in people_profiles_loose \"\n",
    "        \"with lower confidence and clear rationale.\\n\"\n",
    "        \"- Do not invent education/employment. Only include what is in the provided texts.\\n\"\n",
    "        \"- Return a single JSON object only (no markdown, no commentary).\"\n",
    "    )\n",
    "\n",
    "    schema = {\n",
    "        \"people_profiles_strict\": [\n",
    "            {\n",
    "                \"person_id\": \"string\",\n",
    "                \"full_name\": \"string\",\n",
    "                \"role_title\": \"string|null\",\n",
    "                \"current_company\": \"string|null\",\n",
    "                \"bio_summary\": \"string|null\",\n",
    "                \"career_facts\": \"array of strings\",\n",
    "                \"public_presence_urls\": \"array of strings\",\n",
    "                \"confidence\": \"0..1\",\n",
    "                \"evidence_urls\": \"array of strings\"\n",
    "            }\n",
    "        ],\n",
    "        \"people_profiles_loose\": [\n",
    "            {\n",
    "                \"person_id\": \"string\",\n",
    "                \"full_name\": \"string\",\n",
    "                \"role_title\": \"string|null\",\n",
    "                \"candidate_summary\": \"string (best-effort, may be incomplete)\",\n",
    "                \"why_this_is_likely_the_same_person\": \"string\",\n",
    "                \"confidence\": \"0..1\",\n",
    "                \"supporting_urls\": \"array of strings\"\n",
    "            }\n",
    "        ],\n",
    "        \"content_index\": [\n",
    "            {\n",
    "                \"person_name\": \"string\",\n",
    "                \"content_type\": \"profile|interview|article|podcast|talk|press|post|other\",\n",
    "                \"title\": \"string|null\",\n",
    "                \"publisher_or_platform\": \"string|null\",\n",
    "                \"date\": \"string|null\",\n",
    "                \"url\": \"string\",\n",
    "                \"what_it_contains\": \"string (short)\",\n",
    "                \"notable_points\": \"array of strings\",\n",
    "                \"relevance\": \"0..1\",\n",
    "                \"confidence\": \"0..1\"\n",
    "            }\n",
    "        ],\n",
    "        \"meeting_person_insights\": {\n",
    "            \"person_name\": \"string|null\",\n",
    "            \"facts\": \"array of strings\",\n",
    "            \"hypotheses\": \"array of strings\",\n",
    "            \"open_questions\": \"array of strings\"\n",
    "        },\n",
    "        \"notes\": {\n",
    "            \"name_collisions\": \"array of strings\",\n",
    "            \"data_gaps\": \"array of strings\",\n",
    "            \"conflicts\": \"array of strings\"\n",
    "        }\n",
    "    }\n",
    "\n",
    "    bundle = {\n",
    "        \"startup_name\": startup_name,\n",
    "        \"official_website\": official_website,\n",
    "        \"meeting_person_name\": meeting_person_name or None,\n",
    "        \"target_people\": targets,\n",
    "        \"pages\": [\n",
    "            {\n",
    "                \"url\": p[\"url\"],\n",
    "                \"person_name\": p.get(\"person_name\"),\n",
    "                \"source_type\": p.get(\"source_type\"),\n",
    "                \"domain\": p.get(\"domain\"),\n",
    "                \"text\": (p.get(\"text\") or \"\")[:MAX_TEXT_CHARS_PROMPT],\n",
    "            } for p in pages\n",
    "        ],\n",
    "        \"schema\": schema,\n",
    "        \"extraction_priorities\": [\n",
    "            \"First, produce content_index rows for each target person (at least 1 if possible).\",\n",
    "            \"Then, attempt people_profiles_strict if the text contains biography-like facts.\",\n",
    "            \"If strict is empty, populate people_profiles_loose using best-effort matching and supporting_urls.\"\n",
    "        ]\n",
    "    }\n",
    "\n",
    "    # Prefer strict JSON mode if supported\n",
    "    try:\n",
    "        resp = client.chat.completions.create(\n",
    "            model=\"gpt-4o-mini\",\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": system},\n",
    "                {\"role\": \"user\", \"content\": json.dumps(bundle, ensure_ascii=False)},\n",
    "            ],\n",
    "            temperature=0.0,\n",
    "            response_format={\"type\": \"json_object\"},\n",
    "        )\n",
    "        return json.loads((resp.choices[0].message.content or \"\").strip())\n",
    "    except Exception:\n",
    "        resp2 = client.chat.completions.create(\n",
    "            model=\"gpt-4o-mini\",\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": system},\n",
    "                {\"role\": \"user\", \"content\": json.dumps(bundle, ensure_ascii=False)},\n",
    "            ],\n",
    "            temperature=0.0,\n",
    "        )\n",
    "        raw = resp2.choices[0].message.content or \"\"\n",
    "        candidate = _extract_json_object(raw)\n",
    "        if candidate:\n",
    "            return json.loads(candidate)\n",
    "        raise RuntimeError(\"Model did not return a JSON object.\")\n",
    "\n",
    "\n",
    "deep_dive = openai_deep_dive_people_and_content(\n",
    "    startup_name=startup_name,\n",
    "    official_website=official_website,\n",
    "    meeting_person_name=meeting_person_name,\n",
    "    targets=targets,\n",
    "    pages=shortlist\n",
    ")\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Save JSON + DataFrames\n",
    "# ------------------------------------------------------------\n",
    "JSON_PATH = ART_DIR / f\"meeting_person_deep_dive_{run_id}.json\"\n",
    "JSON_PATH.write_text(json.dumps(deep_dive, ensure_ascii=False, indent=2), encoding=\"utf-8\")\n",
    "\n",
    "profiles = deep_dive.get(\"people_profiles\", []) or []\n",
    "content_index = deep_dive.get(\"content_index\", []) or []\n",
    "\n",
    "df_profiles = pd.DataFrame(profiles)\n",
    "df_content = pd.DataFrame(content_index)\n",
    "\n",
    "# Light post-processing for CSV friendliness\n",
    "if not df_profiles.empty:\n",
    "    df_profiles.insert(0, \"run_id\", run_id)\n",
    "    df_profiles.insert(1, \"startup_name\", startup_name)\n",
    "\n",
    "if not df_content.empty:\n",
    "    df_content.insert(0, \"run_id\", run_id)\n",
    "    df_content.insert(1, \"startup_name\", startup_name)\n",
    "\n",
    "PROFILES_CSV = ART_DIR / f\"people_profiles_{run_id}.csv\"\n",
    "CONTENT_CSV = ART_DIR / f\"people_content_index_{run_id}.csv\"\n",
    "\n",
    "df_profiles.to_csv(PROFILES_CSV, index=False)\n",
    "df_content.to_csv(CONTENT_CSV, index=False)\n",
    "\n",
    "print(\"✅ Meeting Person Deep Dive complete\")\n",
    "print(f\"- Raw pages: {RAW_PATH.as_posix()}\")\n",
    "print(f\"- JSON: {JSON_PATH.as_posix()}\")\n",
    "print(f\"- Profiles CSV: {PROFILES_CSV.as_posix()}\")\n",
    "print(f\"- Content CSV: {CONTENT_CSV.as_posix()}\")\n",
    "\n",
    "display(df_profiles)\n",
    "display(df_content.head(30) if not df_content.empty else df_content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "27441bf1-8578-477b-ab77-7d228605853a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Official product targets: 17\n",
      "CSE product targets: 30\n",
      "✅ Business/Product raw pages saved: artifacts/meeting_deep_dive/business_product_raw_pages_20260107_225628.jsonl\n",
      "Usable pages: 25 | Shortlist to LLM: 20\n",
      "✅ Business & Product Understanding complete\n",
      "- Raw pages: artifacts/meeting_deep_dive/business_product_raw_pages_20260107_225628.jsonl\n",
      "- JSON: artifacts/meeting_deep_dive/business_product_extraction_20260107_225628.json\n",
      "- Summary CSV: artifacts/meeting_deep_dive/business_product_summary_20260107_225628.csv\n",
      "- Claims CSV: artifacts/meeting_deep_dive/business_product_claims_20260107_225628.csv\n",
      "- Content CSV: artifacts/meeting_deep_dive/business_product_content_index_20260107_225628.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>run_id</th>\n",
       "      <th>company_name</th>\n",
       "      <th>one_liner</th>\n",
       "      <th>product_overview</th>\n",
       "      <th>how_it_works_as_described</th>\n",
       "      <th>key_differentiators</th>\n",
       "      <th>primary_use_cases</th>\n",
       "      <th>target_customers</th>\n",
       "      <th>pricing_packaging_signals</th>\n",
       "      <th>positioning_tags</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20260107_225628</td>\n",
       "      <td>Sakana AI</td>\n",
       "      <td>A Tokyo-based AI startup focused on developing...</td>\n",
       "      <td>Sakana AI develops advanced AI systems that le...</td>\n",
       "      <td>[Utilizes evolutionary algorithms to merge exi...</td>\n",
       "      <td>[Focus on nature-inspired AI development., Inn...</td>\n",
       "      <td>[Automating scientific research and discovery....</td>\n",
       "      <td>[Financial institutions., Government agencies....</td>\n",
       "      <td>[Cost-efficient AI model generation at approxi...</td>\n",
       "      <td>[Nature-inspired AI, Generative AI, AI for sci...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            run_id company_name  \\\n",
       "0  20260107_225628    Sakana AI   \n",
       "\n",
       "                                           one_liner  \\\n",
       "0  A Tokyo-based AI startup focused on developing...   \n",
       "\n",
       "                                    product_overview  \\\n",
       "0  Sakana AI develops advanced AI systems that le...   \n",
       "\n",
       "                           how_it_works_as_described  \\\n",
       "0  [Utilizes evolutionary algorithms to merge exi...   \n",
       "\n",
       "                                 key_differentiators  \\\n",
       "0  [Focus on nature-inspired AI development., Inn...   \n",
       "\n",
       "                                   primary_use_cases  \\\n",
       "0  [Automating scientific research and discovery....   \n",
       "\n",
       "                                    target_customers  \\\n",
       "0  [Financial institutions., Government agencies....   \n",
       "\n",
       "                           pricing_packaging_signals  \\\n",
       "0  [Cost-efficient AI model generation at approxi...   \n",
       "\n",
       "                                    positioning_tags  \n",
       "0  [Nature-inspired AI, Generative AI, AI for sci...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>run_id</th>\n",
       "      <th>category</th>\n",
       "      <th>claim_type</th>\n",
       "      <th>claim</th>\n",
       "      <th>evidence_url</th>\n",
       "      <th>source_type</th>\n",
       "      <th>confidence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20260107_225628</td>\n",
       "      <td>product</td>\n",
       "      <td>fact</td>\n",
       "      <td>Sakana AI's AI Scientist can autonomously cond...</td>\n",
       "      <td>https://siliconangle.com/2024/08/13/sakana-ai-...</td>\n",
       "      <td>cse</td>\n",
       "      <td>0.90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20260107_225628</td>\n",
       "      <td>tech</td>\n",
       "      <td>fact</td>\n",
       "      <td>The AI Scientist has produced papers that pass...</td>\n",
       "      <td>https://sakana.ai/ai-scientist-first-publication/</td>\n",
       "      <td>cse</td>\n",
       "      <td>0.85</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20260107_225628</td>\n",
       "      <td>use_case</td>\n",
       "      <td>fact</td>\n",
       "      <td>Sakana AI's technology is being applied in the...</td>\n",
       "      <td>https://sakana.ai/mufg-bank/</td>\n",
       "      <td>cse</td>\n",
       "      <td>0.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>20260107_225628</td>\n",
       "      <td>customer</td>\n",
       "      <td>fact</td>\n",
       "      <td>Sakana AI has partnered with MUFG Bank to deve...</td>\n",
       "      <td>https://sakana.ai/mufg-bank/</td>\n",
       "      <td>cse</td>\n",
       "      <td>0.90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20260107_225628</td>\n",
       "      <td>other</td>\n",
       "      <td>fact</td>\n",
       "      <td>Sakana AI raised $135 million in a Series B fu...</td>\n",
       "      <td>https://siliconangle.com/2025/11/17/sakana-ai-...</td>\n",
       "      <td>cse</td>\n",
       "      <td>0.95</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            run_id  category claim_type  \\\n",
       "0  20260107_225628   product       fact   \n",
       "1  20260107_225628      tech       fact   \n",
       "2  20260107_225628  use_case       fact   \n",
       "3  20260107_225628  customer       fact   \n",
       "4  20260107_225628     other       fact   \n",
       "\n",
       "                                               claim  \\\n",
       "0  Sakana AI's AI Scientist can autonomously cond...   \n",
       "1  The AI Scientist has produced papers that pass...   \n",
       "2  Sakana AI's technology is being applied in the...   \n",
       "3  Sakana AI has partnered with MUFG Bank to deve...   \n",
       "4  Sakana AI raised $135 million in a Series B fu...   \n",
       "\n",
       "                                        evidence_url source_type  confidence  \n",
       "0  https://siliconangle.com/2024/08/13/sakana-ai-...         cse        0.90  \n",
       "1  https://sakana.ai/ai-scientist-first-publication/         cse        0.85  \n",
       "2                       https://sakana.ai/mufg-bank/         cse        0.80  \n",
       "3                       https://sakana.ai/mufg-bank/         cse        0.90  \n",
       "4  https://siliconangle.com/2025/11/17/sakana-ai-...         cse        0.95  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>run_id</th>\n",
       "      <th>content_type</th>\n",
       "      <th>title</th>\n",
       "      <th>publisher_or_platform</th>\n",
       "      <th>date</th>\n",
       "      <th>url</th>\n",
       "      <th>what_it_contains</th>\n",
       "      <th>relevance</th>\n",
       "      <th>confidence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20260107_225628</td>\n",
       "      <td>blog_post</td>\n",
       "      <td>Sakana AI creates an ‘AI Scientist’ to automat...</td>\n",
       "      <td>SiliconANGLE</td>\n",
       "      <td>August 13, 2024</td>\n",
       "      <td>https://siliconangle.com/2024/08/13/sakana-ai-...</td>\n",
       "      <td>Details on the AI Scientist's capabilities and...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20260107_225628</td>\n",
       "      <td>blog_post</td>\n",
       "      <td>The AI Scientist Generates its First Peer-Revi...</td>\n",
       "      <td>Sakana AI</td>\n",
       "      <td>March 12, 2025</td>\n",
       "      <td>https://sakana.ai/ai-scientist-first-publication/</td>\n",
       "      <td>Announcement of the AI Scientist's first peer-...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.85</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20260107_225628</td>\n",
       "      <td>press</td>\n",
       "      <td>Announcing a Multiyear Partnership between Sak...</td>\n",
       "      <td>Sakana AI</td>\n",
       "      <td>May 19, 2025</td>\n",
       "      <td>https://sakana.ai/mufg-bank/</td>\n",
       "      <td>Details on the partnership with MUFG Bank to d...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>20260107_225628</td>\n",
       "      <td>article</td>\n",
       "      <td>Sakana AI lands $135M on $2.635B valuation to ...</td>\n",
       "      <td>SiliconANGLE</td>\n",
       "      <td>November 17, 2025</td>\n",
       "      <td>https://siliconangle.com/2025/11/17/sakana-ai-...</td>\n",
       "      <td>Information about the funding round and its im...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20260107_225628</td>\n",
       "      <td>blog_post</td>\n",
       "      <td>Sakana AI Agent Wins AtCoder Heuristic Contest...</td>\n",
       "      <td>Sakana AI</td>\n",
       "      <td>January 05, 2026</td>\n",
       "      <td>https://sakana.ai/ahc058/</td>\n",
       "      <td>Details on the AI agent's victory in a competi...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.90</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            run_id content_type  \\\n",
       "0  20260107_225628    blog_post   \n",
       "1  20260107_225628    blog_post   \n",
       "2  20260107_225628        press   \n",
       "3  20260107_225628      article   \n",
       "4  20260107_225628    blog_post   \n",
       "\n",
       "                                               title publisher_or_platform  \\\n",
       "0  Sakana AI creates an ‘AI Scientist’ to automat...          SiliconANGLE   \n",
       "1  The AI Scientist Generates its First Peer-Revi...             Sakana AI   \n",
       "2  Announcing a Multiyear Partnership between Sak...             Sakana AI   \n",
       "3  Sakana AI lands $135M on $2.635B valuation to ...          SiliconANGLE   \n",
       "4  Sakana AI Agent Wins AtCoder Heuristic Contest...             Sakana AI   \n",
       "\n",
       "                date                                                url  \\\n",
       "0    August 13, 2024  https://siliconangle.com/2024/08/13/sakana-ai-...   \n",
       "1     March 12, 2025  https://sakana.ai/ai-scientist-first-publication/   \n",
       "2       May 19, 2025                       https://sakana.ai/mufg-bank/   \n",
       "3  November 17, 2025  https://siliconangle.com/2025/11/17/sakana-ai-...   \n",
       "4   January 05, 2026                          https://sakana.ai/ahc058/   \n",
       "\n",
       "                                    what_it_contains  relevance  confidence  \n",
       "0  Details on the AI Scientist's capabilities and...          1        0.90  \n",
       "1  Announcement of the AI Scientist's first peer-...          1        0.85  \n",
       "2  Details on the partnership with MUFG Bank to d...          1        0.90  \n",
       "3  Information about the funding round and its im...          1        0.95  \n",
       "4  Details on the AI agent's victory in a competi...          1        0.90  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ============================================================\n",
    "# 6. Business & Product Understanding (Facts-first + Content Index)\n",
    "# ============================================================\n",
    "# Goal:\n",
    "# - Build an evidence-linked understanding of the startup's business & product:\n",
    "#   - What the product is, how it works (as described), key differentiators\n",
    "#   - Use cases, customer segments, pricing/packaging signals (if any)\n",
    "#   - Public narratives: blog posts, interviews, articles, press releases\n",
    "# - Use the same pattern as prior cells:\n",
    "#   official pages first + CSE enrichment, robots check,\n",
    "#   store raw pages as JSONL, then OpenAI normalizes into:\n",
    "#     1) business_product_summary (single object)\n",
    "#     2) product_claims (long-form table with evidence_url + confidence)\n",
    "#     3) product_content_index (content list: posts/interviews/articles/press)\n",
    "#\n",
    "# Outputs:\n",
    "# - business_product_raw_pages_<run_id>.jsonl\n",
    "# - business_product_extraction_<run_id>.json\n",
    "# - business_product_summary_<run_id>.csv\n",
    "# - business_product_claims_<run_id>.csv\n",
    "# - business_product_content_index_<run_id>.csv\n",
    "#\n",
    "# Notes:\n",
    "# - Conservative: only include items supported by evidence URLs in fetched pages.\n",
    "# - If official pages are JS-rendered and thin, rely more on CSE articles and press.\n",
    "\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "import time\n",
    "import requests\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "from pathlib import Path\n",
    "from urllib.parse import urlparse, urljoin\n",
    "from datetime import datetime, timezone\n",
    "from urllib.robotparser import RobotFileParser\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Global configuration\n",
    "# ------------------------------------------------------------\n",
    "load_dotenv(\"env.txt\")\n",
    "\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "if not OPENAI_API_KEY:\n",
    "    raise EnvironmentError(\"OPENAI_API_KEY is not set.\")\n",
    "\n",
    "GOOGLE_API_KEY = os.getenv(\"GOOGLE_API_KEY\")\n",
    "GOOGLE_CSE_CX = os.getenv(\"GOOGLE_CSE_CX\")\n",
    "if not GOOGLE_API_KEY or not GOOGLE_CSE_CX:\n",
    "    raise EnvironmentError(\"GOOGLE_API_KEY and GOOGLE_CSE_CX must be set.\")\n",
    "\n",
    "client = OpenAI(api_key=OPENAI_API_KEY)\n",
    "\n",
    "ART_DIR = Path(\"artifacts\") / \"meeting_deep_dive\"\n",
    "ART_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Load inputs + entity (from prior cells)\n",
    "# ------------------------------------------------------------\n",
    "def load_latest_json(pattern: str) -> tuple[dict, Path]:\n",
    "    files = sorted(ART_DIR.glob(pattern), key=lambda p: p.stat().st_mtime, reverse=True)\n",
    "    if not files:\n",
    "        raise FileNotFoundError(f\"No {pattern} found. Please run previous cells.\")\n",
    "    p = files[0]\n",
    "    return json.loads(p.read_text(encoding=\"utf-8\")), p\n",
    "\n",
    "try:\n",
    "    run_id = inputs[\"meta\"][\"run_id\"]\n",
    "except Exception:\n",
    "    inp, _ = load_latest_json(\"inputs_*.json\")\n",
    "    run_id = inp.get(\"meta\", {}).get(\"run_id\") or datetime.now(timezone.utc).strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "entity, _ = load_latest_json(\"entity_*.json\")\n",
    "startup_name = (entity.get(\"canonical_name\") or \"Unknown Startup\").strip()\n",
    "official_website = (entity.get(\"official_website\") or \"\").strip()\n",
    "if not official_website:\n",
    "    raise ValueError(\"official_website is missing. Run cell #2 first.\")\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Knobs\n",
    "# ------------------------------------------------------------\n",
    "USER_AGENT = \"researchOSv2-bot/0.3 (+contact: internal-research)\"\n",
    "TIMEOUT = 25\n",
    "SLEEP_SEC = 0.25\n",
    "\n",
    "MAX_OFFICIAL_PAGES = 35\n",
    "MAX_CSE_PAGES = 30\n",
    "MAX_TEXT_CHARS_STORE = 70000\n",
    "MAX_TEXT_CHARS_PROMPT = 9000\n",
    "MAX_PAGES_TO_LLM = 20\n",
    "\n",
    "PRODUCT_KEY_PATHS = [\n",
    "    \"/product\", \"/products\", \"/service\", \"/services\", \"/solutions\",\n",
    "    \"/research\", \"/blog\", \"/news\", \"/press\", \"/updates\",\n",
    "    \"/customers\", \"/case\", \"/cases\", \"/case-studies\", \"/usecase\", \"/use-cases\",\n",
    "    \"/pricing\", \"/docs\"\n",
    "]\n",
    "\n",
    "PRODUCT_KEYWORDS = [\n",
    "    \"product\", \"products\", \"service\", \"services\", \"solutions\",\n",
    "    \"usecase\", \"use-case\", \"usecases\", \"case-study\", \"case\", \"customers\",\n",
    "    \"pricing\", \"docs\", \"api\", \"research\", \"blog\", \"press\", \"news\"\n",
    "]\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# URL / domain / robots helpers\n",
    "# ------------------------------------------------------------\n",
    "def normalize_url(url: str) -> str:\n",
    "    url = (url or \"\").strip()\n",
    "    if not url:\n",
    "        return \"\"\n",
    "    if not re.match(r\"^https?://\", url, flags=re.I):\n",
    "        url = \"https://\" + url\n",
    "    p = urlparse(url)\n",
    "    return p._replace(netloc=p.netloc.lower(), fragment=\"\").geturl()\n",
    "\n",
    "def get_domain(url: str) -> str:\n",
    "    return re.sub(r\"^www\\.\", \"\", urlparse(normalize_url(url)).netloc)\n",
    "\n",
    "def same_domain(a: str, b: str) -> bool:\n",
    "    da, db = get_domain(a), get_domain(b)\n",
    "    return da == db or da.endswith(\".\" + db) or db.endswith(\".\" + da)\n",
    "\n",
    "def is_asset_url(url: str) -> bool:\n",
    "    path = (urlparse(url).path or \"\").lower()\n",
    "    return bool(re.search(r\"\\.(pdf|jpg|jpeg|png|gif|svg|zip|mp4|mov)$\", path))\n",
    "\n",
    "def build_robot_parser(base_url: str) -> RobotFileParser | None:\n",
    "    rp = RobotFileParser()\n",
    "    rp.set_url(urljoin(normalize_url(base_url), \"/robots.txt\"))\n",
    "    try:\n",
    "        rp.read()\n",
    "        return rp\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "def robots_allows(rp: RobotFileParser | None, url: str) -> bool:\n",
    "    if rp is None:\n",
    "        p = (urlparse(url).path or \"/\").lower()\n",
    "        return p in [\"/\", \"\"] or any(k in p for k in PRODUCT_KEYWORDS)\n",
    "    try:\n",
    "        return rp.can_fetch(USER_AGENT, url)\n",
    "    except Exception:\n",
    "        return False\n",
    "\n",
    "# robots cache for 3rd-party\n",
    "_robot_cache: dict[str, RobotFileParser | None] = {}\n",
    "\n",
    "def get_rp_for_url(url: str) -> RobotFileParser | None:\n",
    "    dom = get_domain(url)\n",
    "    if dom in _robot_cache:\n",
    "        return _robot_cache[dom]\n",
    "    rp = build_robot_parser(url)\n",
    "    _robot_cache[dom] = rp\n",
    "    return rp\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Fetch + text extraction\n",
    "# ------------------------------------------------------------\n",
    "def fetch_html(url: str) -> tuple[int, str]:\n",
    "    r = requests.get(url, headers={\"User-Agent\": USER_AGENT}, timeout=TIMEOUT, allow_redirects=True)\n",
    "    return r.status_code, (r.text if r.ok else \"\")\n",
    "\n",
    "def html_to_text(html: str) -> str:\n",
    "    soup = BeautifulSoup(html, \"lxml\")\n",
    "    for tag in soup([\"script\", \"style\", \"noscript\", \"svg\"]):\n",
    "        tag.decompose()\n",
    "    node = soup.find(\"main\") or soup.find(\"article\") or (soup.body if soup.body else soup)\n",
    "    text = node.get_text(\"\\n\", strip=True)\n",
    "    text = re.sub(r\"\\n{3,}\", \"\\n\\n\", text)\n",
    "    return text.strip()\n",
    "\n",
    "def extract_links(base_url: str, html: str) -> list[str]:\n",
    "    soup = BeautifulSoup(html, \"lxml\")\n",
    "    out = []\n",
    "    for a in soup.find_all(\"a\", href=True):\n",
    "        href = a[\"href\"].strip()\n",
    "        if href.startswith(\"#\") or href.lower().startswith(\"mailto:\") or href.lower().startswith(\"tel:\"):\n",
    "            continue\n",
    "        u = normalize_url(urljoin(base_url, href))\n",
    "        if u.startswith((\"http://\", \"https://\")):\n",
    "            out.append(u)\n",
    "    # unique preserve order\n",
    "    seen, uniq = set(), []\n",
    "    for u in out:\n",
    "        if u not in seen:\n",
    "            seen.add(u)\n",
    "            uniq.append(u)\n",
    "    return uniq\n",
    "\n",
    "def score_product_link(url: str) -> float:\n",
    "    p = (urlparse(url).path or \"/\").lower()\n",
    "    score = 0.0\n",
    "    if p in [\"/product\", \"/products\", \"/service\", \"/services\", \"/solutions\"]:\n",
    "        score += 3.0\n",
    "    for k in PRODUCT_KEYWORDS:\n",
    "        if k in p:\n",
    "            score += 1.0\n",
    "    if is_asset_url(url):\n",
    "        score -= 4.0\n",
    "    if p.count(\"/\") >= 4:\n",
    "        score -= 0.4\n",
    "    return score\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Google CSE\n",
    "# ------------------------------------------------------------\n",
    "def google_cse_search(query: str, num: int = 5) -> list[dict]:\n",
    "    url = \"https://www.googleapis.com/customsearch/v1\"\n",
    "    params = {\"key\": GOOGLE_API_KEY, \"cx\": GOOGLE_CSE_CX, \"q\": query, \"num\": min(max(num, 1), 10)}\n",
    "    r = requests.get(url, params=params, timeout=30)\n",
    "    r.raise_for_status()\n",
    "    data = r.json()\n",
    "    items = data.get(\"items\", []) or []\n",
    "    return [{\n",
    "        \"title\": it.get(\"title\"),\n",
    "        \"link\": normalize_url(it.get(\"link\") or \"\"),\n",
    "        \"snippet\": it.get(\"snippet\"),\n",
    "        \"displayLink\": it.get(\"displayLink\"),\n",
    "    } for it in items]\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Build official targets: homepage + explicit paths + internal links\n",
    "# ------------------------------------------------------------\n",
    "official_home = normalize_url(official_website)\n",
    "rp_official = build_robot_parser(official_home)\n",
    "\n",
    "status, html = fetch_html(official_home)\n",
    "time.sleep(SLEEP_SEC)\n",
    "if status >= 400 or not html:\n",
    "    raise RuntimeError(f\"Failed to fetch official homepage: HTTP {status}\")\n",
    "\n",
    "home_links = extract_links(official_home, html)\n",
    "internal_links = [l for l in home_links if same_domain(l, official_home) and not is_asset_url(l)]\n",
    "\n",
    "explicit = []\n",
    "for p in PRODUCT_KEY_PATHS:\n",
    "    u = normalize_url(urljoin(official_home, p))\n",
    "    if robots_allows(rp_official, u) and not is_asset_url(u):\n",
    "        explicit.append(u)\n",
    "\n",
    "candidates = list(dict.fromkeys([official_home] + explicit + internal_links))\n",
    "scored = [{\"url\": u, \"score\": score_product_link(u)} for u in candidates]\n",
    "scored.sort(key=lambda x: x[\"score\"], reverse=True)\n",
    "\n",
    "official_targets = []\n",
    "for it in scored:\n",
    "    if len(official_targets) >= MAX_OFFICIAL_PAGES:\n",
    "        break\n",
    "    if it[\"score\"] < 0.2:\n",
    "        continue\n",
    "    if robots_allows(rp_official, it[\"url\"]):\n",
    "        official_targets.append(it[\"url\"])\n",
    "\n",
    "official_targets = list(dict.fromkeys(official_targets))\n",
    "print(f\"Official product targets: {len(official_targets)}\")\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Build CSE targets (business/product focused, bilingual-friendly)\n",
    "# ------------------------------------------------------------\n",
    "official_domain = get_domain(official_home)\n",
    "queries = [\n",
    "    f'\"{startup_name}\" {official_domain}',\n",
    "    f'\"{startup_name}\" product OR technology OR 技術',\n",
    "    f'\"{startup_name}\" use case OR 導入事例 OR 活用事例',\n",
    "    f'\"{startup_name}\" customers OR 顧客 OR 導入',\n",
    "    f'\"{startup_name}\" pricing OR 料金',\n",
    "    f'\"{startup_name}\" API OR docs OR documentation',\n",
    "    f'\"{startup_name}\" blog OR research OR paper OR 研究',\n",
    "    f'\"{startup_name}\" interview OR インタビュー product',\n",
    "    f'\"{startup_name}\" press release OR プレスリリース',\n",
    "    f'\"{startup_name}\" partnership OR 提携',\n",
    "]\n",
    "cse_items = []\n",
    "for q in queries:\n",
    "    try:\n",
    "        cse_items.extend(google_cse_search(q, num=5))\n",
    "        time.sleep(0.2)\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "seen = set()\n",
    "cse_links = []\n",
    "for it in cse_items:\n",
    "    link = it.get(\"link\")\n",
    "    if link and link not in seen and link.startswith((\"http://\", \"https://\")) and not is_asset_url(link):\n",
    "        seen.add(link)\n",
    "        cse_links.append(link)\n",
    "cse_links = cse_links[:MAX_CSE_PAGES]\n",
    "print(f\"CSE product targets: {len(cse_links)}\")\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Fetch + store raw pages (JSONL)\n",
    "# ------------------------------------------------------------\n",
    "RAW_PATH = ART_DIR / f\"business_product_raw_pages_{run_id}.jsonl\"\n",
    "RAW_PATH.write_text(\"\", encoding=\"utf-8\")\n",
    "\n",
    "def append_jsonl(path: Path, obj: dict):\n",
    "    with path.open(\"a\", encoding=\"utf-8\") as f:\n",
    "        f.write(json.dumps(obj, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "def collect_pages(urls: list[str], source_type: str):\n",
    "    for u in urls:\n",
    "        u = normalize_url(u)\n",
    "        if not u or is_asset_url(u):\n",
    "            continue\n",
    "\n",
    "        rp = rp_official if source_type == \"official\" else get_rp_for_url(u)\n",
    "        allowed = robots_allows(rp, u)\n",
    "\n",
    "        if not allowed:\n",
    "            append_jsonl(RAW_PATH, {\n",
    "                \"url\": u,\n",
    "                \"source_type\": source_type,\n",
    "                \"domain\": get_domain(u),\n",
    "                \"fetched_at_utc\": datetime.now(timezone.utc).isoformat(),\n",
    "                \"http_status\": None,\n",
    "                \"robots_allowed\": False,\n",
    "                \"text\": \"\",\n",
    "                \"notes\": \"Skipped due to robots.txt (or conservative fallback).\",\n",
    "            })\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            code, html = fetch_html(u)\n",
    "            time.sleep(SLEEP_SEC)\n",
    "            text = html_to_text(html) if html else \"\"\n",
    "            text = text[:MAX_TEXT_CHARS_STORE]\n",
    "\n",
    "            append_jsonl(RAW_PATH, {\n",
    "                \"url\": u,\n",
    "                \"source_type\": source_type,\n",
    "                \"domain\": get_domain(u),\n",
    "                \"fetched_at_utc\": datetime.now(timezone.utc).isoformat(),\n",
    "                \"http_status\": code,\n",
    "                \"robots_allowed\": True,\n",
    "                \"text\": text,\n",
    "                \"text_char_len\": len(text),\n",
    "            })\n",
    "        except Exception as e:\n",
    "            append_jsonl(RAW_PATH, {\n",
    "                \"url\": u,\n",
    "                \"source_type\": source_type,\n",
    "                \"domain\": get_domain(u),\n",
    "                \"fetched_at_utc\": datetime.now(timezone.utc).isoformat(),\n",
    "                \"http_status\": None,\n",
    "                \"robots_allowed\": True,\n",
    "                \"text\": \"\",\n",
    "                \"notes\": f\"Fetch error: {str(e)}\",\n",
    "            })\n",
    "\n",
    "collect_pages(official_targets, \"official\")\n",
    "collect_pages(cse_links, \"cse\")\n",
    "\n",
    "print(f\"✅ Business/Product raw pages saved: {RAW_PATH.as_posix()}\")\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Load raw pages + shortlist to LLM (product-heavy URLs first)\n",
    "# ------------------------------------------------------------\n",
    "def load_raw_pages(path: Path) -> list[dict]:\n",
    "    rows = []\n",
    "    for line in path.read_text(encoding=\"utf-8\").splitlines():\n",
    "        if line.strip():\n",
    "            rows.append(json.loads(line))\n",
    "    return rows\n",
    "\n",
    "raw_pages = load_raw_pages(RAW_PATH)\n",
    "\n",
    "usable = [\n",
    "    r for r in raw_pages\n",
    "    if r.get(\"robots_allowed\")\n",
    "    and isinstance(r.get(\"http_status\"), int) and r[\"http_status\"] < 400\n",
    "    and (r.get(\"text\") or \"\").strip()\n",
    "]\n",
    "\n",
    "def score_page_for_product(r):\n",
    "    url = r.get(\"url\") or \"\"\n",
    "    text = r.get(\"text\") or \"\"\n",
    "    p = (urlparse(url).path or \"\").lower()\n",
    "    s = 0\n",
    "    for k in PRODUCT_KEYWORDS:\n",
    "        if k in p:\n",
    "            s += 2\n",
    "    s += min(len(text), 20000) / 6000\n",
    "    if r.get(\"source_type\") == \"official\":\n",
    "        s += 2\n",
    "    return s\n",
    "\n",
    "usable_sorted = sorted(usable, key=score_page_for_product, reverse=True)\n",
    "shortlist = usable_sorted[:MAX_PAGES_TO_LLM]\n",
    "print(f\"Usable pages: {len(usable)} | Shortlist to LLM: {len(shortlist)}\")\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Robust JSON parsing helper\n",
    "# ------------------------------------------------------------\n",
    "def _extract_json_object(text: str) -> str:\n",
    "    if not text:\n",
    "        return \"\"\n",
    "    t = text.strip()\n",
    "    m = re.search(r\"```(?:json)?\\s*(\\{.*?\\})\\s*```\", t, flags=re.DOTALL | re.IGNORECASE)\n",
    "    if m:\n",
    "        return m.group(1).strip()\n",
    "    start, end = t.find(\"{\"), t.rfind(\"}\")\n",
    "    if start != -1 and end != -1 and end > start:\n",
    "        return t[start:end+1].strip()\n",
    "    return \"\"\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# OpenAI: business/product summary + claims + content index (anti-empty)\n",
    "# ------------------------------------------------------------\n",
    "def openai_extract_business_product(pages: list[dict]) -> dict:\n",
    "    system = (\n",
    "        \"You are a research analyst assistant. \"\n",
    "        \"From the provided webpage texts, extract an evidence-linked understanding of the startup's business and product. \"\n",
    "        \"CRITICAL: Always produce a non-empty content_index if there are any relevant pages. \"\n",
    "        \"Only include claims that can be supported by an evidence_url from the provided pages. \"\n",
    "        \"Return a single JSON object only (no markdown).\"\n",
    "    )\n",
    "\n",
    "    schema = {\n",
    "        \"business_product_summary\": {\n",
    "            \"company_name\": \"string\",\n",
    "            \"one_liner\": \"string|null\",\n",
    "            \"product_overview\": \"string|null\",\n",
    "            \"how_it_works_as_described\": \"array of strings\",\n",
    "            \"key_differentiators\": \"array of strings\",\n",
    "            \"primary_use_cases\": \"array of strings\",\n",
    "            \"target_customers\": \"array of strings\",\n",
    "            \"pricing_packaging_signals\": \"array of strings\",\n",
    "            \"positioning_tags\": \"array of strings\"\n",
    "        },\n",
    "        \"product_claims\": [\n",
    "            {\n",
    "                \"category\": \"product|tech|use_case|customer|pricing|positioning|other\",\n",
    "                \"claim_type\": \"fact|hypothesis\",\n",
    "                \"claim\": \"string\",\n",
    "                \"evidence_url\": \"string\",\n",
    "                \"source_type\": \"official|cse\",\n",
    "                \"confidence\": \"0..1\"\n",
    "            }\n",
    "        ],\n",
    "        \"content_index\": [\n",
    "            {\n",
    "                \"content_type\": \"product_page|blog_post|research_post|press|interview|article|docs|case_study|other\",\n",
    "                \"title\": \"string|null\",\n",
    "                \"publisher_or_platform\": \"string|null\",\n",
    "                \"date\": \"string|null\",\n",
    "                \"url\": \"string\",\n",
    "                \"what_it_contains\": \"string\",\n",
    "                \"relevance\": \"0..1\",\n",
    "                \"confidence\": \"0..1\"\n",
    "            }\n",
    "        ],\n",
    "        \"notes\": {\n",
    "            \"data_gaps\": \"array of strings\",\n",
    "            \"conflicts\": \"array of strings\"\n",
    "        },\n",
    "        \"confidence\": {\"overall\": \"0..1\", \"rationale\": \"string\"}\n",
    "    }\n",
    "\n",
    "    bundle = {\n",
    "        \"startup_name\": startup_name,\n",
    "        \"official_website\": official_home,\n",
    "        \"pages\": [\n",
    "            {\n",
    "                \"url\": p[\"url\"],\n",
    "                \"source_type\": p.get(\"source_type\"),\n",
    "                \"domain\": p.get(\"domain\"),\n",
    "                \"text\": (p.get(\"text\") or \"\")[:MAX_TEXT_CHARS_PROMPT],\n",
    "            } for p in pages\n",
    "        ],\n",
    "        \"schema\": schema,\n",
    "        \"instructions\": [\n",
    "            \"Prefer official sources where possible.\",\n",
    "            \"For product_claims, attach evidence_url to each claim.\",\n",
    "            \"For content_index, include at least 5 items if available.\"\n",
    "        ]\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        resp = client.chat.completions.create(\n",
    "            model=\"gpt-4o-mini\",\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": system},\n",
    "                {\"role\": \"user\", \"content\": json.dumps(bundle, ensure_ascii=False)},\n",
    "            ],\n",
    "            temperature=0.0,\n",
    "            response_format={\"type\": \"json_object\"},\n",
    "        )\n",
    "        return json.loads((resp.choices[0].message.content or \"\").strip())\n",
    "    except Exception:\n",
    "        resp2 = client.chat.completions.create(\n",
    "            model=\"gpt-4o-mini\",\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": system},\n",
    "                {\"role\": \"user\", \"content\": json.dumps(bundle, ensure_ascii=False)},\n",
    "            ],\n",
    "            temperature=0.0,\n",
    "        )\n",
    "        raw = resp2.choices[0].message.content or \"\"\n",
    "        candidate = _extract_json_object(raw)\n",
    "        if candidate:\n",
    "            return json.loads(candidate)\n",
    "        raise RuntimeError(\"Model did not return a JSON object.\")\n",
    "\n",
    "extracted = openai_extract_business_product(shortlist)\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Save JSON + build DataFrames\n",
    "# ------------------------------------------------------------\n",
    "JSON_PATH = ART_DIR / f\"business_product_extraction_{run_id}.json\"\n",
    "JSON_PATH.write_text(json.dumps(extracted, ensure_ascii=False, indent=2), encoding=\"utf-8\")\n",
    "\n",
    "summary = extracted.get(\"business_product_summary\", {}) or {}\n",
    "claims = extracted.get(\"product_claims\", []) or []\n",
    "content = extracted.get(\"content_index\", []) or []\n",
    "\n",
    "df_summary = pd.DataFrame([{\"run_id\": run_id, **summary}])\n",
    "df_claims = pd.DataFrame(claims)\n",
    "df_content = pd.DataFrame(content)\n",
    "\n",
    "if not df_claims.empty:\n",
    "    df_claims.insert(0, \"run_id\", run_id)\n",
    "if not df_content.empty:\n",
    "    df_content.insert(0, \"run_id\", run_id)\n",
    "\n",
    "SUMMARY_CSV = ART_DIR / f\"business_product_summary_{run_id}.csv\"\n",
    "CLAIMS_CSV = ART_DIR / f\"business_product_claims_{run_id}.csv\"\n",
    "CONTENT_CSV = ART_DIR / f\"business_product_content_index_{run_id}.csv\"\n",
    "\n",
    "df_summary.to_csv(SUMMARY_CSV, index=False)\n",
    "df_claims.to_csv(CLAIMS_CSV, index=False)\n",
    "df_content.to_csv(CONTENT_CSV, index=False)\n",
    "\n",
    "print(\"✅ Business & Product Understanding complete\")\n",
    "print(f\"- Raw pages: {RAW_PATH.as_posix()}\")\n",
    "print(f\"- JSON: {JSON_PATH.as_posix()}\")\n",
    "print(f\"- Summary CSV: {SUMMARY_CSV.as_posix()}\")\n",
    "print(f\"- Claims CSV: {CLAIMS_CSV.as_posix()}\")\n",
    "print(f\"- Content CSV: {CONTENT_CSV.as_posix()}\")\n",
    "\n",
    "display(df_summary)\n",
    "display(df_claims.head(30) if not df_claims.empty else df_claims)\n",
    "display(df_content.head(30) if not df_content.empty else df_content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a0153342-9abc-4387-936b-6e1593eaee4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Official market targets: 13\n",
      "CSE market targets: 35\n",
      "✅ Market raw pages saved: artifacts/meeting_deep_dive/market_raw_pages_20260107_225628.jsonl\n",
      "Usable pages: 28 | Shortlist to LLM: 22\n",
      "✅ Customer & Market Structure complete\n",
      "- Raw pages: artifacts/meeting_deep_dive/market_raw_pages_20260107_225628.jsonl\n",
      "- JSON: artifacts/meeting_deep_dive/market_extraction_20260107_225628.json\n",
      "- Summary CSV: artifacts/meeting_deep_dive/market_summary_20260107_225628.csv\n",
      "- Claims CSV: artifacts/meeting_deep_dive/market_claims_20260107_225628.csv\n",
      "- Content CSV: artifacts/meeting_deep_dive/market_content_index_20260107_225628.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>run_id</th>\n",
       "      <th>company_name</th>\n",
       "      <th>customer_segments</th>\n",
       "      <th>buyer_personas</th>\n",
       "      <th>primary_use_cases</th>\n",
       "      <th>market_definition</th>\n",
       "      <th>market_structure_notes</th>\n",
       "      <th>tam_sam_som_signals</th>\n",
       "      <th>geography_focus</th>\n",
       "      <th>gtm_signals</th>\n",
       "      <th>pricing_signals</th>\n",
       "      <th>partnership_signals</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20260107_225628</td>\n",
       "      <td>Sakana AI</td>\n",
       "      <td>[Financial institutions, Government agencies, ...</td>\n",
       "      <td>[CIOs and CTOs of financial institutions, Data...</td>\n",
       "      <td>[Automating banking operations, Developing AI ...</td>\n",
       "      <td>Sakana AI operates within the AI research and ...</td>\n",
       "      <td>[Sakana AI is positioned as a leader in Japan'...</td>\n",
       "      <td>[Japan's venture capital investment in startup...</td>\n",
       "      <td>Japan</td>\n",
       "      <td>[Partnerships with major financial institution...</td>\n",
       "      <td>[Sakana AI's AI Scientist can generate researc...</td>\n",
       "      <td>[Collaboration with NTT Group for R&amp;D of AI co...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            run_id company_name  \\\n",
       "0  20260107_225628    Sakana AI   \n",
       "\n",
       "                                   customer_segments  \\\n",
       "0  [Financial institutions, Government agencies, ...   \n",
       "\n",
       "                                      buyer_personas  \\\n",
       "0  [CIOs and CTOs of financial institutions, Data...   \n",
       "\n",
       "                                   primary_use_cases  \\\n",
       "0  [Automating banking operations, Developing AI ...   \n",
       "\n",
       "                                   market_definition  \\\n",
       "0  Sakana AI operates within the AI research and ...   \n",
       "\n",
       "                              market_structure_notes  \\\n",
       "0  [Sakana AI is positioned as a leader in Japan'...   \n",
       "\n",
       "                                 tam_sam_som_signals geography_focus  \\\n",
       "0  [Japan's venture capital investment in startup...           Japan   \n",
       "\n",
       "                                         gtm_signals  \\\n",
       "0  [Partnerships with major financial institution...   \n",
       "\n",
       "                                     pricing_signals  \\\n",
       "0  [Sakana AI's AI Scientist can generate researc...   \n",
       "\n",
       "                                 partnership_signals  \n",
       "0  [Collaboration with NTT Group for R&D of AI co...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>run_id</th>\n",
       "      <th>category</th>\n",
       "      <th>claim_type</th>\n",
       "      <th>claim</th>\n",
       "      <th>evidence_url</th>\n",
       "      <th>source_type</th>\n",
       "      <th>confidence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20260107_225628</td>\n",
       "      <td>customer_segment</td>\n",
       "      <td>fact</td>\n",
       "      <td>Sakana AI primarily serves large enterprises a...</td>\n",
       "      <td>https://promptloop.com/directory/what-does-sak...</td>\n",
       "      <td>cse</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20260107_225628</td>\n",
       "      <td>use_case</td>\n",
       "      <td>fact</td>\n",
       "      <td>Sakana AI is developing AI specialized for ban...</td>\n",
       "      <td>https://sakana.ai/mufg-bank/</td>\n",
       "      <td>cse</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20260107_225628</td>\n",
       "      <td>partnership</td>\n",
       "      <td>fact</td>\n",
       "      <td>Sakana AI has established a comprehensive mult...</td>\n",
       "      <td>https://sakana.ai/mufg-bank/</td>\n",
       "      <td>cse</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>20260107_225628</td>\n",
       "      <td>market</td>\n",
       "      <td>fact</td>\n",
       "      <td>Sakana AI is positioned as a leader in Japan's...</td>\n",
       "      <td>https://www.japantimes.co.jp/business/2024/04/...</td>\n",
       "      <td>cse</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20260107_225628</td>\n",
       "      <td>gtm</td>\n",
       "      <td>fact</td>\n",
       "      <td>Sakana AI's collaboration with NVIDIA aims to ...</td>\n",
       "      <td>https://sakana.ai/series-a/</td>\n",
       "      <td>cse</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>20260107_225628</td>\n",
       "      <td>pricing</td>\n",
       "      <td>fact</td>\n",
       "      <td>Sakana AI's AI Scientist can generate research...</td>\n",
       "      <td>https://sakana.ai/ai-scientist/</td>\n",
       "      <td>cse</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>20260107_225628</td>\n",
       "      <td>customer_segment</td>\n",
       "      <td>fact</td>\n",
       "      <td>Sakana AI is targeting government agency work ...</td>\n",
       "      <td>https://www.japantimes.co.jp/business/2024/04/...</td>\n",
       "      <td>cse</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>20260107_225628</td>\n",
       "      <td>market</td>\n",
       "      <td>fact</td>\n",
       "      <td>Japan's venture capital investment in startups...</td>\n",
       "      <td>https://nea.com/blog/our-investment-in-sakana-...</td>\n",
       "      <td>cse</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            run_id          category claim_type  \\\n",
       "0  20260107_225628  customer_segment       fact   \n",
       "1  20260107_225628          use_case       fact   \n",
       "2  20260107_225628       partnership       fact   \n",
       "3  20260107_225628            market       fact   \n",
       "4  20260107_225628               gtm       fact   \n",
       "5  20260107_225628           pricing       fact   \n",
       "6  20260107_225628  customer_segment       fact   \n",
       "7  20260107_225628            market       fact   \n",
       "\n",
       "                                               claim  \\\n",
       "0  Sakana AI primarily serves large enterprises a...   \n",
       "1  Sakana AI is developing AI specialized for ban...   \n",
       "2  Sakana AI has established a comprehensive mult...   \n",
       "3  Sakana AI is positioned as a leader in Japan's...   \n",
       "4  Sakana AI's collaboration with NVIDIA aims to ...   \n",
       "5  Sakana AI's AI Scientist can generate research...   \n",
       "6  Sakana AI is targeting government agency work ...   \n",
       "7  Japan's venture capital investment in startups...   \n",
       "\n",
       "                                        evidence_url source_type  confidence  \n",
       "0  https://promptloop.com/directory/what-does-sak...         cse           1  \n",
       "1                       https://sakana.ai/mufg-bank/         cse           1  \n",
       "2                       https://sakana.ai/mufg-bank/         cse           1  \n",
       "3  https://www.japantimes.co.jp/business/2024/04/...         cse           1  \n",
       "4                        https://sakana.ai/series-a/         cse           1  \n",
       "5                    https://sakana.ai/ai-scientist/         cse           1  \n",
       "6  https://www.japantimes.co.jp/business/2024/04/...         cse           1  \n",
       "7  https://nea.com/blog/our-investment-in-sakana-...         cse           1  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>run_id</th>\n",
       "      <th>content_type</th>\n",
       "      <th>title</th>\n",
       "      <th>publisher_or_platform</th>\n",
       "      <th>date</th>\n",
       "      <th>url</th>\n",
       "      <th>what_it_contains</th>\n",
       "      <th>relevance</th>\n",
       "      <th>confidence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20260107_225628</td>\n",
       "      <td>press</td>\n",
       "      <td>MUFG enters multiyear AI partnership with Saka...</td>\n",
       "      <td>Retail Banker International</td>\n",
       "      <td>May 20, 2025</td>\n",
       "      <td>https://www.retailbankerinternational.com/news...</td>\n",
       "      <td>Details on the partnership between MUFG and Sa...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20260107_225628</td>\n",
       "      <td>press</td>\n",
       "      <td>NTT and Sakana AI sign a collaboration agreement</td>\n",
       "      <td>NTT Group</td>\n",
       "      <td>November 13, 2023</td>\n",
       "      <td>https://group.ntt/en/newsrelease/2023/11/13/23...</td>\n",
       "      <td>Collaboration agreement for R&amp;D of AI constell...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20260107_225628</td>\n",
       "      <td>blog</td>\n",
       "      <td>Our Investment in Japan’s Sakana AI</td>\n",
       "      <td>NEA</td>\n",
       "      <td>September 04, 2024</td>\n",
       "      <td>https://www.nea.com/blog/our-investment-in-sak...</td>\n",
       "      <td>Insights into NEA's investment in Sakana AI an...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>20260107_225628</td>\n",
       "      <td>press</td>\n",
       "      <td>Sakana AI raises $135M Series B</td>\n",
       "      <td>Yahoo Finance</td>\n",
       "      <td>November 17, 2025</td>\n",
       "      <td>https://finance.yahoo.com/news/sakana-ai-raise...</td>\n",
       "      <td>Details on Sakana AI's Series B funding and it...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20260107_225628</td>\n",
       "      <td>press</td>\n",
       "      <td>Sakana AI and Daiwa Securities Group to Develo...</td>\n",
       "      <td>Sakana AI</td>\n",
       "      <td>October 03, 2025</td>\n",
       "      <td>https://sakana.ai/daiwa-securities/</td>\n",
       "      <td>Announcement of a partnership to innovate Japa...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>20260107_225628</td>\n",
       "      <td>press</td>\n",
       "      <td>Sakana AI raises over $100M in Series A</td>\n",
       "      <td>The SaaS News</td>\n",
       "      <td>September 04, 2024</td>\n",
       "      <td>https://thesaasnews.com/news/sakana-ai-raises-...</td>\n",
       "      <td>Overview of Sakana AI's Series A funding and i...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>20260107_225628</td>\n",
       "      <td>press</td>\n",
       "      <td>Sakana AI and ANA HOLDINGS' Investment</td>\n",
       "      <td>ANA Holdings</td>\n",
       "      <td>November 05, 2024</td>\n",
       "      <td>https://www.anahd.co.jp/group/en/pr/202411/202...</td>\n",
       "      <td>Details on ANA Holdings' investment in Sakana ...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>20260107_225628</td>\n",
       "      <td>press</td>\n",
       "      <td>Sakana AI lands $135M on $2.635B valuation</td>\n",
       "      <td>SiliconANGLE</td>\n",
       "      <td>November 17, 2025</td>\n",
       "      <td>https://siliconangle.com/2025/11/17/sakana-ai-...</td>\n",
       "      <td>Information on Sakana AI's funding and its foc...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            run_id content_type  \\\n",
       "0  20260107_225628        press   \n",
       "1  20260107_225628        press   \n",
       "2  20260107_225628         blog   \n",
       "3  20260107_225628        press   \n",
       "4  20260107_225628        press   \n",
       "5  20260107_225628        press   \n",
       "6  20260107_225628        press   \n",
       "7  20260107_225628        press   \n",
       "\n",
       "                                               title  \\\n",
       "0  MUFG enters multiyear AI partnership with Saka...   \n",
       "1   NTT and Sakana AI sign a collaboration agreement   \n",
       "2                Our Investment in Japan’s Sakana AI   \n",
       "3                    Sakana AI raises $135M Series B   \n",
       "4  Sakana AI and Daiwa Securities Group to Develo...   \n",
       "5            Sakana AI raises over $100M in Series A   \n",
       "6             Sakana AI and ANA HOLDINGS' Investment   \n",
       "7         Sakana AI lands $135M on $2.635B valuation   \n",
       "\n",
       "         publisher_or_platform                date  \\\n",
       "0  Retail Banker International        May 20, 2025   \n",
       "1                    NTT Group   November 13, 2023   \n",
       "2                          NEA  September 04, 2024   \n",
       "3                Yahoo Finance   November 17, 2025   \n",
       "4                    Sakana AI    October 03, 2025   \n",
       "5                The SaaS News  September 04, 2024   \n",
       "6                 ANA Holdings   November 05, 2024   \n",
       "7                 SiliconANGLE   November 17, 2025   \n",
       "\n",
       "                                                 url  \\\n",
       "0  https://www.retailbankerinternational.com/news...   \n",
       "1  https://group.ntt/en/newsrelease/2023/11/13/23...   \n",
       "2  https://www.nea.com/blog/our-investment-in-sak...   \n",
       "3  https://finance.yahoo.com/news/sakana-ai-raise...   \n",
       "4                https://sakana.ai/daiwa-securities/   \n",
       "5  https://thesaasnews.com/news/sakana-ai-raises-...   \n",
       "6  https://www.anahd.co.jp/group/en/pr/202411/202...   \n",
       "7  https://siliconangle.com/2025/11/17/sakana-ai-...   \n",
       "\n",
       "                                    what_it_contains  relevance  confidence  \n",
       "0  Details on the partnership between MUFG and Sa...          1           1  \n",
       "1  Collaboration agreement for R&D of AI constell...          1           1  \n",
       "2  Insights into NEA's investment in Sakana AI an...          1           1  \n",
       "3  Details on Sakana AI's Series B funding and it...          1           1  \n",
       "4  Announcement of a partnership to innovate Japa...          1           1  \n",
       "5  Overview of Sakana AI's Series A funding and i...          1           1  \n",
       "6  Details on ANA Holdings' investment in Sakana ...          1           1  \n",
       "7  Information on Sakana AI's funding and its foc...          1           1  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ============================================================\n",
    "# 7. Customer & Market Structure (Facts-first + Evidence-linked)\n",
    "# ============================================================\n",
    "# Goal:\n",
    "# - Build an evidence-linked view of:\n",
    "#   - Customer segments and buyer personas\n",
    "#   - Use cases / workflows / jobs-to-be-done\n",
    "#   - Market structure: TAM/SAM/SOM signals, vertical focus, geography, trends\n",
    "#   - GTM signals: channels, partnerships, sales motion, pricing signals (as mentioned)\n",
    "# - Identify relevant public narratives: posts, interviews, articles, reports.\n",
    "# - Use the same pattern:\n",
    "#   official pages first + CSE enrichment, robots check,\n",
    "#   store raw pages as JSONL, then OpenAI normalizes into:\n",
    "#     1) market_summary (single object)\n",
    "#     2) market_claims (long-form with evidence_url + confidence)\n",
    "#     3) market_content_index (content list with URLs)\n",
    "#\n",
    "# Outputs:\n",
    "# - market_raw_pages_<run_id>.jsonl\n",
    "# - market_extraction_<run_id>.json\n",
    "# - market_summary_<run_id>.csv\n",
    "# - market_claims_<run_id>.csv\n",
    "# - market_content_index_<run_id>.csv\n",
    "\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "import time\n",
    "import requests\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "from pathlib import Path\n",
    "from urllib.parse import urlparse, urljoin\n",
    "from datetime import datetime, timezone\n",
    "from urllib.robotparser import RobotFileParser\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Global configuration\n",
    "# ------------------------------------------------------------\n",
    "load_dotenv(\"env.txt\")\n",
    "\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "if not OPENAI_API_KEY:\n",
    "    raise EnvironmentError(\"OPENAI_API_KEY is not set.\")\n",
    "\n",
    "GOOGLE_API_KEY = os.getenv(\"GOOGLE_API_KEY\")\n",
    "GOOGLE_CSE_CX = os.getenv(\"GOOGLE_CSE_CX\")\n",
    "if not GOOGLE_API_KEY or not GOOGLE_CSE_CX:\n",
    "    raise EnvironmentError(\"GOOGLE_API_KEY and GOOGLE_CSE_CX must be set.\")\n",
    "\n",
    "client = OpenAI(api_key=OPENAI_API_KEY)\n",
    "\n",
    "ART_DIR = Path(\"artifacts\") / \"meeting_deep_dive\"\n",
    "ART_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Load inputs + entity\n",
    "# ------------------------------------------------------------\n",
    "def load_latest_json(pattern: str) -> tuple[dict, Path]:\n",
    "    files = sorted(ART_DIR.glob(pattern), key=lambda p: p.stat().st_mtime, reverse=True)\n",
    "    if not files:\n",
    "        raise FileNotFoundError(f\"No {pattern} found. Please run previous cells.\")\n",
    "    p = files[0]\n",
    "    return json.loads(p.read_text(encoding=\"utf-8\")), p\n",
    "\n",
    "try:\n",
    "    run_id = inputs[\"meta\"][\"run_id\"]\n",
    "except Exception:\n",
    "    inp, _ = load_latest_json(\"inputs_*.json\")\n",
    "    run_id = inp.get(\"meta\", {}).get(\"run_id\") or datetime.now(timezone.utc).strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "entity, _ = load_latest_json(\"entity_*.json\")\n",
    "startup_name = (entity.get(\"canonical_name\") or \"Unknown Startup\").strip()\n",
    "official_website = (entity.get(\"official_website\") or \"\").strip()\n",
    "if not official_website:\n",
    "    raise ValueError(\"official_website is missing. Run cell #2 first.\")\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Knobs\n",
    "# ------------------------------------------------------------\n",
    "USER_AGENT = \"researchOSv2-bot/0.3 (+contact: internal-research)\"\n",
    "TIMEOUT = 25\n",
    "SLEEP_SEC = 0.25\n",
    "\n",
    "MAX_OFFICIAL_PAGES = 35\n",
    "MAX_CSE_PAGES = 35\n",
    "MAX_TEXT_CHARS_STORE = 70000\n",
    "MAX_TEXT_CHARS_PROMPT = 9000\n",
    "MAX_PAGES_TO_LLM = 22\n",
    "\n",
    "MARKET_KEY_PATHS = [\n",
    "    \"/customers\", \"/case\", \"/cases\", \"/case-studies\", \"/usecase\", \"/use-cases\",\n",
    "    \"/solutions\", \"/industries\",\n",
    "    \"/pricing\",\n",
    "    \"/blog\", \"/news\", \"/press\", \"/research\", \"/updates\",\n",
    "]\n",
    "MARKET_KEYWORDS = [\n",
    "    \"customer\", \"customers\", \"case\", \"case-study\", \"usecase\", \"use-case\", \"industries\",\n",
    "    \"solutions\", \"pricing\",\n",
    "    \"market\", \"enterprise\", \"partner\", \"partnership\",\n",
    "    \"blog\", \"news\", \"press\", \"research\"\n",
    "]\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# URL / domain / robots helpers\n",
    "# ------------------------------------------------------------\n",
    "def normalize_url(url: str) -> str:\n",
    "    url = (url or \"\").strip()\n",
    "    if not url:\n",
    "        return \"\"\n",
    "    if not re.match(r\"^https?://\", url, flags=re.I):\n",
    "        url = \"https://\" + url\n",
    "    p = urlparse(url)\n",
    "    return p._replace(netloc=p.netloc.lower(), fragment=\"\").geturl()\n",
    "\n",
    "def get_domain(url: str) -> str:\n",
    "    return re.sub(r\"^www\\.\", \"\", urlparse(normalize_url(url)).netloc)\n",
    "\n",
    "def same_domain(a: str, b: str) -> bool:\n",
    "    da, db = get_domain(a), get_domain(b)\n",
    "    return da == db or da.endswith(\".\" + db) or db.endswith(\".\" + da)\n",
    "\n",
    "def is_asset_url(url: str) -> bool:\n",
    "    path = (urlparse(url).path or \"\").lower()\n",
    "    return bool(re.search(r\"\\.(pdf|jpg|jpeg|png|gif|svg|zip|mp4|mov)$\", path))\n",
    "\n",
    "def build_robot_parser(base_url: str) -> RobotFileParser | None:\n",
    "    rp = RobotFileParser()\n",
    "    rp.set_url(urljoin(normalize_url(base_url), \"/robots.txt\"))\n",
    "    try:\n",
    "        rp.read()\n",
    "        return rp\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "def robots_allows(rp: RobotFileParser | None, url: str) -> bool:\n",
    "    if rp is None:\n",
    "        p = (urlparse(url).path or \"/\").lower()\n",
    "        return p in [\"/\", \"\"] or any(k in p for k in MARKET_KEYWORDS)\n",
    "    try:\n",
    "        return rp.can_fetch(USER_AGENT, url)\n",
    "    except Exception:\n",
    "        return False\n",
    "\n",
    "_robot_cache: dict[str, RobotFileParser | None] = {}\n",
    "\n",
    "def get_rp_for_url(url: str) -> RobotFileParser | None:\n",
    "    dom = get_domain(url)\n",
    "    if dom in _robot_cache:\n",
    "        return _robot_cache[dom]\n",
    "    rp = build_robot_parser(url)\n",
    "    _robot_cache[dom] = rp\n",
    "    return rp\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Fetch + text extraction\n",
    "# ------------------------------------------------------------\n",
    "def fetch_html(url: str) -> tuple[int, str]:\n",
    "    r = requests.get(url, headers={\"User-Agent\": USER_AGENT}, timeout=TIMEOUT, allow_redirects=True)\n",
    "    return r.status_code, (r.text if r.ok else \"\")\n",
    "\n",
    "def html_to_text(html: str) -> str:\n",
    "    soup = BeautifulSoup(html, \"lxml\")\n",
    "    for tag in soup([\"script\", \"style\", \"noscript\", \"svg\"]):\n",
    "        tag.decompose()\n",
    "    node = soup.find(\"main\") or soup.find(\"article\") or (soup.body if soup.body else soup)\n",
    "    text = node.get_text(\"\\n\", strip=True)\n",
    "    text = re.sub(r\"\\n{3,}\", \"\\n\\n\", text)\n",
    "    return text.strip()\n",
    "\n",
    "def extract_links(base_url: str, html: str) -> list[str]:\n",
    "    soup = BeautifulSoup(html, \"lxml\")\n",
    "    out = []\n",
    "    for a in soup.find_all(\"a\", href=True):\n",
    "        href = a[\"href\"].strip()\n",
    "        if href.startswith(\"#\") or href.lower().startswith(\"mailto:\") or href.lower().startswith(\"tel:\"):\n",
    "            continue\n",
    "        u = normalize_url(urljoin(base_url, href))\n",
    "        if u.startswith((\"http://\", \"https://\")):\n",
    "            out.append(u)\n",
    "    seen, uniq = set(), []\n",
    "    for u in out:\n",
    "        if u not in seen:\n",
    "            seen.add(u)\n",
    "            uniq.append(u)\n",
    "    return uniq\n",
    "\n",
    "def score_market_link(url: str) -> float:\n",
    "    p = (urlparse(url).path or \"/\").lower()\n",
    "    score = 0.0\n",
    "    for k in MARKET_KEYWORDS:\n",
    "        if k in p:\n",
    "            score += 1.0\n",
    "    if \"case\" in p or \"customer\" in p:\n",
    "        score += 1.5\n",
    "    if \"pricing\" in p:\n",
    "        score += 1.0\n",
    "    if is_asset_url(url):\n",
    "        score -= 4.0\n",
    "    if p.count(\"/\") >= 4:\n",
    "        score -= 0.4\n",
    "    return score\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Google CSE\n",
    "# ------------------------------------------------------------\n",
    "def google_cse_search(query: str, num: int = 5) -> list[dict]:\n",
    "    url = \"https://www.googleapis.com/customsearch/v1\"\n",
    "    params = {\"key\": GOOGLE_API_KEY, \"cx\": GOOGLE_CSE_CX, \"q\": query, \"num\": min(max(num, 1), 10)}\n",
    "    r = requests.get(url, params=params, timeout=30)\n",
    "    r.raise_for_status()\n",
    "    data = r.json()\n",
    "    items = data.get(\"items\", []) or []\n",
    "    return [{\n",
    "        \"title\": it.get(\"title\"),\n",
    "        \"link\": normalize_url(it.get(\"link\") or \"\"),\n",
    "        \"snippet\": it.get(\"snippet\"),\n",
    "        \"displayLink\": it.get(\"displayLink\"),\n",
    "    } for it in items]\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Official targets\n",
    "# ------------------------------------------------------------\n",
    "official_home = normalize_url(official_website)\n",
    "rp_official = build_robot_parser(official_home)\n",
    "\n",
    "status, html = fetch_html(official_home)\n",
    "time.sleep(SLEEP_SEC)\n",
    "if status >= 400 or not html:\n",
    "    raise RuntimeError(f\"Failed to fetch official homepage: HTTP {status}\")\n",
    "\n",
    "home_links = extract_links(official_home, html)\n",
    "internal_links = [l for l in home_links if same_domain(l, official_home) and not is_asset_url(l)]\n",
    "\n",
    "explicit = []\n",
    "for p in MARKET_KEY_PATHS:\n",
    "    u = normalize_url(urljoin(official_home, p))\n",
    "    if robots_allows(rp_official, u) and not is_asset_url(u):\n",
    "        explicit.append(u)\n",
    "\n",
    "candidates = list(dict.fromkeys([official_home] + explicit + internal_links))\n",
    "scored = [{\"url\": u, \"score\": score_market_link(u)} for u in candidates]\n",
    "scored.sort(key=lambda x: x[\"score\"], reverse=True)\n",
    "\n",
    "official_targets = []\n",
    "for it in scored:\n",
    "    if len(official_targets) >= MAX_OFFICIAL_PAGES:\n",
    "        break\n",
    "    if it[\"score\"] < 0.2:\n",
    "        continue\n",
    "    if robots_allows(rp_official, it[\"url\"]):\n",
    "        official_targets.append(it[\"url\"])\n",
    "\n",
    "official_targets = list(dict.fromkeys(official_targets))\n",
    "print(f\"Official market targets: {len(official_targets)}\")\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# CSE targets (customers + market structure focused)\n",
    "# ------------------------------------------------------------\n",
    "official_domain = get_domain(official_home)\n",
    "queries = [\n",
    "    f'\"{startup_name}\" {official_domain}',\n",
    "    f'\"{startup_name}\" customer OR customers OR 導入 OR 顧客',\n",
    "    f'\"{startup_name}\" case study OR 導入事例 OR 事例',\n",
    "    f'\"{startup_name}\" target market OR 市場 OR TAM OR SAM OR SOM',\n",
    "    f'\"{startup_name}\" pricing OR 料金 OR enterprise plan',\n",
    "    f'\"{startup_name}\" go-to-market OR GTM OR sales OR 営業',\n",
    "    f'\"{startup_name}\" partnership OR partner OR 提携',\n",
    "    f'\"{startup_name}\" industry OR industries OR 業界',\n",
    "    f'\"{startup_name}\" procurement OR RFP OR 導入',\n",
    "    f'\"{startup_name}\" interview market OR インタビュー 市場',\n",
    "]\n",
    "\n",
    "cse_items = []\n",
    "for q in queries:\n",
    "    try:\n",
    "        cse_items.extend(google_cse_search(q, num=5))\n",
    "        time.sleep(0.2)\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "seen = set()\n",
    "cse_links = []\n",
    "for it in cse_items:\n",
    "    link = it.get(\"link\")\n",
    "    if link and link not in seen and link.startswith((\"http://\", \"https://\")) and not is_asset_url(link):\n",
    "        seen.add(link)\n",
    "        cse_links.append(link)\n",
    "\n",
    "cse_links = cse_links[:MAX_CSE_PAGES]\n",
    "print(f\"CSE market targets: {len(cse_links)}\")\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Fetch + store raw pages (JSONL)\n",
    "# ------------------------------------------------------------\n",
    "RAW_PATH = ART_DIR / f\"market_raw_pages_{run_id}.jsonl\"\n",
    "RAW_PATH.write_text(\"\", encoding=\"utf-8\")\n",
    "\n",
    "def append_jsonl(path: Path, obj: dict):\n",
    "    with path.open(\"a\", encoding=\"utf-8\") as f:\n",
    "        f.write(json.dumps(obj, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "def collect_pages(urls: list[str], source_type: str):\n",
    "    for u in urls:\n",
    "        u = normalize_url(u)\n",
    "        if not u or is_asset_url(u):\n",
    "            continue\n",
    "\n",
    "        rp = rp_official if source_type == \"official\" else get_rp_for_url(u)\n",
    "        allowed = robots_allows(rp, u)\n",
    "\n",
    "        if not allowed:\n",
    "            append_jsonl(RAW_PATH, {\n",
    "                \"url\": u,\n",
    "                \"source_type\": source_type,\n",
    "                \"domain\": get_domain(u),\n",
    "                \"fetched_at_utc\": datetime.now(timezone.utc).isoformat(),\n",
    "                \"http_status\": None,\n",
    "                \"robots_allowed\": False,\n",
    "                \"text\": \"\",\n",
    "                \"notes\": \"Skipped due to robots.txt (or conservative fallback).\",\n",
    "            })\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            code, html = fetch_html(u)\n",
    "            time.sleep(SLEEP_SEC)\n",
    "            text = html_to_text(html) if html else \"\"\n",
    "            text = text[:MAX_TEXT_CHARS_STORE]\n",
    "\n",
    "            append_jsonl(RAW_PATH, {\n",
    "                \"url\": u,\n",
    "                \"source_type\": source_type,\n",
    "                \"domain\": get_domain(u),\n",
    "                \"fetched_at_utc\": datetime.now(timezone.utc).isoformat(),\n",
    "                \"http_status\": code,\n",
    "                \"robots_allowed\": True,\n",
    "                \"text\": text,\n",
    "                \"text_char_len\": len(text),\n",
    "            })\n",
    "        except Exception as e:\n",
    "            append_jsonl(RAW_PATH, {\n",
    "                \"url\": u,\n",
    "                \"source_type\": source_type,\n",
    "                \"domain\": get_domain(u),\n",
    "                \"fetched_at_utc\": datetime.now(timezone.utc).isoformat(),\n",
    "                \"http_status\": None,\n",
    "                \"robots_allowed\": True,\n",
    "                \"text\": \"\",\n",
    "                \"notes\": f\"Fetch error: {str(e)}\",\n",
    "            })\n",
    "\n",
    "collect_pages(official_targets, \"official\")\n",
    "collect_pages(cse_links, \"cse\")\n",
    "\n",
    "print(f\"✅ Market raw pages saved: {RAW_PATH.as_posix()}\")\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Load raw pages + shortlist to LLM (market-heavy URLs first)\n",
    "# ------------------------------------------------------------\n",
    "def load_raw_pages(path: Path) -> list[dict]:\n",
    "    rows = []\n",
    "    for line in path.read_text(encoding=\"utf-8\").splitlines():\n",
    "        if line.strip():\n",
    "            rows.append(json.loads(line))\n",
    "    return rows\n",
    "\n",
    "raw_pages = load_raw_pages(RAW_PATH)\n",
    "\n",
    "usable = [\n",
    "    r for r in raw_pages\n",
    "    if r.get(\"robots_allowed\")\n",
    "    and isinstance(r.get(\"http_status\"), int) and r[\"http_status\"] < 400\n",
    "    and (r.get(\"text\") or \"\").strip()\n",
    "]\n",
    "\n",
    "def score_page_for_market(r):\n",
    "    url = r.get(\"url\") or \"\"\n",
    "    text = r.get(\"text\") or \"\"\n",
    "    p = (urlparse(url).path or \"\").lower()\n",
    "    s = 0\n",
    "    for k in MARKET_KEYWORDS:\n",
    "        if k in p:\n",
    "            s += 2\n",
    "    # prefer pages that mention customer words in the text\n",
    "    if re.search(r\"\\b(customer|customers|client|導入|顧客|事例|利用)\\b\", text, flags=re.IGNORECASE):\n",
    "        s += 2\n",
    "    s += min(len(text), 20000) / 7000\n",
    "    if r.get(\"source_type\") == \"official\":\n",
    "        s += 2\n",
    "    return s\n",
    "\n",
    "usable_sorted = sorted(usable, key=score_page_for_market, reverse=True)\n",
    "shortlist = usable_sorted[:MAX_PAGES_TO_LLM]\n",
    "print(f\"Usable pages: {len(usable)} | Shortlist to LLM: {len(shortlist)}\")\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Robust JSON parsing helper\n",
    "# ------------------------------------------------------------\n",
    "def _extract_json_object(text: str) -> str:\n",
    "    if not text:\n",
    "        return \"\"\n",
    "    t = text.strip()\n",
    "    m = re.search(r\"```(?:json)?\\s*(\\{.*?\\})\\s*```\", t, flags=re.DOTALL | re.IGNORECASE)\n",
    "    if m:\n",
    "        return m.group(1).strip()\n",
    "    start, end = t.find(\"{\"), t.rfind(\"}\")\n",
    "    if start != -1 and end != -1 and end > start:\n",
    "        return t[start:end+1].strip()\n",
    "    return \"\"\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# OpenAI: market summary + claims + content index\n",
    "# ------------------------------------------------------------\n",
    "def openai_extract_market(pages: list[dict]) -> dict:\n",
    "    system = (\n",
    "        \"You are a research analyst assistant. \"\n",
    "        \"From the provided webpage texts, extract an evidence-linked understanding of customer segments and market structure. \"\n",
    "        \"CRITICAL: Always produce a non-empty content_index if there are relevant pages. \"\n",
    "        \"Only include claims that can be supported by an evidence_url from the provided pages. \"\n",
    "        \"Return a single JSON object only (no markdown).\"\n",
    "    )\n",
    "\n",
    "    schema = {\n",
    "        \"market_summary\": {\n",
    "            \"company_name\": \"string\",\n",
    "            \"customer_segments\": \"array of strings\",\n",
    "            \"buyer_personas\": \"array of strings\",\n",
    "            \"primary_use_cases\": \"array of strings\",\n",
    "            \"market_definition\": \"string|null\",\n",
    "            \"market_structure_notes\": \"array of strings\",\n",
    "            \"tam_sam_som_signals\": \"array of strings\",\n",
    "            \"geography_focus\": \"string|null\",\n",
    "            \"gtm_signals\": \"array of strings\",\n",
    "            \"pricing_signals\": \"array of strings\",\n",
    "            \"partnership_signals\": \"array of strings\"\n",
    "        },\n",
    "        \"market_claims\": [\n",
    "            {\n",
    "                \"category\": \"customer_segment|buyer|use_case|market|tam|gtm|pricing|partnership|other\",\n",
    "                \"claim_type\": \"fact|hypothesis\",\n",
    "                \"claim\": \"string\",\n",
    "                \"evidence_url\": \"string\",\n",
    "                \"source_type\": \"official|cse\",\n",
    "                \"confidence\": \"0..1\"\n",
    "            }\n",
    "        ],\n",
    "        \"content_index\": [\n",
    "            {\n",
    "                \"content_type\": \"case_study|customer_story|interview|article|press|blog|report|other\",\n",
    "                \"title\": \"string|null\",\n",
    "                \"publisher_or_platform\": \"string|null\",\n",
    "                \"date\": \"string|null\",\n",
    "                \"url\": \"string\",\n",
    "                \"what_it_contains\": \"string\",\n",
    "                \"relevance\": \"0..1\",\n",
    "                \"confidence\": \"0..1\"\n",
    "            }\n",
    "        ],\n",
    "        \"notes\": {\n",
    "            \"data_gaps\": \"array of strings\",\n",
    "            \"conflicts\": \"array of strings\"\n",
    "        },\n",
    "        \"confidence\": {\"overall\": \"0..1\", \"rationale\": \"string\"}\n",
    "    }\n",
    "\n",
    "    bundle = {\n",
    "        \"startup_name\": startup_name,\n",
    "        \"official_website\": official_home,\n",
    "        \"pages\": [\n",
    "            {\n",
    "                \"url\": p[\"url\"],\n",
    "                \"source_type\": p.get(\"source_type\"),\n",
    "                \"domain\": p.get(\"domain\"),\n",
    "                \"text\": (p.get(\"text\") or \"\")[:MAX_TEXT_CHARS_PROMPT],\n",
    "            } for p in pages\n",
    "        ],\n",
    "        \"schema\": schema,\n",
    "        \"instructions\": [\n",
    "            \"Prefer official sources where possible.\",\n",
    "            \"Attach evidence_url to each claim.\",\n",
    "            \"For content_index, include at least 8 items if available.\"\n",
    "        ]\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        resp = client.chat.completions.create(\n",
    "            model=\"gpt-4o-mini\",\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": system},\n",
    "                {\"role\": \"user\", \"content\": json.dumps(bundle, ensure_ascii=False)},\n",
    "            ],\n",
    "            temperature=0.0,\n",
    "            response_format={\"type\": \"json_object\"},\n",
    "        )\n",
    "        return json.loads((resp.choices[0].message.content or \"\").strip())\n",
    "    except Exception:\n",
    "        resp2 = client.chat.completions.create(\n",
    "            model=\"gpt-4o-mini\",\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": system},\n",
    "                {\"role\": \"user\", \"content\": json.dumps(bundle, ensure_ascii=False)},\n",
    "            ],\n",
    "            temperature=0.0,\n",
    "        )\n",
    "        raw = resp2.choices[0].message.content or \"\"\n",
    "        candidate = _extract_json_object(raw)\n",
    "        if candidate:\n",
    "            return json.loads(candidate)\n",
    "        raise RuntimeError(\"Model did not return a JSON object.\")\n",
    "\n",
    "extracted = openai_extract_market(shortlist)\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Save JSON + build DataFrames\n",
    "# ------------------------------------------------------------\n",
    "JSON_PATH = ART_DIR / f\"market_extraction_{run_id}.json\"\n",
    "JSON_PATH.write_text(json.dumps(extracted, ensure_ascii=False, indent=2), encoding=\"utf-8\")\n",
    "\n",
    "summary = extracted.get(\"market_summary\", {}) or {}\n",
    "claims = extracted.get(\"market_claims\", []) or []\n",
    "content = extracted.get(\"content_index\", []) or []\n",
    "\n",
    "df_summary = pd.DataFrame([{\"run_id\": run_id, **summary}])\n",
    "df_claims = pd.DataFrame(claims)\n",
    "df_content = pd.DataFrame(content)\n",
    "\n",
    "if not df_claims.empty:\n",
    "    df_claims.insert(0, \"run_id\", run_id)\n",
    "if not df_content.empty:\n",
    "    df_content.insert(0, \"run_id\", run_id)\n",
    "\n",
    "SUMMARY_CSV = ART_DIR / f\"market_summary_{run_id}.csv\"\n",
    "CLAIMS_CSV = ART_DIR / f\"market_claims_{run_id}.csv\"\n",
    "CONTENT_CSV = ART_DIR / f\"market_content_index_{run_id}.csv\"\n",
    "\n",
    "df_summary.to_csv(SUMMARY_CSV, index=False)\n",
    "df_claims.to_csv(CLAIMS_CSV, index=False)\n",
    "df_content.to_csv(CONTENT_CSV, index=False)\n",
    "\n",
    "print(\"✅ Customer & Market Structure complete\")\n",
    "print(f\"- Raw pages: {RAW_PATH.as_posix()}\")\n",
    "print(f\"- JSON: {JSON_PATH.as_posix()}\")\n",
    "print(f\"- Summary CSV: {SUMMARY_CSV.as_posix()}\")\n",
    "print(f\"- Claims CSV: {CLAIMS_CSV.as_posix()}\")\n",
    "print(f\"- Content CSV: {CONTENT_CSV.as_posix()}\")\n",
    "\n",
    "display(df_summary)\n",
    "display(df_claims.head(30) if not df_claims.empty else df_claims)\n",
    "display(df_content.head(30) if not df_content.empty else df_content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0089f67b-462d-41d4-8281-dbaade27bb61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Official competitive targets: 9\n",
      "CSE competitive targets: 33\n",
      "✅ Competitive raw pages saved: artifacts/meeting_deep_dive/competitive_raw_pages_20260107_225628.jsonl\n",
      "Usable pages: 19 | Shortlist to LLM: 19\n",
      "✅ Competitive Landscape complete\n",
      "- Raw pages: artifacts/meeting_deep_dive/competitive_raw_pages_20260107_225628.jsonl\n",
      "- JSON: artifacts/meeting_deep_dive/competitive_extraction_20260107_225628.json\n",
      "- Summary CSV: artifacts/meeting_deep_dive/competitive_summary_20260107_225628.csv\n",
      "- Competitors CSV: artifacts/meeting_deep_dive/competitor_table_20260107_225628.csv\n",
      "- Claims CSV: artifacts/meeting_deep_dive/competitive_claims_20260107_225628.csv\n",
      "- Content CSV: artifacts/meeting_deep_dive/competitive_content_index_20260107_225628.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>run_id</th>\n",
       "      <th>company_name</th>\n",
       "      <th>category_description</th>\n",
       "      <th>positioning_statements</th>\n",
       "      <th>differentiation_points</th>\n",
       "      <th>competitive_dynamics_notes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20260107_225628</td>\n",
       "      <td>Sakana AI</td>\n",
       "      <td>AI research and development focused on nature-...</td>\n",
       "      <td>[Sakana AI aims to develop transformative AI t...</td>\n",
       "      <td>[Nature-inspired intelligence and evolutionary...</td>\n",
       "      <td>[Sakana AI is positioned uniquely in the Japan...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            run_id company_name  \\\n",
       "0  20260107_225628    Sakana AI   \n",
       "\n",
       "                                category_description  \\\n",
       "0  AI research and development focused on nature-...   \n",
       "\n",
       "                              positioning_statements  \\\n",
       "0  [Sakana AI aims to develop transformative AI t...   \n",
       "\n",
       "                              differentiation_points  \\\n",
       "0  [Nature-inspired intelligence and evolutionary...   \n",
       "\n",
       "                          competitive_dynamics_notes  \n",
       "0  [Sakana AI is positioned uniquely in the Japan...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>run_id</th>\n",
       "      <th>name</th>\n",
       "      <th>type</th>\n",
       "      <th>why_it_is_a_competitor</th>\n",
       "      <th>comparison_points</th>\n",
       "      <th>evidence_url</th>\n",
       "      <th>confidence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20260107_225628</td>\n",
       "      <td>OpenAI</td>\n",
       "      <td>direct</td>\n",
       "      <td>OpenAI develops advanced AI models and technol...</td>\n",
       "      <td>[Both companies focus on developing large-scal...</td>\n",
       "      <td>https://www.promptloop.com/directory/what-does...</td>\n",
       "      <td>0.90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20260107_225628</td>\n",
       "      <td>Google DeepMind</td>\n",
       "      <td>direct</td>\n",
       "      <td>DeepMind is a leader in AI research and develo...</td>\n",
       "      <td>[Both companies are involved in cutting-edge A...</td>\n",
       "      <td>https://www.promptloop.com/directory/what-does...</td>\n",
       "      <td>0.85</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20260107_225628</td>\n",
       "      <td>Anthropic</td>\n",
       "      <td>direct</td>\n",
       "      <td>Anthropic focuses on AI safety and developing ...</td>\n",
       "      <td>[Both companies are innovating in the AI space...</td>\n",
       "      <td>https://www.promptloop.com/directory/what-does...</td>\n",
       "      <td>0.80</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            run_id             name    type  \\\n",
       "0  20260107_225628           OpenAI  direct   \n",
       "1  20260107_225628  Google DeepMind  direct   \n",
       "2  20260107_225628        Anthropic  direct   \n",
       "\n",
       "                              why_it_is_a_competitor  \\\n",
       "0  OpenAI develops advanced AI models and technol...   \n",
       "1  DeepMind is a leader in AI research and develo...   \n",
       "2  Anthropic focuses on AI safety and developing ...   \n",
       "\n",
       "                                   comparison_points  \\\n",
       "0  [Both companies focus on developing large-scal...   \n",
       "1  [Both companies are involved in cutting-edge A...   \n",
       "2  [Both companies are innovating in the AI space...   \n",
       "\n",
       "                                        evidence_url  confidence  \n",
       "0  https://www.promptloop.com/directory/what-does...        0.90  \n",
       "1  https://www.promptloop.com/directory/what-does...        0.85  \n",
       "2  https://www.promptloop.com/directory/what-does...        0.80  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>run_id</th>\n",
       "      <th>category</th>\n",
       "      <th>claim_type</th>\n",
       "      <th>claim</th>\n",
       "      <th>evidence_url</th>\n",
       "      <th>source_type</th>\n",
       "      <th>confidence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20260107_225628</td>\n",
       "      <td>positioning</td>\n",
       "      <td>fact</td>\n",
       "      <td>Sakana AI is focused on developing efficient, ...</td>\n",
       "      <td>https://siliconangle.com/2025/11/17/sakana-ai-...</td>\n",
       "      <td>cse</td>\n",
       "      <td>0.95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20260107_225628</td>\n",
       "      <td>differentiation</td>\n",
       "      <td>fact</td>\n",
       "      <td>Sakana AI's approach emphasizes collective int...</td>\n",
       "      <td>https://www.promptloop.com/directory/what-does...</td>\n",
       "      <td>cse</td>\n",
       "      <td>0.90</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            run_id         category claim_type  \\\n",
       "0  20260107_225628      positioning       fact   \n",
       "1  20260107_225628  differentiation       fact   \n",
       "\n",
       "                                               claim  \\\n",
       "0  Sakana AI is focused on developing efficient, ...   \n",
       "1  Sakana AI's approach emphasizes collective int...   \n",
       "\n",
       "                                        evidence_url source_type  confidence  \n",
       "0  https://siliconangle.com/2025/11/17/sakana-ai-...         cse        0.95  \n",
       "1  https://www.promptloop.com/directory/what-does...         cse        0.90  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>run_id</th>\n",
       "      <th>content_type</th>\n",
       "      <th>title</th>\n",
       "      <th>publisher_or_platform</th>\n",
       "      <th>date</th>\n",
       "      <th>url</th>\n",
       "      <th>what_it_contains</th>\n",
       "      <th>relevance</th>\n",
       "      <th>confidence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20260107_225628</td>\n",
       "      <td>article</td>\n",
       "      <td>Sakana AI lands $135M on $2.635B valuation to ...</td>\n",
       "      <td>SiliconANGLE</td>\n",
       "      <td>November 17, 2025</td>\n",
       "      <td>https://siliconangle.com/2025/11/17/sakana-ai-...</td>\n",
       "      <td>Details on Sakana AI's funding and strategic f...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20260107_225628</td>\n",
       "      <td>blog</td>\n",
       "      <td>What Does Sakana AI Do? - Company Overview</td>\n",
       "      <td>PromptLoop</td>\n",
       "      <td>January 2025</td>\n",
       "      <td>https://www.promptloop.com/directory/what-does...</td>\n",
       "      <td>Overview of Sakana AI's mission, products, and...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20260107_225628</td>\n",
       "      <td>press</td>\n",
       "      <td>Announcing Our Series A</td>\n",
       "      <td>Sakana AI</td>\n",
       "      <td>September 04, 2024</td>\n",
       "      <td>https://sakana.ai/series-a/</td>\n",
       "      <td>Details on Sakana AI's Series A funding round ...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>20260107_225628</td>\n",
       "      <td>article</td>\n",
       "      <td>Sakana AI Agent Wins AtCoder Heuristic Contest...</td>\n",
       "      <td>Sakana AI</td>\n",
       "      <td>January 05, 2026</td>\n",
       "      <td>https://sakana.ai/blog</td>\n",
       "      <td>Announcement of Sakana AI's agent winning a co...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20260107_225628</td>\n",
       "      <td>article</td>\n",
       "      <td>EDINET-Bench: Evaluating LLMs on Complex Finan...</td>\n",
       "      <td>Sakana AI</td>\n",
       "      <td>June 09, 2025</td>\n",
       "      <td>https://sakana.ai/edinet-bench/</td>\n",
       "      <td>Introduction of a benchmark for evaluating LLM...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.85</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>20260107_225628</td>\n",
       "      <td>article</td>\n",
       "      <td>Population-based Model Merging via Quality Div...</td>\n",
       "      <td>Sakana AI</td>\n",
       "      <td>December 03, 2024</td>\n",
       "      <td>https://sakana.ai/cycleqd/</td>\n",
       "      <td>Research on evolving AI models through populat...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.85</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>20260107_225628</td>\n",
       "      <td>article</td>\n",
       "      <td>The AI Scientist: Towards Fully Automated Open...</td>\n",
       "      <td>Sakana AI</td>\n",
       "      <td>August 13, 2024</td>\n",
       "      <td>https://sakana.ai/ai-scientist/</td>\n",
       "      <td>Overview of Sakana AI's AI Scientist project f...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.85</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>20260107_225628</td>\n",
       "      <td>article</td>\n",
       "      <td>An Evolved Universal Transformer Memory</td>\n",
       "      <td>Sakana AI</td>\n",
       "      <td>December 10, 2024</td>\n",
       "      <td>https://sakana.ai/namm/</td>\n",
       "      <td>Introduction of a new memory system for transf...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.85</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            run_id content_type  \\\n",
       "0  20260107_225628      article   \n",
       "1  20260107_225628         blog   \n",
       "2  20260107_225628        press   \n",
       "3  20260107_225628      article   \n",
       "4  20260107_225628      article   \n",
       "5  20260107_225628      article   \n",
       "6  20260107_225628      article   \n",
       "7  20260107_225628      article   \n",
       "\n",
       "                                               title publisher_or_platform  \\\n",
       "0  Sakana AI lands $135M on $2.635B valuation to ...          SiliconANGLE   \n",
       "1         What Does Sakana AI Do? - Company Overview            PromptLoop   \n",
       "2                            Announcing Our Series A             Sakana AI   \n",
       "3  Sakana AI Agent Wins AtCoder Heuristic Contest...             Sakana AI   \n",
       "4  EDINET-Bench: Evaluating LLMs on Complex Finan...             Sakana AI   \n",
       "5  Population-based Model Merging via Quality Div...             Sakana AI   \n",
       "6  The AI Scientist: Towards Fully Automated Open...             Sakana AI   \n",
       "7            An Evolved Universal Transformer Memory             Sakana AI   \n",
       "\n",
       "                 date                                                url  \\\n",
       "0   November 17, 2025  https://siliconangle.com/2025/11/17/sakana-ai-...   \n",
       "1        January 2025  https://www.promptloop.com/directory/what-does...   \n",
       "2  September 04, 2024                        https://sakana.ai/series-a/   \n",
       "3    January 05, 2026                             https://sakana.ai/blog   \n",
       "4       June 09, 2025                    https://sakana.ai/edinet-bench/   \n",
       "5   December 03, 2024                         https://sakana.ai/cycleqd/   \n",
       "6     August 13, 2024                    https://sakana.ai/ai-scientist/   \n",
       "7   December 10, 2024                            https://sakana.ai/namm/   \n",
       "\n",
       "                                    what_it_contains  relevance  confidence  \n",
       "0  Details on Sakana AI's funding and strategic f...          1        0.95  \n",
       "1  Overview of Sakana AI's mission, products, and...          1        0.90  \n",
       "2  Details on Sakana AI's Series A funding round ...          1        0.90  \n",
       "3  Announcement of Sakana AI's agent winning a co...          1        0.90  \n",
       "4  Introduction of a benchmark for evaluating LLM...          1        0.85  \n",
       "5  Research on evolving AI models through populat...          1        0.85  \n",
       "6  Overview of Sakana AI's AI Scientist project f...          1        0.85  \n",
       "7  Introduction of a new memory system for transf...          1        0.85  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ============================================================\n",
    "# 8. Competitive Landscape (Evidence-linked + Content Index)\n",
    "# ============================================================\n",
    "# Goal:\n",
    "# - Build an evidence-linked view of the competitive landscape:\n",
    "#   - Direct competitors (same category / same buyer)\n",
    "#   - Indirect alternatives (different approach / adjacent category)\n",
    "#   - Positioning comparisons (how the company differentiates, as stated)\n",
    "#   - Category map (where the startup sits relative to known players)\n",
    "# - Identify public narratives: comparisons, analyst notes, interviews, articles.\n",
    "# - Use the same pattern:\n",
    "#   official pages first + CSE enrichment, robots check,\n",
    "#   store raw pages as JSONL, then OpenAI normalizes into:\n",
    "#     1) competitive_summary (single object)\n",
    "#     2) competitor_table (one row per competitor)\n",
    "#     3) competitive_claims (long-form evidence-linked claims)\n",
    "#     4) competitive_content_index (posts/interviews/articles with URLs)\n",
    "#\n",
    "# Outputs:\n",
    "# - competitive_raw_pages_<run_id>.jsonl\n",
    "# - competitive_extraction_<run_id>.json\n",
    "# - competitive_summary_<run_id>.csv\n",
    "# - competitor_table_<run_id>.csv\n",
    "# - competitive_claims_<run_id>.csv\n",
    "# - competitive_content_index_<run_id>.csv\n",
    "\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "import time\n",
    "import requests\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "from pathlib import Path\n",
    "from urllib.parse import urlparse, urljoin\n",
    "from datetime import datetime, timezone\n",
    "from urllib.robotparser import RobotFileParser\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Global configuration\n",
    "# ------------------------------------------------------------\n",
    "load_dotenv(\"env.txt\")\n",
    "\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "if not OPENAI_API_KEY:\n",
    "    raise EnvironmentError(\"OPENAI_API_KEY is not set.\")\n",
    "\n",
    "GOOGLE_API_KEY = os.getenv(\"GOOGLE_API_KEY\")\n",
    "GOOGLE_CSE_CX = os.getenv(\"GOOGLE_CSE_CX\")\n",
    "if not GOOGLE_API_KEY or not GOOGLE_CSE_CX:\n",
    "    raise EnvironmentError(\"GOOGLE_API_KEY and GOOGLE_CSE_CX must be set.\")\n",
    "\n",
    "client = OpenAI(api_key=OPENAI_API_KEY)\n",
    "\n",
    "ART_DIR = Path(\"artifacts\") / \"meeting_deep_dive\"\n",
    "ART_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Load inputs + entity + (optional) product/market artifacts\n",
    "# ------------------------------------------------------------\n",
    "def load_latest_json(pattern: str) -> tuple[dict, Path]:\n",
    "    files = sorted(ART_DIR.glob(pattern), key=lambda p: p.stat().st_mtime, reverse=True)\n",
    "    if not files:\n",
    "        raise FileNotFoundError(f\"No {pattern} found. Please run previous cells.\")\n",
    "    p = files[0]\n",
    "    return json.loads(p.read_text(encoding=\"utf-8\")), p\n",
    "\n",
    "try:\n",
    "    run_id = inputs[\"meta\"][\"run_id\"]\n",
    "except Exception:\n",
    "    inp, _ = load_latest_json(\"inputs_*.json\")\n",
    "    run_id = inp.get(\"meta\", {}).get(\"run_id\") or datetime.now(timezone.utc).strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "entity, _ = load_latest_json(\"entity_*.json\")\n",
    "startup_name = (entity.get(\"canonical_name\") or \"Unknown Startup\").strip()\n",
    "official_website = (entity.get(\"official_website\") or \"\").strip()\n",
    "if not official_website:\n",
    "    raise ValueError(\"official_website is missing. Run cell #2 first.\")\n",
    "\n",
    "# Optional: use known positioning tags from cell #6/#7 if present\n",
    "positioning_tags = []\n",
    "try:\n",
    "    # business_product_summary_*.csv contains positioning_tags column (list-like)\n",
    "    bp_paths = sorted(ART_DIR.glob(\"business_product_summary_*.csv\"), key=lambda p: p.stat().st_mtime, reverse=True)\n",
    "    if bp_paths:\n",
    "        bp = pd.read_csv(bp_paths[0])\n",
    "        if \"positioning_tags\" in bp.columns:\n",
    "            positioning_tags = [str(bp.loc[0, \"positioning_tags\"])]\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Knobs\n",
    "# ------------------------------------------------------------\n",
    "USER_AGENT = \"researchOSv2-bot/0.3 (+contact: internal-research)\"\n",
    "TIMEOUT = 25\n",
    "SLEEP_SEC = 0.25\n",
    "\n",
    "MAX_OFFICIAL_PAGES = 25\n",
    "MAX_CSE_PAGES = 45\n",
    "MAX_TEXT_CHARS_STORE = 70000\n",
    "MAX_TEXT_CHARS_PROMPT = 9000\n",
    "MAX_PAGES_TO_LLM = 24\n",
    "\n",
    "COMP_KEY_PATHS = [\n",
    "    \"/blog\", \"/news\", \"/press\", \"/research\", \"/updates\",\n",
    "    \"/pricing\", \"/docs\", \"/product\", \"/products\", \"/solutions\"\n",
    "]\n",
    "COMP_KEYWORDS = [\n",
    "    \"compare\", \"comparison\", \"vs\", \"alternative\", \"competitor\", \"competition\",\n",
    "    \"pricing\", \"docs\", \"api\", \"blog\", \"press\", \"news\", \"research\", \"product\", \"solutions\"\n",
    "]\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# URL / domain / robots helpers\n",
    "# ------------------------------------------------------------\n",
    "def normalize_url(url: str) -> str:\n",
    "    url = (url or \"\").strip()\n",
    "    if not url:\n",
    "        return \"\"\n",
    "    if not re.match(r\"^https?://\", url, flags=re.I):\n",
    "        url = \"https://\" + url\n",
    "    p = urlparse(url)\n",
    "    return p._replace(netloc=p.netloc.lower(), fragment=\"\").geturl()\n",
    "\n",
    "def get_domain(url: str) -> str:\n",
    "    return re.sub(r\"^www\\.\", \"\", urlparse(normalize_url(url)).netloc)\n",
    "\n",
    "def same_domain(a: str, b: str) -> bool:\n",
    "    da, db = get_domain(a), get_domain(b)\n",
    "    return da == db or da.endswith(\".\" + db) or db.endswith(\".\" + da)\n",
    "\n",
    "def is_asset_url(url: str) -> bool:\n",
    "    path = (urlparse(url).path or \"\").lower()\n",
    "    return bool(re.search(r\"\\.(pdf|jpg|jpeg|png|gif|svg|zip|mp4|mov)$\", path))\n",
    "\n",
    "def build_robot_parser(base_url: str) -> RobotFileParser | None:\n",
    "    rp = RobotFileParser()\n",
    "    rp.set_url(urljoin(normalize_url(base_url), \"/robots.txt\"))\n",
    "    try:\n",
    "        rp.read()\n",
    "        return rp\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "def robots_allows(rp: RobotFileParser | None, url: str) -> bool:\n",
    "    if rp is None:\n",
    "        p = (urlparse(url).path or \"/\").lower()\n",
    "        return p in [\"/\", \"\"] or any(k in p for k in [\"blog\", \"news\", \"press\", \"research\", \"product\", \"pricing\", \"docs\"])\n",
    "    try:\n",
    "        return rp.can_fetch(USER_AGENT, url)\n",
    "    except Exception:\n",
    "        return False\n",
    "\n",
    "_robot_cache: dict[str, RobotFileParser | None] = {}\n",
    "\n",
    "def get_rp_for_url(url: str) -> RobotFileParser | None:\n",
    "    dom = get_domain(url)\n",
    "    if dom in _robot_cache:\n",
    "        return _robot_cache[dom]\n",
    "    rp = build_robot_parser(url)\n",
    "    _robot_cache[dom] = rp\n",
    "    return rp\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Fetch + text extraction\n",
    "# ------------------------------------------------------------\n",
    "def fetch_html(url: str) -> tuple[int, str]:\n",
    "    r = requests.get(url, headers={\"User-Agent\": USER_AGENT}, timeout=TIMEOUT, allow_redirects=True)\n",
    "    return r.status_code, (r.text if r.ok else \"\")\n",
    "\n",
    "def html_to_text(html: str) -> str:\n",
    "    soup = BeautifulSoup(html, \"lxml\")\n",
    "    for tag in soup([\"script\", \"style\", \"noscript\", \"svg\"]):\n",
    "        tag.decompose()\n",
    "    node = soup.find(\"main\") or soup.find(\"article\") or (soup.body if soup.body else soup)\n",
    "    text = node.get_text(\"\\n\", strip=True)\n",
    "    text = re.sub(r\"\\n{3,}\", \"\\n\\n\", text)\n",
    "    return text.strip()\n",
    "\n",
    "def extract_links(base_url: str, html: str) -> list[str]:\n",
    "    soup = BeautifulSoup(html, \"lxml\")\n",
    "    out = []\n",
    "    for a in soup.find_all(\"a\", href=True):\n",
    "        href = a[\"href\"].strip()\n",
    "        if href.startswith(\"#\") or href.lower().startswith(\"mailto:\") or href.lower().startswith(\"tel:\"):\n",
    "            continue\n",
    "        u = normalize_url(urljoin(base_url, href))\n",
    "        if u.startswith((\"http://\", \"https://\")):\n",
    "            out.append(u)\n",
    "    seen, uniq = set(), []\n",
    "    for u in out:\n",
    "        if u not in seen:\n",
    "            seen.add(u)\n",
    "            uniq.append(u)\n",
    "    return uniq\n",
    "\n",
    "def score_competitive_link(url: str) -> float:\n",
    "    \"\"\"\n",
    "    Prefer pages that might contain comparisons, alternatives, or category statements.\n",
    "    \"\"\"\n",
    "    u = normalize_url(url)\n",
    "    p = (urlparse(u).path or \"/\").lower()\n",
    "    score = 0.0\n",
    "    # strong signals\n",
    "    if any(x in p for x in [\"compare\", \"comparison\", \"vs\", \"alternative\", \"competitor\"]):\n",
    "        score += 4.0\n",
    "    # medium signals\n",
    "    for k in [\"pricing\", \"docs\", \"api\", \"blog\", \"press\", \"news\", \"research\", \"product\", \"solutions\"]:\n",
    "        if k in p:\n",
    "            score += 1.0\n",
    "    if is_asset_url(u):\n",
    "        score -= 4.0\n",
    "    if p.count(\"/\") >= 4:\n",
    "        score -= 0.4\n",
    "    return score\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Google CSE\n",
    "# ------------------------------------------------------------\n",
    "def google_cse_search(query: str, num: int = 5) -> list[dict]:\n",
    "    url = \"https://www.googleapis.com/customsearch/v1\"\n",
    "    params = {\"key\": GOOGLE_API_KEY, \"cx\": GOOGLE_CSE_CX, \"q\": query, \"num\": min(max(num, 1), 10)}\n",
    "    r = requests.get(url, params=params, timeout=30)\n",
    "    r.raise_for_status()\n",
    "    data = r.json()\n",
    "    items = data.get(\"items\", []) or []\n",
    "    return [{\n",
    "        \"title\": it.get(\"title\"),\n",
    "        \"link\": normalize_url(it.get(\"link\") or \"\"),\n",
    "        \"snippet\": it.get(\"snippet\"),\n",
    "        \"displayLink\": it.get(\"displayLink\"),\n",
    "    } for it in items]\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Build official targets (light)\n",
    "# ------------------------------------------------------------\n",
    "official_home = normalize_url(official_website)\n",
    "rp_official = build_robot_parser(official_home)\n",
    "\n",
    "status, html = fetch_html(official_home)\n",
    "time.sleep(SLEEP_SEC)\n",
    "if status >= 400 or not html:\n",
    "    raise RuntimeError(f\"Failed to fetch official homepage: HTTP {status}\")\n",
    "\n",
    "home_links = extract_links(official_home, html)\n",
    "internal_links = [l for l in home_links if same_domain(l, official_home) and not is_asset_url(l)]\n",
    "\n",
    "explicit = []\n",
    "for p in COMP_KEY_PATHS:\n",
    "    u = normalize_url(urljoin(official_home, p))\n",
    "    if robots_allows(rp_official, u) and not is_asset_url(u):\n",
    "        explicit.append(u)\n",
    "\n",
    "candidates = list(dict.fromkeys([official_home] + explicit + internal_links))\n",
    "scored = [{\"url\": u, \"score\": score_competitive_link(u)} for u in candidates]\n",
    "scored.sort(key=lambda x: x[\"score\"], reverse=True)\n",
    "\n",
    "official_targets = []\n",
    "for it in scored:\n",
    "    if len(official_targets) >= MAX_OFFICIAL_PAGES:\n",
    "        break\n",
    "    if it[\"score\"] < 0.2:\n",
    "        continue\n",
    "    if robots_allows(rp_official, it[\"url\"]):\n",
    "        official_targets.append(it[\"url\"])\n",
    "\n",
    "official_targets = list(dict.fromkeys(official_targets))\n",
    "print(f\"Official competitive targets: {len(official_targets)}\")\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Build CSE targets (competitors + alternatives + comparisons)\n",
    "# ------------------------------------------------------------\n",
    "tag_hint = \" \".join(positioning_tags) if positioning_tags else \"\"\n",
    "queries = [\n",
    "    f'\"{startup_name}\" competitor OR competitors OR 競合',\n",
    "    f'\"{startup_name}\" alternative OR alternatives OR 代替',\n",
    "    f'\"{startup_name}\" vs',\n",
    "    f'\"{startup_name}\" comparison OR compare',\n",
    "    f'\"{startup_name}\" competitive landscape',\n",
    "    f'\"{startup_name}\" market category {tag_hint}'.strip(),\n",
    "    f'\"{startup_name}\" pricing compare',\n",
    "    f'\"{startup_name}\" feature comparison',\n",
    "    f'\"{startup_name}\" interview competitor',\n",
    "    f'\"{startup_name}\" 評判 OR レビュー OR 比較',\n",
    "]\n",
    "\n",
    "# Optional: if you know the product category keywords, add them here\n",
    "# e.g., queries.append(f'\"{startup_name}\" \"agentic AI\" competitor')\n",
    "\n",
    "cse_items = []\n",
    "for q in queries:\n",
    "    try:\n",
    "        cse_items.extend(google_cse_search(q, num=5))\n",
    "        time.sleep(0.2)\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "seen = set()\n",
    "cse_links = []\n",
    "for it in cse_items:\n",
    "    link = it.get(\"link\")\n",
    "    if link and link not in seen and link.startswith((\"http://\", \"https://\")) and not is_asset_url(link):\n",
    "        seen.add(link)\n",
    "        cse_links.append(link)\n",
    "\n",
    "cse_links = cse_links[:MAX_CSE_PAGES]\n",
    "print(f\"CSE competitive targets: {len(cse_links)}\")\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Fetch + store raw pages (JSONL)\n",
    "# ------------------------------------------------------------\n",
    "RAW_PATH = ART_DIR / f\"competitive_raw_pages_{run_id}.jsonl\"\n",
    "RAW_PATH.write_text(\"\", encoding=\"utf-8\")\n",
    "\n",
    "def append_jsonl(path: Path, obj: dict):\n",
    "    with path.open(\"a\", encoding=\"utf-8\") as f:\n",
    "        f.write(json.dumps(obj, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "def collect_pages(urls: list[str], source_type: str):\n",
    "    for u in urls:\n",
    "        u = normalize_url(u)\n",
    "        if not u or is_asset_url(u):\n",
    "            continue\n",
    "\n",
    "        rp = rp_official if source_type == \"official\" else get_rp_for_url(u)\n",
    "        allowed = robots_allows(rp, u)\n",
    "\n",
    "        if not allowed:\n",
    "            append_jsonl(RAW_PATH, {\n",
    "                \"url\": u,\n",
    "                \"source_type\": source_type,\n",
    "                \"domain\": get_domain(u),\n",
    "                \"fetched_at_utc\": datetime.now(timezone.utc).isoformat(),\n",
    "                \"http_status\": None,\n",
    "                \"robots_allowed\": False,\n",
    "                \"text\": \"\",\n",
    "                \"notes\": \"Skipped due to robots.txt (or conservative fallback).\",\n",
    "            })\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            code, html = fetch_html(u)\n",
    "            time.sleep(SLEEP_SEC)\n",
    "            text = html_to_text(html) if html else \"\"\n",
    "            text = text[:MAX_TEXT_CHARS_STORE]\n",
    "\n",
    "            append_jsonl(RAW_PATH, {\n",
    "                \"url\": u,\n",
    "                \"source_type\": source_type,\n",
    "                \"domain\": get_domain(u),\n",
    "                \"fetched_at_utc\": datetime.now(timezone.utc).isoformat(),\n",
    "                \"http_status\": code,\n",
    "                \"robots_allowed\": True,\n",
    "                \"text\": text,\n",
    "                \"text_char_len\": len(text),\n",
    "            })\n",
    "        except Exception as e:\n",
    "            append_jsonl(RAW_PATH, {\n",
    "                \"url\": u,\n",
    "                \"source_type\": source_type,\n",
    "                \"domain\": get_domain(u),\n",
    "                \"fetched_at_utc\": datetime.now(timezone.utc).isoformat(),\n",
    "                \"http_status\": None,\n",
    "                \"robots_allowed\": True,\n",
    "                \"text\": \"\",\n",
    "                \"notes\": f\"Fetch error: {str(e)}\",\n",
    "            })\n",
    "\n",
    "collect_pages(official_targets, \"official\")\n",
    "collect_pages(cse_links, \"cse\")\n",
    "\n",
    "print(f\"✅ Competitive raw pages saved: {RAW_PATH.as_posix()}\")\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Load raw pages + shortlist to LLM (prefer compare-ish URLs)\n",
    "# ------------------------------------------------------------\n",
    "def load_raw_pages(path: Path) -> list[dict]:\n",
    "    rows = []\n",
    "    for line in path.read_text(encoding=\"utf-8\").splitlines():\n",
    "        if line.strip():\n",
    "            rows.append(json.loads(line))\n",
    "    return rows\n",
    "\n",
    "raw_pages = load_raw_pages(RAW_PATH)\n",
    "\n",
    "usable = [\n",
    "    r for r in raw_pages\n",
    "    if r.get(\"robots_allowed\")\n",
    "    and isinstance(r.get(\"http_status\"), int) and r[\"http_status\"] < 400\n",
    "    and (r.get(\"text\") or \"\").strip()\n",
    "]\n",
    "\n",
    "def score_page_for_competition(r):\n",
    "    url = r.get(\"url\") or \"\"\n",
    "    text = r.get(\"text\") or \"\"\n",
    "    p = (urlparse(url).path or \"\").lower()\n",
    "    s = 0\n",
    "    for k in [\"compare\", \"comparison\", \"vs\", \"alternative\", \"competitor\", \"pricing\"]:\n",
    "        if k in p:\n",
    "            s += 3\n",
    "    if re.search(r\"\\b(vs\\.?|versus|alternative|competitor|competition|rival)\\b\", text, flags=re.IGNORECASE):\n",
    "        s += 2\n",
    "    s += min(len(text), 20000) / 7000\n",
    "    if r.get(\"source_type\") == \"official\":\n",
    "        s += 2\n",
    "    return s\n",
    "\n",
    "usable_sorted = sorted(usable, key=score_page_for_competition, reverse=True)\n",
    "shortlist = usable_sorted[:MAX_PAGES_TO_LLM]\n",
    "print(f\"Usable pages: {len(usable)} | Shortlist to LLM: {len(shortlist)}\")\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Robust JSON parsing helper\n",
    "# ------------------------------------------------------------\n",
    "def _extract_json_object(text: str) -> str:\n",
    "    if not text:\n",
    "        return \"\"\n",
    "    t = text.strip()\n",
    "    m = re.search(r\"```(?:json)?\\s*(\\{.*?\\})\\s*```\", t, flags=re.DOTALL | re.IGNORECASE)\n",
    "    if m:\n",
    "        return m.group(1).strip()\n",
    "    start, end = t.find(\"{\"), t.rfind(\"}\")\n",
    "    if start != -1 and end != -1 and end > start:\n",
    "        return t[start:end+1].strip()\n",
    "    return \"\"\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# OpenAI: competitive summary + competitor table + claims + content index\n",
    "# ------------------------------------------------------------\n",
    "def openai_extract_competitive(pages: list[dict]) -> dict:\n",
    "    system = (\n",
    "        \"You are a research analyst assistant. \"\n",
    "        \"From the provided webpage texts, extract an evidence-linked competitive landscape. \"\n",
    "        \"CRITICAL: Always produce a non-empty content_index if there are relevant pages. \"\n",
    "        \"Only include competitors/alternatives if supported by an evidence_url from the provided pages. \"\n",
    "        \"Return a single JSON object only (no markdown).\"\n",
    "    )\n",
    "\n",
    "    schema = {\n",
    "        \"competitive_summary\": {\n",
    "            \"company_name\": \"string\",\n",
    "            \"category_description\": \"string|null\",\n",
    "            \"positioning_statements\": \"array of strings\",\n",
    "            \"differentiation_points\": \"array of strings\",\n",
    "            \"competitive_dynamics_notes\": \"array of strings\"\n",
    "        },\n",
    "        \"competitors\": [\n",
    "            {\n",
    "                \"name\": \"string\",\n",
    "                \"type\": \"direct|indirect|adjacent|incumbent|open_source|other\",\n",
    "                \"why_it_is_a_competitor\": \"string\",\n",
    "                \"comparison_points\": \"array of strings\",\n",
    "                \"evidence_url\": \"string\",\n",
    "                \"confidence\": \"0..1\"\n",
    "            }\n",
    "        ],\n",
    "        \"competitive_claims\": [\n",
    "            {\n",
    "                \"category\": \"competitor|alternative|positioning|differentiation|category|other\",\n",
    "                \"claim_type\": \"fact|hypothesis\",\n",
    "                \"claim\": \"string\",\n",
    "                \"evidence_url\": \"string\",\n",
    "                \"source_type\": \"official|cse\",\n",
    "                \"confidence\": \"0..1\"\n",
    "            }\n",
    "        ],\n",
    "        \"content_index\": [\n",
    "            {\n",
    "                \"content_type\": \"article|interview|blog|press|comparison|review|other\",\n",
    "                \"title\": \"string|null\",\n",
    "                \"publisher_or_platform\": \"string|null\",\n",
    "                \"date\": \"string|null\",\n",
    "                \"url\": \"string\",\n",
    "                \"what_it_contains\": \"string\",\n",
    "                \"relevance\": \"0..1\",\n",
    "                \"confidence\": \"0..1\"\n",
    "            }\n",
    "        ],\n",
    "        \"notes\": {\"data_gaps\": \"array of strings\", \"conflicts\": \"array of strings\"},\n",
    "        \"confidence\": {\"overall\": \"0..1\", \"rationale\": \"string\"}\n",
    "    }\n",
    "\n",
    "    bundle = {\n",
    "        \"startup_name\": startup_name,\n",
    "        \"official_website\": official_home,\n",
    "        \"pages\": [\n",
    "            {\n",
    "                \"url\": p[\"url\"],\n",
    "                \"source_type\": p.get(\"source_type\"),\n",
    "                \"domain\": p.get(\"domain\"),\n",
    "                \"text\": (p.get(\"text\") or \"\")[:MAX_TEXT_CHARS_PROMPT],\n",
    "            } for p in pages\n",
    "        ],\n",
    "        \"schema\": schema,\n",
    "        \"instructions\": [\n",
    "            \"Prefer official sources where possible for positioning statements.\",\n",
    "            \"For competitors, include evidence_url that actually names the competitor or implies it clearly.\",\n",
    "            \"For content_index, include at least 8 items if available.\"\n",
    "        ]\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        resp = client.chat.completions.create(\n",
    "            model=\"gpt-4o-mini\",\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": system},\n",
    "                {\"role\": \"user\", \"content\": json.dumps(bundle, ensure_ascii=False)},\n",
    "            ],\n",
    "            temperature=0.0,\n",
    "            response_format={\"type\": \"json_object\"},\n",
    "        )\n",
    "        return json.loads((resp.choices[0].message.content or \"\").strip())\n",
    "    except Exception:\n",
    "        resp2 = client.chat.completions.create(\n",
    "            model=\"gpt-4o-mini\",\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": system},\n",
    "                {\"role\": \"user\", \"content\": json.dumps(bundle, ensure_ascii=False)},\n",
    "            ],\n",
    "            temperature=0.0,\n",
    "        )\n",
    "        raw = resp2.choices[0].message.content or \"\"\n",
    "        candidate = _extract_json_object(raw)\n",
    "        if candidate:\n",
    "            return json.loads(candidate)\n",
    "        raise RuntimeError(\"Model did not return a JSON object.\")\n",
    "\n",
    "extracted = openai_extract_competitive(shortlist)\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Save JSON + build DataFrames\n",
    "# ------------------------------------------------------------\n",
    "JSON_PATH = ART_DIR / f\"competitive_extraction_{run_id}.json\"\n",
    "JSON_PATH.write_text(json.dumps(extracted, ensure_ascii=False, indent=2), encoding=\"utf-8\")\n",
    "\n",
    "summary = extracted.get(\"competitive_summary\", {}) or {}\n",
    "competitors = extracted.get(\"competitors\", []) or []\n",
    "claims = extracted.get(\"competitive_claims\", []) or []\n",
    "content = extracted.get(\"content_index\", []) or []\n",
    "\n",
    "df_summary = pd.DataFrame([{\"run_id\": run_id, **summary}])\n",
    "df_competitors = pd.DataFrame(competitors)\n",
    "df_claims = pd.DataFrame(claims)\n",
    "df_content = pd.DataFrame(content)\n",
    "\n",
    "for df in [df_competitors, df_claims, df_content]:\n",
    "    if not df.empty:\n",
    "        df.insert(0, \"run_id\", run_id)\n",
    "\n",
    "SUMMARY_CSV = ART_DIR / f\"competitive_summary_{run_id}.csv\"\n",
    "COMP_CSV = ART_DIR / f\"competitor_table_{run_id}.csv\"\n",
    "CLAIMS_CSV = ART_DIR / f\"competitive_claims_{run_id}.csv\"\n",
    "CONTENT_CSV = ART_DIR / f\"competitive_content_index_{run_id}.csv\"\n",
    "\n",
    "df_summary.to_csv(SUMMARY_CSV, index=False)\n",
    "df_competitors.to_csv(COMP_CSV, index=False)\n",
    "df_claims.to_csv(CLAIMS_CSV, index=False)\n",
    "df_content.to_csv(CONTENT_CSV, index=False)\n",
    "\n",
    "print(\"✅ Competitive Landscape complete\")\n",
    "print(f\"- Raw pages: {RAW_PATH.as_posix()}\")\n",
    "print(f\"- JSON: {JSON_PATH.as_posix()}\")\n",
    "print(f\"- Summary CSV: {SUMMARY_CSV.as_posix()}\")\n",
    "print(f\"- Competitors CSV: {COMP_CSV.as_posix()}\")\n",
    "print(f\"- Claims CSV: {CLAIMS_CSV.as_posix()}\")\n",
    "print(f\"- Content CSV: {CONTENT_CSV.as_posix()}\")\n",
    "\n",
    "display(df_summary)\n",
    "display(df_competitors.head(30) if not df_competitors.empty else df_competitors)\n",
    "display(df_claims.head(30) if not df_claims.empty else df_claims)\n",
    "display(df_content.head(30) if not df_content.empty else df_content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "849cad61-4faa-4ec0-86bb-2d23dea60011",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Official funding targets: 10\n",
      "CSE funding targets: 31\n",
      "✅ Funding raw pages saved: artifacts/meeting_deep_dive/funding_raw_pages_20260107_225628.jsonl\n",
      "Usable pages: 23 | Shortlist to LLM: 23\n",
      "✅ Funding & Cap Table Signals complete\n",
      "- Raw pages: artifacts/meeting_deep_dive/funding_raw_pages_20260107_225628.jsonl\n",
      "- JSON: artifacts/meeting_deep_dive/funding_extraction_20260107_225628.json\n",
      "- Summary CSV: artifacts/meeting_deep_dive/funding_summary_20260107_225628.csv\n",
      "- Events CSV: artifacts/meeting_deep_dive/funding_events_20260107_225628.csv\n",
      "- Investor mentions CSV: artifacts/meeting_deep_dive/investor_mentions_20260107_225628.csv\n",
      "- Cap table signals CSV: artifacts/meeting_deep_dive/cap_table_signals_20260107_225628.csv\n",
      "- Claims CSV: artifacts/meeting_deep_dive/funding_claims_20260107_225628.csv\n",
      "- Content CSV: artifacts/meeting_deep_dive/funding_content_index_20260107_225628.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>run_id</th>\n",
       "      <th>company_name</th>\n",
       "      <th>latest_round_hint</th>\n",
       "      <th>funding_stage_hint</th>\n",
       "      <th>total_funding_mentioned</th>\n",
       "      <th>valuation_signals</th>\n",
       "      <th>runway_burn_signals</th>\n",
       "      <th>notable_investors</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20260107_225628</td>\n",
       "      <td>Sakana AI</td>\n",
       "      <td>Series B</td>\n",
       "      <td>Growth</td>\n",
       "      <td>[¥52 billion (approximately $347 million), ¥20...</td>\n",
       "      <td>[Post-money valuation of approximately ¥400 bi...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[Mitsubishi UFJ Financial Group (MUFG), Khosla...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            run_id company_name latest_round_hint funding_stage_hint  \\\n",
       "0  20260107_225628    Sakana AI          Series B             Growth   \n",
       "\n",
       "                             total_funding_mentioned  \\\n",
       "0  [¥52 billion (approximately $347 million), ¥20...   \n",
       "\n",
       "                                   valuation_signals runway_burn_signals  \\\n",
       "0  [Post-money valuation of approximately ¥400 bi...                  []   \n",
       "\n",
       "                                   notable_investors  \n",
       "0  [Mitsubishi UFJ Financial Group (MUFG), Khosla...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>run_id</th>\n",
       "      <th>round_label</th>\n",
       "      <th>date</th>\n",
       "      <th>amount</th>\n",
       "      <th>lead_investor</th>\n",
       "      <th>investors_participants</th>\n",
       "      <th>source_url</th>\n",
       "      <th>confidence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20260107_225628</td>\n",
       "      <td>Series B</td>\n",
       "      <td>November 17, 2025</td>\n",
       "      <td>¥20 billion (approximately $135 million)</td>\n",
       "      <td>None</td>\n",
       "      <td>[Mitsubishi UFJ Financial Group (MUFG), Khosla...</td>\n",
       "      <td>https://techcrunch.com/2025/11/17/sakana-ai-ra...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20260107_225628</td>\n",
       "      <td>Series A</td>\n",
       "      <td>September 4, 2024</td>\n",
       "      <td>¥30 billion (approximately $214 million)</td>\n",
       "      <td>New Enterprise Associates</td>\n",
       "      <td>[Khosla Ventures, Lux Capital, NVIDIA, Mitsubi...</td>\n",
       "      <td>https://sakana.ai/series-a/</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20260107_225628</td>\n",
       "      <td>Seed Round</td>\n",
       "      <td>January 16, 2024</td>\n",
       "      <td>¥4.5 billion (approximately $30 million)</td>\n",
       "      <td>Lux Capital</td>\n",
       "      <td>[Khosla Ventures, NTT Group, KDDI CVC, Sony Gr...</td>\n",
       "      <td>https://sakana.ai/seed-round/</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            run_id round_label               date  \\\n",
       "0  20260107_225628    Series B  November 17, 2025   \n",
       "1  20260107_225628    Series A  September 4, 2024   \n",
       "2  20260107_225628  Seed Round   January 16, 2024   \n",
       "\n",
       "                                     amount              lead_investor  \\\n",
       "0  ¥20 billion (approximately $135 million)                       None   \n",
       "1  ¥30 billion (approximately $214 million)  New Enterprise Associates   \n",
       "2  ¥4.5 billion (approximately $30 million)                Lux Capital   \n",
       "\n",
       "                              investors_participants  \\\n",
       "0  [Mitsubishi UFJ Financial Group (MUFG), Khosla...   \n",
       "1  [Khosla Ventures, Lux Capital, NVIDIA, Mitsubi...   \n",
       "2  [Khosla Ventures, NTT Group, KDDI CVC, Sony Gr...   \n",
       "\n",
       "                                          source_url  confidence  \n",
       "0  https://techcrunch.com/2025/11/17/sakana-ai-ra...           1  \n",
       "1                        https://sakana.ai/series-a/           1  \n",
       "2                      https://sakana.ai/seed-round/           1  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>run_id</th>\n",
       "      <th>investor_name</th>\n",
       "      <th>mention_context</th>\n",
       "      <th>type_hint</th>\n",
       "      <th>source_url</th>\n",
       "      <th>confidence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20260107_225628</td>\n",
       "      <td>Mitsubishi UFJ Financial Group (MUFG)</td>\n",
       "      <td>Participated in Series B and Series A funding ...</td>\n",
       "      <td>strategic</td>\n",
       "      <td>https://techcrunch.com/2025/11/17/sakana-ai-ra...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20260107_225628</td>\n",
       "      <td>Khosla Ventures</td>\n",
       "      <td>Participated in Series B, Series A, and Seed R...</td>\n",
       "      <td>lead</td>\n",
       "      <td>https://techcrunch.com/2025/11/17/sakana-ai-ra...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20260107_225628</td>\n",
       "      <td>New Enterprise Associates (NEA)</td>\n",
       "      <td>Lead investor in Series A.</td>\n",
       "      <td>lead</td>\n",
       "      <td>https://sakana.ai/series-a/</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>20260107_225628</td>\n",
       "      <td>Lux Capital</td>\n",
       "      <td>Lead investor in Seed Round and participated i...</td>\n",
       "      <td>lead</td>\n",
       "      <td>https://sakana.ai/seed-round/</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20260107_225628</td>\n",
       "      <td>In-Q-Tel</td>\n",
       "      <td>Participated in Series B.</td>\n",
       "      <td>strategic</td>\n",
       "      <td>https://techcrunch.com/2025/11/17/sakana-ai-ra...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>20260107_225628</td>\n",
       "      <td>Factorial Funds</td>\n",
       "      <td>Participated in Series B.</td>\n",
       "      <td>participant</td>\n",
       "      <td>https://thesaasnews.com/news/sakana-ai-secures...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>20260107_225628</td>\n",
       "      <td>Macquarie Capital</td>\n",
       "      <td>Participated in Series B.</td>\n",
       "      <td>participant</td>\n",
       "      <td>https://thesaasnews.com/news/sakana-ai-secures...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>20260107_225628</td>\n",
       "      <td>Mouro Capital</td>\n",
       "      <td>Participated in Series B.</td>\n",
       "      <td>participant</td>\n",
       "      <td>https://thesaasnews.com/news/sakana-ai-secures...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>20260107_225628</td>\n",
       "      <td>Geodesic Capital</td>\n",
       "      <td>Participated in Series B.</td>\n",
       "      <td>participant</td>\n",
       "      <td>https://thesaasnews.com/news/sakana-ai-secures...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>20260107_225628</td>\n",
       "      <td>Ora Global</td>\n",
       "      <td>Participated in Series B.</td>\n",
       "      <td>participant</td>\n",
       "      <td>https://thesaasnews.com/news/sakana-ai-secures...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>20260107_225628</td>\n",
       "      <td>MPower Partners</td>\n",
       "      <td>Participated in Series B.</td>\n",
       "      <td>strategic</td>\n",
       "      <td>https://prtimes.jp/main/html/rd/p/000000039.00...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>20260107_225628</td>\n",
       "      <td>Shikoku Electric Power</td>\n",
       "      <td>Participated in Series B.</td>\n",
       "      <td>strategic</td>\n",
       "      <td>https://thesaasnews.com/news/sakana-ai-secures...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             run_id                          investor_name  \\\n",
       "0   20260107_225628  Mitsubishi UFJ Financial Group (MUFG)   \n",
       "1   20260107_225628                        Khosla Ventures   \n",
       "2   20260107_225628        New Enterprise Associates (NEA)   \n",
       "3   20260107_225628                            Lux Capital   \n",
       "4   20260107_225628                               In-Q-Tel   \n",
       "5   20260107_225628                        Factorial Funds   \n",
       "6   20260107_225628                      Macquarie Capital   \n",
       "7   20260107_225628                          Mouro Capital   \n",
       "8   20260107_225628                       Geodesic Capital   \n",
       "9   20260107_225628                             Ora Global   \n",
       "10  20260107_225628                        MPower Partners   \n",
       "11  20260107_225628                 Shikoku Electric Power   \n",
       "\n",
       "                                      mention_context    type_hint  \\\n",
       "0   Participated in Series B and Series A funding ...    strategic   \n",
       "1   Participated in Series B, Series A, and Seed R...         lead   \n",
       "2                          Lead investor in Series A.         lead   \n",
       "3   Lead investor in Seed Round and participated i...         lead   \n",
       "4                           Participated in Series B.    strategic   \n",
       "5                           Participated in Series B.  participant   \n",
       "6                           Participated in Series B.  participant   \n",
       "7                           Participated in Series B.  participant   \n",
       "8                           Participated in Series B.  participant   \n",
       "9                           Participated in Series B.  participant   \n",
       "10                          Participated in Series B.    strategic   \n",
       "11                          Participated in Series B.    strategic   \n",
       "\n",
       "                                           source_url  confidence  \n",
       "0   https://techcrunch.com/2025/11/17/sakana-ai-ra...           1  \n",
       "1   https://techcrunch.com/2025/11/17/sakana-ai-ra...           1  \n",
       "2                         https://sakana.ai/series-a/           1  \n",
       "3                       https://sakana.ai/seed-round/           1  \n",
       "4   https://techcrunch.com/2025/11/17/sakana-ai-ra...           1  \n",
       "5   https://thesaasnews.com/news/sakana-ai-secures...           1  \n",
       "6   https://thesaasnews.com/news/sakana-ai-secures...           1  \n",
       "7   https://thesaasnews.com/news/sakana-ai-secures...           1  \n",
       "8   https://thesaasnews.com/news/sakana-ai-secures...           1  \n",
       "9   https://thesaasnews.com/news/sakana-ai-secures...           1  \n",
       "10  https://prtimes.jp/main/html/rd/p/000000039.00...           1  \n",
       "11  https://thesaasnews.com/news/sakana-ai-secures...           1  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>run_id</th>\n",
       "      <th>signal_type</th>\n",
       "      <th>signal</th>\n",
       "      <th>source_url</th>\n",
       "      <th>confidence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20260107_225628</td>\n",
       "      <td>ownership_hint</td>\n",
       "      <td>NTT Group became the largest shareholder in Sa...</td>\n",
       "      <td>https://nttdocomo-v.com/en/news/ibcqisb5bp/</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            run_id     signal_type  \\\n",
       "0  20260107_225628  ownership_hint   \n",
       "\n",
       "                                              signal  \\\n",
       "0  NTT Group became the largest shareholder in Sa...   \n",
       "\n",
       "                                    source_url  confidence  \n",
       "0  https://nttdocomo-v.com/en/news/ibcqisb5bp/           1  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: []\n",
       "Index: []"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>run_id</th>\n",
       "      <th>content_type</th>\n",
       "      <th>title</th>\n",
       "      <th>publisher_or_platform</th>\n",
       "      <th>date</th>\n",
       "      <th>url</th>\n",
       "      <th>what_it_contains</th>\n",
       "      <th>relevance</th>\n",
       "      <th>confidence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20260107_225628</td>\n",
       "      <td>article</td>\n",
       "      <td>Sakana AI Raises $135M as Japan Pushes Soverei...</td>\n",
       "      <td>The Corporate World</td>\n",
       "      <td>December 4, 2025</td>\n",
       "      <td>https://www.thecorporate.world/post/sakana-ai-...</td>\n",
       "      <td>Details on Sakana AI's Series B funding and it...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20260107_225628</td>\n",
       "      <td>article</td>\n",
       "      <td>Sakana AI lands $135M on $2.635B valuation to ...</td>\n",
       "      <td>SiliconANGLE</td>\n",
       "      <td>November 17, 2025</td>\n",
       "      <td>https://siliconangle.com/2025/11/17/sakana-ai-...</td>\n",
       "      <td>Information about Sakana AI's funding round an...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20260107_225628</td>\n",
       "      <td>article</td>\n",
       "      <td>Sakana AI secures $100m in Series A funding round</td>\n",
       "      <td>Yahoo Finance</td>\n",
       "      <td>September 5, 2024</td>\n",
       "      <td>https://finance.yahoo.com/news/japan-sakana-ai...</td>\n",
       "      <td>Overview of Sakana AI's Series A funding and s...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>20260107_225628</td>\n",
       "      <td>article</td>\n",
       "      <td>Sakana AI announces Series A funding</td>\n",
       "      <td>Sakana AI Official</td>\n",
       "      <td>September 4, 2024</td>\n",
       "      <td>https://sakana.ai/series-a/</td>\n",
       "      <td>Details on the Series A funding round and inve...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20260107_225628</td>\n",
       "      <td>article</td>\n",
       "      <td>Sakana AI raises $30M to develop nature-inspir...</td>\n",
       "      <td>Sakana AI Official</td>\n",
       "      <td>January 16, 2024</td>\n",
       "      <td>https://sakana.ai/seed-round/</td>\n",
       "      <td>Information on the seed funding round and init...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            run_id content_type  \\\n",
       "0  20260107_225628      article   \n",
       "1  20260107_225628      article   \n",
       "2  20260107_225628      article   \n",
       "3  20260107_225628      article   \n",
       "4  20260107_225628      article   \n",
       "\n",
       "                                               title publisher_or_platform  \\\n",
       "0  Sakana AI Raises $135M as Japan Pushes Soverei...   The Corporate World   \n",
       "1  Sakana AI lands $135M on $2.635B valuation to ...          SiliconANGLE   \n",
       "2  Sakana AI secures $100m in Series A funding round         Yahoo Finance   \n",
       "3               Sakana AI announces Series A funding    Sakana AI Official   \n",
       "4  Sakana AI raises $30M to develop nature-inspir...    Sakana AI Official   \n",
       "\n",
       "                date                                                url  \\\n",
       "0   December 4, 2025  https://www.thecorporate.world/post/sakana-ai-...   \n",
       "1  November 17, 2025  https://siliconangle.com/2025/11/17/sakana-ai-...   \n",
       "2  September 5, 2024  https://finance.yahoo.com/news/japan-sakana-ai...   \n",
       "3  September 4, 2024                        https://sakana.ai/series-a/   \n",
       "4   January 16, 2024                      https://sakana.ai/seed-round/   \n",
       "\n",
       "                                    what_it_contains  relevance  confidence  \n",
       "0  Details on Sakana AI's Series B funding and it...          1           1  \n",
       "1  Information about Sakana AI's funding round an...          1           1  \n",
       "2  Overview of Sakana AI's Series A funding and s...          1           1  \n",
       "3  Details on the Series A funding round and inve...          1           1  \n",
       "4  Information on the seed funding round and init...          1           1  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ============================================================\n",
    "# 9. Funding & Cap Table Signals (Evidence-linked + Content Index)\n",
    "# ============================================================\n",
    "# Goal:\n",
    "# - Build an evidence-linked view of funding & cap table signals:\n",
    "#   - Funding events (round type, amount, date, lead/participants) when mentioned\n",
    "#   - Investor names and roles (lead/participant/strategic) when mentioned\n",
    "#   - Cap table signals (ownership hints, board seats, strategic partners) when mentioned\n",
    "#   - Hiring / runway / burn / revenue signals when mentioned (as weak proxies)\n",
    "# - Identify public narratives: funding announcements, interviews, articles, filings.\n",
    "# - Use the same pattern:\n",
    "#   official pages first + CSE enrichment, robots check,\n",
    "#   store raw pages as JSONL, then OpenAI normalizes into:\n",
    "#     1) funding_summary (single object)\n",
    "#     2) funding_events (table)\n",
    "#     3) investor_mentions (table)\n",
    "#     4) cap_table_signals (table)\n",
    "#     5) funding_claims (evidence-linked)\n",
    "#     6) funding_content_index (posts/interviews/articles with URLs)\n",
    "#\n",
    "# Outputs:\n",
    "# - funding_raw_pages_<run_id>.jsonl\n",
    "# - funding_extraction_<run_id>.json\n",
    "# - funding_summary_<run_id>.csv\n",
    "# - funding_events_<run_id>.csv\n",
    "# - investor_mentions_<run_id>.csv\n",
    "# - cap_table_signals_<run_id>.csv\n",
    "# - funding_claims_<run_id>.csv\n",
    "# - funding_content_index_<run_id>.csv\n",
    "\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "import time\n",
    "import requests\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "from pathlib import Path\n",
    "from urllib.parse import urlparse, urljoin\n",
    "from datetime import datetime, timezone\n",
    "from urllib.robotparser import RobotFileParser\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Global configuration\n",
    "# ------------------------------------------------------------\n",
    "load_dotenv(\"env.txt\")\n",
    "\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "if not OPENAI_API_KEY:\n",
    "    raise EnvironmentError(\"OPENAI_API_KEY is not set.\")\n",
    "\n",
    "GOOGLE_API_KEY = os.getenv(\"GOOGLE_API_KEY\")\n",
    "GOOGLE_CSE_CX = os.getenv(\"GOOGLE_CSE_CX\")\n",
    "if not GOOGLE_API_KEY or not GOOGLE_CSE_CX:\n",
    "    raise EnvironmentError(\"GOOGLE_API_KEY and GOOGLE_CSE_CX must be set in env.txt.\")\n",
    "\n",
    "client = OpenAI(api_key=OPENAI_API_KEY)\n",
    "\n",
    "ART_DIR = Path(\"artifacts\") / \"meeting_deep_dive\"\n",
    "ART_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Load inputs + entity\n",
    "# ------------------------------------------------------------\n",
    "def load_latest_json(pattern: str) -> tuple[dict, Path]:\n",
    "    files = sorted(ART_DIR.glob(pattern), key=lambda p: p.stat().st_mtime, reverse=True)\n",
    "    if not files:\n",
    "        raise FileNotFoundError(f\"No {pattern} found. Please run previous cells.\")\n",
    "    p = files[0]\n",
    "    return json.loads(p.read_text(encoding=\"utf-8\")), p\n",
    "\n",
    "try:\n",
    "    run_id = inputs[\"meta\"][\"run_id\"]\n",
    "except Exception:\n",
    "    inp, _ = load_latest_json(\"inputs_*.json\")\n",
    "    run_id = inp.get(\"meta\", {}).get(\"run_id\") or datetime.now(timezone.utc).strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "entity, _ = load_latest_json(\"entity_*.json\")\n",
    "startup_name = (entity.get(\"canonical_name\") or \"Unknown Startup\").strip()\n",
    "official_website = (entity.get(\"official_website\") or \"\").strip()\n",
    "if not official_website:\n",
    "    raise ValueError(\"official_website is missing. Run cell #2 first.\")\n",
    "\n",
    "# Optional: use Company Basics extraction as hints\n",
    "funding_hint_urls = []\n",
    "try:\n",
    "    cb_paths = sorted(ART_DIR.glob(\"company_claims_*.csv\"), key=lambda p: p.stat().st_mtime, reverse=True)\n",
    "    if cb_paths:\n",
    "        df_cb = pd.read_csv(cb_paths[0])\n",
    "        # Keep only funding-related claims evidence URLs\n",
    "        if \"category\" in df_cb.columns and \"evidence_url\" in df_cb.columns:\n",
    "            funding_hint_urls = df_cb[df_cb[\"category\"].astype(str).str.contains(\"funding\", case=False, na=False)][\"evidence_url\"].dropna().unique().tolist()\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Knobs\n",
    "# ------------------------------------------------------------\n",
    "USER_AGENT = \"researchOSv2-bot/0.3 (+contact: internal-research)\"\n",
    "TIMEOUT = 25\n",
    "SLEEP_SEC = 0.25\n",
    "\n",
    "MAX_OFFICIAL_PAGES = 25\n",
    "MAX_CSE_PAGES = 50\n",
    "MAX_TEXT_CHARS_STORE = 70000\n",
    "MAX_TEXT_CHARS_PROMPT = 9000\n",
    "MAX_PAGES_TO_LLM = 26\n",
    "\n",
    "FUND_KEY_PATHS = [\n",
    "    \"/press\", \"/news\", \"/blog\", \"/updates\",\n",
    "    \"/investor\", \"/investors\", \"/funding\", \"/ir\",\n",
    "]\n",
    "FUND_KEYWORDS = [\n",
    "    \"seed\", \"series\", \"funding\", \"raised\", \"round\", \"investment\", \"investor\",\n",
    "    \"press\", \"news\", \"blog\", \"ir\", \"investor\"\n",
    "]\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# URL / domain / robots helpers\n",
    "# ------------------------------------------------------------\n",
    "def normalize_url(url: str) -> str:\n",
    "    url = (url or \"\").strip()\n",
    "    if not url:\n",
    "        return \"\"\n",
    "    if not re.match(r\"^https?://\", url, flags=re.I):\n",
    "        url = \"https://\" + url\n",
    "    p = urlparse(url)\n",
    "    return p._replace(netloc=p.netloc.lower(), fragment=\"\").geturl()\n",
    "\n",
    "def get_domain(url: str) -> str:\n",
    "    return re.sub(r\"^www\\.\", \"\", urlparse(normalize_url(url)).netloc)\n",
    "\n",
    "def same_domain(a: str, b: str) -> bool:\n",
    "    da, db = get_domain(a), get_domain(b)\n",
    "    return da == db or da.endswith(\".\" + db) or db.endswith(\".\" + da)\n",
    "\n",
    "def is_asset_url(url: str) -> bool:\n",
    "    path = (urlparse(url).path or \"\").lower()\n",
    "    return bool(re.search(r\"\\.(pdf|jpg|jpeg|png|gif|svg|zip|mp4|mov)$\", path))\n",
    "\n",
    "def build_robot_parser(base_url: str) -> RobotFileParser | None:\n",
    "    rp = RobotFileParser()\n",
    "    rp.set_url(urljoin(normalize_url(base_url), \"/robots.txt\"))\n",
    "    try:\n",
    "        rp.read()\n",
    "        return rp\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "def robots_allows(rp: RobotFileParser | None, url: str) -> bool:\n",
    "    if rp is None:\n",
    "        p = (urlparse(url).path or \"/\").lower()\n",
    "        return p in [\"/\", \"\"] or any(k in p for k in FUND_KEYWORDS)\n",
    "    try:\n",
    "        return rp.can_fetch(USER_AGENT, url)\n",
    "    except Exception:\n",
    "        return False\n",
    "\n",
    "_robot_cache: dict[str, RobotFileParser | None] = {}\n",
    "\n",
    "def get_rp_for_url(url: str) -> RobotFileParser | None:\n",
    "    dom = get_domain(url)\n",
    "    if dom in _robot_cache:\n",
    "        return _robot_cache[dom]\n",
    "    rp = build_robot_parser(url)\n",
    "    _robot_cache[dom] = rp\n",
    "    return rp\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Fetch + text extraction\n",
    "# ------------------------------------------------------------\n",
    "def fetch_html(url: str) -> tuple[int, str]:\n",
    "    r = requests.get(url, headers={\"User-Agent\": USER_AGENT}, timeout=TIMEOUT, allow_redirects=True)\n",
    "    return r.status_code, (r.text if r.ok else \"\")\n",
    "\n",
    "def html_to_text(html: str) -> str:\n",
    "    soup = BeautifulSoup(html, \"lxml\")\n",
    "    for tag in soup([\"script\", \"style\", \"noscript\", \"svg\"]):\n",
    "        tag.decompose()\n",
    "    node = soup.find(\"main\") or soup.find(\"article\") or (soup.body if soup.body else soup)\n",
    "    text = node.get_text(\"\\n\", strip=True)\n",
    "    text = re.sub(r\"\\n{3,}\", \"\\n\\n\", text)\n",
    "    return text.strip()\n",
    "\n",
    "def extract_links(base_url: str, html: str) -> list[str]:\n",
    "    soup = BeautifulSoup(html, \"lxml\")\n",
    "    out = []\n",
    "    for a in soup.find_all(\"a\", href=True):\n",
    "        href = a[\"href\"].strip()\n",
    "        if href.startswith(\"#\") or href.lower().startswith(\"mailto:\") or href.lower().startswith(\"tel:\"):\n",
    "            continue\n",
    "        u = normalize_url(urljoin(base_url, href))\n",
    "        if u.startswith((\"http://\", \"https://\")):\n",
    "            out.append(u)\n",
    "    seen, uniq = set(), []\n",
    "    for u in out:\n",
    "        if u not in seen:\n",
    "            seen.add(u)\n",
    "            uniq.append(u)\n",
    "    return uniq\n",
    "\n",
    "def score_funding_link(url: str) -> float:\n",
    "    p = (urlparse(url).path or \"/\").lower()\n",
    "    score = 0.0\n",
    "    # funding-ish\n",
    "    if any(k in p for k in [\"funding\", \"invest\", \"investor\", \"ir\", \"seed\", \"series\", \"round\"]):\n",
    "        score += 3.0\n",
    "    # press/news\n",
    "    for k in [\"press\", \"news\", \"blog\", \"updates\"]:\n",
    "        if k in p:\n",
    "            score += 1.0\n",
    "    if is_asset_url(url):\n",
    "        score -= 4.0\n",
    "    if p.count(\"/\") >= 4:\n",
    "        score -= 0.4\n",
    "    return score\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Google CSE\n",
    "# ------------------------------------------------------------\n",
    "def google_cse_search(query: str, num: int = 5) -> list[dict]:\n",
    "    url = \"https://www.googleapis.com/customsearch/v1\"\n",
    "    params = {\"key\": GOOGLE_API_KEY, \"cx\": GOOGLE_CSE_CX, \"q\": query, \"num\": min(max(num, 1), 10)}\n",
    "    r = requests.get(url, params=params, timeout=30)\n",
    "    r.raise_for_status()\n",
    "    data = r.json()\n",
    "    items = data.get(\"items\", []) or []\n",
    "    return [{\n",
    "        \"title\": it.get(\"title\"),\n",
    "        \"link\": normalize_url(it.get(\"link\") or \"\"),\n",
    "        \"snippet\": it.get(\"snippet\"),\n",
    "        \"displayLink\": it.get(\"displayLink\"),\n",
    "    } for it in items]\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Official targets\n",
    "# ------------------------------------------------------------\n",
    "official_home = normalize_url(official_website)\n",
    "rp_official = build_robot_parser(official_home)\n",
    "\n",
    "status, html = fetch_html(official_home)\n",
    "time.sleep(SLEEP_SEC)\n",
    "if status >= 400 or not html:\n",
    "    raise RuntimeError(f\"Failed to fetch official homepage: HTTP {status}\")\n",
    "\n",
    "home_links = extract_links(official_home, html)\n",
    "internal_links = [l for l in home_links if same_domain(l, official_home) and not is_asset_url(l)]\n",
    "\n",
    "explicit = []\n",
    "for p in FUND_KEY_PATHS:\n",
    "    u = normalize_url(urljoin(official_home, p))\n",
    "    if robots_allows(rp_official, u) and not is_asset_url(u):\n",
    "        explicit.append(u)\n",
    "\n",
    "# Add hint URLs (from Company Basics) if any\n",
    "hint_urls = [normalize_url(u) for u in funding_hint_urls if same_domain(u, official_home)]\n",
    "candidates = list(dict.fromkeys([official_home] + explicit + hint_urls + internal_links))\n",
    "\n",
    "scored = [{\"url\": u, \"score\": score_funding_link(u)} for u in candidates]\n",
    "scored.sort(key=lambda x: x[\"score\"], reverse=True)\n",
    "\n",
    "official_targets = []\n",
    "for it in scored:\n",
    "    if len(official_targets) >= MAX_OFFICIAL_PAGES:\n",
    "        break\n",
    "    if it[\"score\"] < 0.2:\n",
    "        continue\n",
    "    if robots_allows(rp_official, it[\"url\"]):\n",
    "        official_targets.append(it[\"url\"])\n",
    "\n",
    "official_targets = list(dict.fromkeys(official_targets))\n",
    "print(f\"Official funding targets: {len(official_targets)}\")\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# CSE targets (funding + investors + cap table hints)\n",
    "# ------------------------------------------------------------\n",
    "queries = [\n",
    "    f'\"{startup_name}\" raised OR raises OR funding OR 資金調達',\n",
    "    f'\"{startup_name}\" seed round OR Series A OR Series B OR シード',\n",
    "    f'\"{startup_name}\" investors OR investor OR VC OR venture capital',\n",
    "    f'\"{startup_name}\" cap table OR ownership OR board seat',\n",
    "    f'\"{startup_name}\" valuation OR post-money OR pre-money OR 評価額',\n",
    "    f'\"{startup_name}\" lead investor OR led by',\n",
    "    f'\"{startup_name}\" strategic investment OR corporate venture',\n",
    "    f'\"{startup_name}\" press release funding OR プレスリリース 資金調達',\n",
    "    f'\"{startup_name}\" Crunchbase OR PitchBook',\n",
    "]\n",
    "\n",
    "cse_items = []\n",
    "for q in queries:\n",
    "    try:\n",
    "        cse_items.extend(google_cse_search(q, num=5))\n",
    "        time.sleep(0.2)\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "seen = set()\n",
    "cse_links = []\n",
    "for it in cse_items:\n",
    "    link = it.get(\"link\")\n",
    "    if link and link not in seen and link.startswith((\"http://\", \"https://\")) and not is_asset_url(link):\n",
    "        seen.add(link)\n",
    "        cse_links.append(link)\n",
    "\n",
    "cse_links = cse_links[:MAX_CSE_PAGES]\n",
    "print(f\"CSE funding targets: {len(cse_links)}\")\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Fetch + store raw pages (JSONL)\n",
    "# ------------------------------------------------------------\n",
    "RAW_PATH = ART_DIR / f\"funding_raw_pages_{run_id}.jsonl\"\n",
    "RAW_PATH.write_text(\"\", encoding=\"utf-8\")\n",
    "\n",
    "def append_jsonl(path: Path, obj: dict):\n",
    "    with path.open(\"a\", encoding=\"utf-8\") as f:\n",
    "        f.write(json.dumps(obj, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "def collect_pages(urls: list[str], source_type: str):\n",
    "    for u in urls:\n",
    "        u = normalize_url(u)\n",
    "        if not u or is_asset_url(u):\n",
    "            continue\n",
    "\n",
    "        rp = rp_official if source_type == \"official\" else get_rp_for_url(u)\n",
    "        allowed = robots_allows(rp, u)\n",
    "\n",
    "        if not allowed:\n",
    "            append_jsonl(RAW_PATH, {\n",
    "                \"url\": u,\n",
    "                \"source_type\": source_type,\n",
    "                \"domain\": get_domain(u),\n",
    "                \"fetched_at_utc\": datetime.now(timezone.utc).isoformat(),\n",
    "                \"http_status\": None,\n",
    "                \"robots_allowed\": False,\n",
    "                \"text\": \"\",\n",
    "                \"notes\": \"Skipped due to robots.txt (or conservative fallback).\",\n",
    "            })\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            code, html = fetch_html(u)\n",
    "            time.sleep(SLEEP_SEC)\n",
    "            text = html_to_text(html) if html else \"\"\n",
    "            text = text[:MAX_TEXT_CHARS_STORE]\n",
    "\n",
    "            append_jsonl(RAW_PATH, {\n",
    "                \"url\": u,\n",
    "                \"source_type\": source_type,\n",
    "                \"domain\": get_domain(u),\n",
    "                \"fetched_at_utc\": datetime.now(timezone.utc).isoformat(),\n",
    "                \"http_status\": code,\n",
    "                \"robots_allowed\": True,\n",
    "                \"text\": text,\n",
    "                \"text_char_len\": len(text),\n",
    "            })\n",
    "        except Exception as e:\n",
    "            append_jsonl(RAW_PATH, {\n",
    "                \"url\": u,\n",
    "                \"source_type\": source_type,\n",
    "                \"domain\": get_domain(u),\n",
    "                \"fetched_at_utc\": datetime.now(timezone.utc).isoformat(),\n",
    "                \"http_status\": None,\n",
    "                \"robots_allowed\": True,\n",
    "                \"text\": \"\",\n",
    "                \"notes\": f\"Fetch error: {str(e)}\",\n",
    "            })\n",
    "\n",
    "collect_pages(official_targets, \"official\")\n",
    "collect_pages(cse_links, \"cse\")\n",
    "\n",
    "print(f\"✅ Funding raw pages saved: {RAW_PATH.as_posix()}\")\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Load raw pages + shortlist to LLM (funding-heavy URLs first)\n",
    "# ------------------------------------------------------------\n",
    "def load_raw_pages(path: Path) -> list[dict]:\n",
    "    rows = []\n",
    "    for line in path.read_text(encoding=\"utf-8\").splitlines():\n",
    "        if line.strip():\n",
    "            rows.append(json.loads(line))\n",
    "    return rows\n",
    "\n",
    "raw_pages = load_raw_pages(RAW_PATH)\n",
    "\n",
    "usable = [\n",
    "    r for r in raw_pages\n",
    "    if r.get(\"robots_allowed\")\n",
    "    and isinstance(r.get(\"http_status\"), int) and r[\"http_status\"] < 400\n",
    "    and (r.get(\"text\") or \"\").strip()\n",
    "]\n",
    "\n",
    "def score_page_for_funding(r):\n",
    "    url = r.get(\"url\") or \"\"\n",
    "    text = r.get(\"text\") or \"\"\n",
    "    p = (urlparse(url).path or \"\").lower()\n",
    "    s = 0\n",
    "    for k in [\"seed\", \"series\", \"fund\", \"invest\", \"investor\", \"round\", \"valuation\", \"cap\", \"board\"]:\n",
    "        if k in p:\n",
    "            s += 2\n",
    "    if re.search(r\"\\b(seed|series|raised|funding|investor|valuation|cap table|board)\\b\", text, flags=re.IGNORECASE):\n",
    "        s += 2\n",
    "    if re.search(r\"(資金調達|ラウンド|投資家|評価額|株主|持分|取締役)\", text):\n",
    "        s += 2\n",
    "    s += min(len(text), 20000) / 7000\n",
    "    if r.get(\"source_type\") == \"official\":\n",
    "        s += 2\n",
    "    return s\n",
    "\n",
    "usable_sorted = sorted(usable, key=score_page_for_funding, reverse=True)\n",
    "shortlist = usable_sorted[:MAX_PAGES_TO_LLM]\n",
    "print(f\"Usable pages: {len(usable)} | Shortlist to LLM: {len(shortlist)}\")\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Robust JSON parsing helper\n",
    "# ------------------------------------------------------------\n",
    "def _extract_json_object(text: str) -> str:\n",
    "    if not text:\n",
    "        return \"\"\n",
    "    t = text.strip()\n",
    "    m = re.search(r\"```(?:json)?\\s*(\\{.*?\\})\\s*```\", t, flags=re.DOTALL | re.IGNORECASE)\n",
    "    if m:\n",
    "        return m.group(1).strip()\n",
    "    start, end = t.find(\"{\"), t.rfind(\"}\")\n",
    "    if start != -1 and end != -1 and end > start:\n",
    "        return t[start:end+1].strip()\n",
    "    return \"\"\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# OpenAI: funding summary + events + investors + cap signals + content index\n",
    "# ------------------------------------------------------------\n",
    "def openai_extract_funding(pages: list[dict]) -> dict:\n",
    "    system = (\n",
    "        \"You are a research analyst assistant. \"\n",
    "        \"From the provided webpage texts, extract evidence-linked funding and cap table signals. \"\n",
    "        \"CRITICAL: Always produce a non-empty content_index if there are relevant pages. \"\n",
    "        \"Do not guess missing numbers/dates—only extract what is stated. \"\n",
    "        \"Return a single JSON object only (no markdown).\"\n",
    "    )\n",
    "\n",
    "    schema = {\n",
    "        \"funding_summary\": {\n",
    "            \"company_name\": \"string\",\n",
    "            \"latest_round_hint\": \"string|null\",\n",
    "            \"funding_stage_hint\": \"string|null\",\n",
    "            \"total_funding_mentioned\": \"array of strings\",\n",
    "            \"valuation_signals\": \"array of strings\",\n",
    "            \"runway_burn_signals\": \"array of strings\",\n",
    "            \"notable_investors\": \"array of strings\"\n",
    "        },\n",
    "        \"funding_events\": [\n",
    "            {\n",
    "                \"round_label\": \"string|null\",\n",
    "                \"date\": \"string|null\",\n",
    "                \"amount\": \"string|null\",\n",
    "                \"lead_investor\": \"string|null\",\n",
    "                \"investors_participants\": \"array of strings\",\n",
    "                \"source_url\": \"string\",\n",
    "                \"confidence\": \"0..1\"\n",
    "            }\n",
    "        ],\n",
    "        \"investor_mentions\": [\n",
    "            {\n",
    "                \"investor_name\": \"string\",\n",
    "                \"mention_context\": \"string\",\n",
    "                \"type_hint\": \"lead|participant|strategic|other\",\n",
    "                \"source_url\": \"string\",\n",
    "                \"confidence\": \"0..1\"\n",
    "            }\n",
    "        ],\n",
    "        \"cap_table_signals\": [\n",
    "            {\n",
    "                \"signal_type\": \"board_seat|ownership_hint|strategic_partner|secondary|other\",\n",
    "                \"signal\": \"string\",\n",
    "                \"source_url\": \"string\",\n",
    "                \"confidence\": \"0..1\"\n",
    "            }\n",
    "        ],\n",
    "        \"funding_claims\": [\n",
    "            {\n",
    "                \"category\": \"round|investor|amount|valuation|cap_table|other\",\n",
    "                \"claim_type\": \"fact|hypothesis\",\n",
    "                \"claim\": \"string\",\n",
    "                \"evidence_url\": \"string\",\n",
    "                \"source_type\": \"official|cse\",\n",
    "                \"confidence\": \"0..1\"\n",
    "            }\n",
    "        ],\n",
    "        \"content_index\": [\n",
    "            {\n",
    "                \"content_type\": \"press_release|article|interview|database|blog|other\",\n",
    "                \"title\": \"string|null\",\n",
    "                \"publisher_or_platform\": \"string|null\",\n",
    "                \"date\": \"string|null\",\n",
    "                \"url\": \"string\",\n",
    "                \"what_it_contains\": \"string\",\n",
    "                \"relevance\": \"0..1\",\n",
    "                \"confidence\": \"0..1\"\n",
    "            }\n",
    "        ],\n",
    "        \"notes\": {\"data_gaps\": \"array of strings\", \"conflicts\": \"array of strings\"},\n",
    "        \"confidence\": {\"overall\": \"0..1\", \"rationale\": \"string\"}\n",
    "    }\n",
    "\n",
    "    bundle = {\n",
    "        \"startup_name\": startup_name,\n",
    "        \"official_website\": official_home,\n",
    "        \"pages\": [\n",
    "            {\n",
    "                \"url\": p[\"url\"],\n",
    "                \"source_type\": p.get(\"source_type\"),\n",
    "                \"domain\": p.get(\"domain\"),\n",
    "                \"text\": (p.get(\"text\") or \"\")[:MAX_TEXT_CHARS_PROMPT],\n",
    "            } for p in pages\n",
    "        ],\n",
    "        \"schema\": schema,\n",
    "        \"instructions\": [\n",
    "            \"Prefer official press releases for funding events.\",\n",
    "            \"Extract investors as they appear; do not deduplicate aggressively if contexts differ.\",\n",
    "            \"If Crunchbase/PitchBook is paywalled, still capture what is visible (title/snippet-level hints) as low confidence.\"\n",
    "        ]\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        resp = client.chat.completions.create(\n",
    "            model=\"gpt-4o-mini\",\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": system},\n",
    "                {\"role\": \"user\", \"content\": json.dumps(bundle, ensure_ascii=False)},\n",
    "            ],\n",
    "            temperature=0.0,\n",
    "            response_format={\"type\": \"json_object\"},\n",
    "        )\n",
    "        return json.loads((resp.choices[0].message.content or \"\").strip())\n",
    "    except Exception:\n",
    "        resp2 = client.chat.completions.create(\n",
    "            model=\"gpt-4o-mini\",\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": system},\n",
    "                {\"role\": \"user\", \"content\": json.dumps(bundle, ensure_ascii=False)},\n",
    "            ],\n",
    "            temperature=0.0,\n",
    "        )\n",
    "        raw = resp2.choices[0].message.content or \"\"\n",
    "        candidate = _extract_json_object(raw)\n",
    "        if candidate:\n",
    "            return json.loads(candidate)\n",
    "        raise RuntimeError(\"Model did not return a JSON object.\")\n",
    "\n",
    "extracted = openai_extract_funding(shortlist)\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Save JSON + build DataFrames\n",
    "# ------------------------------------------------------------\n",
    "JSON_PATH = ART_DIR / f\"funding_extraction_{run_id}.json\"\n",
    "JSON_PATH.write_text(json.dumps(extracted, ensure_ascii=False, indent=2), encoding=\"utf-8\")\n",
    "\n",
    "summary = extracted.get(\"funding_summary\", {}) or {}\n",
    "events = extracted.get(\"funding_events\", []) or []\n",
    "investors = extracted.get(\"investor_mentions\", []) or []\n",
    "cap_signals = extracted.get(\"cap_table_signals\", []) or []\n",
    "claims = extracted.get(\"funding_claims\", []) or []\n",
    "content = extracted.get(\"content_index\", []) or []\n",
    "\n",
    "df_summary = pd.DataFrame([{\"run_id\": run_id, **summary}])\n",
    "df_events = pd.DataFrame(events)\n",
    "df_investors = pd.DataFrame(investors)\n",
    "df_cap = pd.DataFrame(cap_signals)\n",
    "df_claims = pd.DataFrame(claims)\n",
    "df_content = pd.DataFrame(content)\n",
    "\n",
    "for df in [df_events, df_investors, df_cap, df_claims, df_content]:\n",
    "    if not df.empty:\n",
    "        df.insert(0, \"run_id\", run_id)\n",
    "\n",
    "SUMMARY_CSV = ART_DIR / f\"funding_summary_{run_id}.csv\"\n",
    "EVENTS_CSV = ART_DIR / f\"funding_events_{run_id}.csv\"\n",
    "INV_CSV = ART_DIR / f\"investor_mentions_{run_id}.csv\"\n",
    "CAP_CSV = ART_DIR / f\"cap_table_signals_{run_id}.csv\"\n",
    "CLAIMS_CSV = ART_DIR / f\"funding_claims_{run_id}.csv\"\n",
    "CONTENT_CSV = ART_DIR / f\"funding_content_index_{run_id}.csv\"\n",
    "\n",
    "df_summary.to_csv(SUMMARY_CSV, index=False)\n",
    "df_events.to_csv(EVENTS_CSV, index=False)\n",
    "df_investors.to_csv(INV_CSV, index=False)\n",
    "df_cap.to_csv(CAP_CSV, index=False)\n",
    "df_claims.to_csv(CLAIMS_CSV, index=False)\n",
    "df_content.to_csv(CONTENT_CSV, index=False)\n",
    "\n",
    "print(\"✅ Funding & Cap Table Signals complete\")\n",
    "print(f\"- Raw pages: {RAW_PATH.as_posix()}\")\n",
    "print(f\"- JSON: {JSON_PATH.as_posix()}\")\n",
    "print(f\"- Summary CSV: {SUMMARY_CSV.as_posix()}\")\n",
    "print(f\"- Events CSV: {EVENTS_CSV.as_posix()}\")\n",
    "print(f\"- Investor mentions CSV: {INV_CSV.as_posix()}\")\n",
    "print(f\"- Cap table signals CSV: {CAP_CSV.as_posix()}\")\n",
    "print(f\"- Claims CSV: {CLAIMS_CSV.as_posix()}\")\n",
    "print(f\"- Content CSV: {CONTENT_CSV.as_posix()}\")\n",
    "\n",
    "display(df_summary)\n",
    "display(df_events.head(30) if not df_events.empty else df_events)\n",
    "display(df_investors.head(30) if not df_investors.empty else df_investors)\n",
    "display(df_cap.head(30) if not df_cap.empty else df_cap)\n",
    "display(df_claims.head(30) if not df_claims.empty else df_claims)\n",
    "display(df_content.head(30) if not df_content.empty else df_content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6e1babfe-fc47-4c06-b268-5b293441b0b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recent window: last 4 months (cutoff ~ 2025-09-10 UTC)\n",
      "Official recent targets: 5\n",
      "CSE recent targets: 48\n",
      "[official] 1/5 fetching: https://sakana.ai/updates\n",
      "[official] 2/5 fetching: https://sakana.ai/news\n",
      "[official] 3/5 fetching: https://sakana.ai/press\n",
      "[official] 4/5 fetching: https://sakana.ai/blog\n",
      "[official] 5/5 fetching: https://sakana.ai/research\n",
      "Done fetching official: saved 5 pages (errors/blocked recorded).\n",
      "[cse] 1/20 fetching: https://theaiinsider.tech/2025/11/17/sakana-ai-secures-135m-series-b-at-a-2-65b-valuation-to-advance-japan-optimized-ai-models/\n",
      "[cse] 2/20 fetching: https://fortune.com/2025/02/09/linkedin-cofounder-reid-hoffman-hugging-face-ceo-clemen-delangue-letter-ai-public-goods-current-ai-action-summit/\n",
      "[cse] 3/20 fetching: https://www.japantimes.co.jp/business/2025/05/13/companies/sakana-ai-defense/\n",
      "[cse] 4/20 fetching: https://techcrunch.com/2026/01/02/nvidias-ai-empire-a-look-at-its-top-startup-investments/\n",
      "[cse] 5/20 fetching: https://www.technologyreview.com/2026/01/05/1130662/whats-next-for-ai-in-2026/\n",
      "[cse] 6/20 fetching: https://siliconangle.com/2025/11/17/sakana-ai-lands-135m-2-635b-valuation-accelerate-frontier-research-applied-ai-japan/\n",
      "[cse] 7/20 fetching: https://techcrunch.com/2025/11/17/sakana-ai-raises-135m-series-b-at-a-2-65b-valuation-to-continue-building-ai-models-for-japan/\n",
      "[cse] 8/20 fetching: https://observer.com/2025/11/sakana-ai-raises-135m-japan-tailored-models/\n",
      "[cse] 9/20 fetching: https://www.japantimes.co.jp/business/2025/11/17/companies/sakana-ai-valuation/\n",
      "[cse] 10/20 fetching: https://cogsci.jhu.edu/2025/11/10/student-spotlight-yash-mehta-on-summer-opportunities/\n",
      "[cse] 11/20 fetching: https://ethanbholland.com/2025/11/21/sakana-ai-news-week-ending-11-21-2025/\n",
      "[cse] 12/20 fetching: https://techstartups.com/2025/10/21/sakana-ai-japans-answer-to-openai-in-talks-to-raise-100m-at-2-5b-valuation/\n",
      "[cse] 13/20 fetching: https://guyanabusinessjournal.com/2025/10/can-africa-birth-a-2-5-billion-ai-powerhouse-lessons-for-the-global-south/\n",
      "[cse] 14/20 fetching: https://www.japantimes.co.jp/business/2025/09/10/companies/sakana-ai-interview/\n",
      "[cse] 15/20 fetching: https://sakana.ai/blog/\n",
      "[cse] 16/20 fetching: https://sakana.ai/ai-scientist/\n",
      "[cse] 17/20 fetching: https://sakana.ai/evolutionary-model-merge/\n",
      "[cse] 18/20 fetching: https://sakana.ai/ai-scientist-first-publication/\n",
      "[cse] 19/20 fetching: https://axisofordinary.substack.com/p/links-for-2026-01-06\n",
      "[cse] 20/20 fetching: https://www.timesofai.com/news/sakana-ai-unveils-the-future-fully-automated-ai-scientist-revolution/\n",
      "Done fetching cse: saved 17 pages (errors/blocked recorded).\n",
      "✅ Recent raw pages saved: artifacts/meeting_deep_dive/recent_raw_pages_20260107_225628.jsonl\n",
      "Usable pages: 18 | Shortlist to LLM: 18\n",
      "✅ Recent Changes Timeline complete\n",
      "- Raw pages: artifacts/meeting_deep_dive/recent_raw_pages_20260107_225628.jsonl\n",
      "- JSON: artifacts/meeting_deep_dive/recent_timeline_extraction_20260107_225628.json\n",
      "- Timeline CSV: artifacts/meeting_deep_dive/recent_timeline_20260107_225628.csv\n",
      "- Content CSV: artifacts/meeting_deep_dive/recent_content_index_20260107_225628.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>run_id</th>\n",
       "      <th>date</th>\n",
       "      <th>event_type</th>\n",
       "      <th>headline</th>\n",
       "      <th>details</th>\n",
       "      <th>evidence_url</th>\n",
       "      <th>source_type</th>\n",
       "      <th>confidence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20260107_225628</td>\n",
       "      <td>2025-11-21</td>\n",
       "      <td>product</td>\n",
       "      <td>Sakana AI takes crown as Japan’s most valuable...</td>\n",
       "      <td>Sakana AI has been recognized as Japan's most ...</td>\n",
       "      <td>https://ethanbholland.com/2025/11/21/sakana-ai...</td>\n",
       "      <td>cse</td>\n",
       "      <td>0.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20260107_225628</td>\n",
       "      <td>2025-11-17</td>\n",
       "      <td>funding</td>\n",
       "      <td>Sakana AI raises $135M Series B at a $2.65B va...</td>\n",
       "      <td>Sakana AI has closed a ¥20 billion (approximat...</td>\n",
       "      <td>https://techcrunch.com/2025/11/17/sakana-ai-ra...</td>\n",
       "      <td>cse</td>\n",
       "      <td>0.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20260107_225628</td>\n",
       "      <td>2025-11-17</td>\n",
       "      <td>partnership</td>\n",
       "      <td>Sakana AI targets defense and banking markets ...</td>\n",
       "      <td>Sakana AI is now eyeing expansion into defense...</td>\n",
       "      <td>https://www.japantimes.co.jp/business/2025/11/...</td>\n",
       "      <td>cse</td>\n",
       "      <td>0.8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            run_id        date   event_type  \\\n",
       "2  20260107_225628  2025-11-21      product   \n",
       "0  20260107_225628  2025-11-17      funding   \n",
       "1  20260107_225628  2025-11-17  partnership   \n",
       "\n",
       "                                            headline  \\\n",
       "2  Sakana AI takes crown as Japan’s most valuable...   \n",
       "0  Sakana AI raises $135M Series B at a $2.65B va...   \n",
       "1  Sakana AI targets defense and banking markets ...   \n",
       "\n",
       "                                             details  \\\n",
       "2  Sakana AI has been recognized as Japan's most ...   \n",
       "0  Sakana AI has closed a ¥20 billion (approximat...   \n",
       "1  Sakana AI is now eyeing expansion into defense...   \n",
       "\n",
       "                                        evidence_url source_type  confidence  \n",
       "2  https://ethanbholland.com/2025/11/21/sakana-ai...         cse         0.8  \n",
       "0  https://techcrunch.com/2025/11/17/sakana-ai-ra...         cse         0.9  \n",
       "1  https://www.japantimes.co.jp/business/2025/11/...         cse         0.8  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>run_id</th>\n",
       "      <th>content_type</th>\n",
       "      <th>title</th>\n",
       "      <th>publisher_or_platform</th>\n",
       "      <th>date</th>\n",
       "      <th>url</th>\n",
       "      <th>what_it_contains</th>\n",
       "      <th>relevance</th>\n",
       "      <th>confidence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20260107_225628</td>\n",
       "      <td>article</td>\n",
       "      <td>Sakana AI raises $135M Series B at a $2.65B va...</td>\n",
       "      <td>TechCrunch</td>\n",
       "      <td>2025-11-17</td>\n",
       "      <td>https://techcrunch.com/2025/11/17/sakana-ai-ra...</td>\n",
       "      <td>Details about Sakana AI's Series B funding rou...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20260107_225628</td>\n",
       "      <td>article</td>\n",
       "      <td>Sakana AI targets defense and banking markets ...</td>\n",
       "      <td>Japan Times</td>\n",
       "      <td>2025-11-17</td>\n",
       "      <td>https://www.japantimes.co.jp/business/2025/11/...</td>\n",
       "      <td>Discussion on Sakana AI's expansion plans into...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20260107_225628</td>\n",
       "      <td>article</td>\n",
       "      <td>Sakana AI takes crown as Japan’s most valuable...</td>\n",
       "      <td>Ethan B Holland</td>\n",
       "      <td>2025-11-21</td>\n",
       "      <td>https://ethanbholland.com/2025/11/21/sakana-ai...</td>\n",
       "      <td>Recognition of Sakana AI as Japan's most valua...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>20260107_225628</td>\n",
       "      <td>article</td>\n",
       "      <td>Sakana AI lands $135M on $2.635B valuation to ...</td>\n",
       "      <td>SiliconANGLE</td>\n",
       "      <td>2025-11-17</td>\n",
       "      <td>https://siliconangle.com/2025/11/17/sakana-ai-...</td>\n",
       "      <td>Details on the funding round and its implicati...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20260107_225628</td>\n",
       "      <td>article</td>\n",
       "      <td>Sakana AI, Japan’s answer to OpenAI, in talks ...</td>\n",
       "      <td>TechStartups</td>\n",
       "      <td>2025-10-21</td>\n",
       "      <td>https://techstartups.com/2025/10/21/sakana-ai-...</td>\n",
       "      <td>Discussion on Sakana AI's funding talks and it...</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>20260107_225628</td>\n",
       "      <td>article</td>\n",
       "      <td>Sakana AI Agent Wins AtCoder Heuristic Contest...</td>\n",
       "      <td>Sakana AI Blog</td>\n",
       "      <td>2026-01-05</td>\n",
       "      <td>https://sakana.ai/blog/</td>\n",
       "      <td>Announcement of Sakana AI's agent winning a co...</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>20260107_225628</td>\n",
       "      <td>article</td>\n",
       "      <td>Sakana AI unveils the future: Fully automated ...</td>\n",
       "      <td>Times of AI</td>\n",
       "      <td>2024-08-13</td>\n",
       "      <td>https://www.timesofai.com/news/sakana-ai-unvei...</td>\n",
       "      <td>Overview of Sakana AI's AI Scientist project a...</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>20260107_225628</td>\n",
       "      <td>article</td>\n",
       "      <td>Sakana AI’s AI Scientist generates its first p...</td>\n",
       "      <td>Sakana AI</td>\n",
       "      <td>2025-03-12</td>\n",
       "      <td>https://sakana.ai/ai-scientist-first-publication/</td>\n",
       "      <td>Details on the AI Scientist's achievement in g...</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>20260107_225628</td>\n",
       "      <td>article</td>\n",
       "      <td>Japan should produce its own AI defense soluti...</td>\n",
       "      <td>Japan Times</td>\n",
       "      <td>2025-05-13</td>\n",
       "      <td>https://www.japantimes.co.jp/business/2025/05/...</td>\n",
       "      <td>Comments from Sakana AI's CEO on the need for ...</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>20260107_225628</td>\n",
       "      <td>article</td>\n",
       "      <td>Sakana AI’s AI Scientist: Towards Fully Automa...</td>\n",
       "      <td>Sakana AI</td>\n",
       "      <td>2024-08-13</td>\n",
       "      <td>https://sakana.ai/ai-scientist/</td>\n",
       "      <td>Introduction to the AI Scientist and its capab...</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            run_id content_type  \\\n",
       "0  20260107_225628      article   \n",
       "1  20260107_225628      article   \n",
       "2  20260107_225628      article   \n",
       "3  20260107_225628      article   \n",
       "4  20260107_225628      article   \n",
       "5  20260107_225628      article   \n",
       "6  20260107_225628      article   \n",
       "7  20260107_225628      article   \n",
       "8  20260107_225628      article   \n",
       "9  20260107_225628      article   \n",
       "\n",
       "                                               title publisher_or_platform  \\\n",
       "0  Sakana AI raises $135M Series B at a $2.65B va...            TechCrunch   \n",
       "1  Sakana AI targets defense and banking markets ...           Japan Times   \n",
       "2  Sakana AI takes crown as Japan’s most valuable...       Ethan B Holland   \n",
       "3  Sakana AI lands $135M on $2.635B valuation to ...          SiliconANGLE   \n",
       "4  Sakana AI, Japan’s answer to OpenAI, in talks ...          TechStartups   \n",
       "5  Sakana AI Agent Wins AtCoder Heuristic Contest...        Sakana AI Blog   \n",
       "6  Sakana AI unveils the future: Fully automated ...           Times of AI   \n",
       "7  Sakana AI’s AI Scientist generates its first p...             Sakana AI   \n",
       "8  Japan should produce its own AI defense soluti...           Japan Times   \n",
       "9  Sakana AI’s AI Scientist: Towards Fully Automa...             Sakana AI   \n",
       "\n",
       "         date                                                url  \\\n",
       "0  2025-11-17  https://techcrunch.com/2025/11/17/sakana-ai-ra...   \n",
       "1  2025-11-17  https://www.japantimes.co.jp/business/2025/11/...   \n",
       "2  2025-11-21  https://ethanbholland.com/2025/11/21/sakana-ai...   \n",
       "3  2025-11-17  https://siliconangle.com/2025/11/17/sakana-ai-...   \n",
       "4  2025-10-21  https://techstartups.com/2025/10/21/sakana-ai-...   \n",
       "5  2026-01-05                            https://sakana.ai/blog/   \n",
       "6  2024-08-13  https://www.timesofai.com/news/sakana-ai-unvei...   \n",
       "7  2025-03-12  https://sakana.ai/ai-scientist-first-publication/   \n",
       "8  2025-05-13  https://www.japantimes.co.jp/business/2025/05/...   \n",
       "9  2024-08-13                    https://sakana.ai/ai-scientist/   \n",
       "\n",
       "                                    what_it_contains  relevance  confidence  \n",
       "0  Details about Sakana AI's Series B funding rou...        1.0         0.9  \n",
       "1  Discussion on Sakana AI's expansion plans into...        1.0         0.8  \n",
       "2  Recognition of Sakana AI as Japan's most valua...        1.0         0.8  \n",
       "3  Details on the funding round and its implicati...        1.0         0.9  \n",
       "4  Discussion on Sakana AI's funding talks and it...        0.8         0.7  \n",
       "5  Announcement of Sakana AI's agent winning a co...        0.8         0.6  \n",
       "6  Overview of Sakana AI's AI Scientist project a...        0.7         0.5  \n",
       "7  Details on the AI Scientist's achievement in g...        0.7         0.5  \n",
       "8  Comments from Sakana AI's CEO on the need for ...        0.6         0.5  \n",
       "9  Introduction to the AI Scientist and its capab...        0.6         0.5  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ============================================================\n",
    "# 10. Recent Changes Timeline (Last N months, Evidence-linked)\n",
    "# ============================================================\n",
    "# Goal:\n",
    "# - Identify meaningful \"recent changes\" in the last few months:\n",
    "#   - product launches / releases / research posts\n",
    "#   - major partnerships / customers / deployments\n",
    "#   - funding announcements / investor updates\n",
    "#   - leadership changes / hiring / org changes\n",
    "# - Build a timeline with:\n",
    "#   - date (or best-effort month)\n",
    "#   - event_type + short description\n",
    "#   - evidence_url + source_type + confidence\n",
    "#\n",
    "# Key fixes vs. earlier draft:\n",
    "# - Progress logging per URL (so it never \"looks stuck\")\n",
    "# - Hard per-request timeouts (connect/read)\n",
    "# - Domain cap + staged CSE fetch (avoid a single slow domain dominating)\n",
    "# - Safer cutoff computation without pandas DateOffset dependency\n",
    "#\n",
    "# Outputs:\n",
    "# - recent_raw_pages_<run_id>.jsonl\n",
    "# - recent_timeline_extraction_<run_id>.json\n",
    "# - recent_timeline_<run_id>.csv\n",
    "# - recent_content_index_<run_id>.csv\n",
    "\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "import time\n",
    "import socket\n",
    "import requests\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "from pathlib import Path\n",
    "from urllib.parse import urlparse, urljoin\n",
    "from datetime import datetime, timezone, timedelta\n",
    "from urllib.robotparser import RobotFileParser\n",
    "from collections import defaultdict\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Global configuration\n",
    "# ------------------------------------------------------------\n",
    "load_dotenv(\"env.txt\")\n",
    "\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "if not OPENAI_API_KEY:\n",
    "    raise EnvironmentError(\"OPENAI_API_KEY is not set.\")\n",
    "\n",
    "GOOGLE_API_KEY = os.getenv(\"GOOGLE_API_KEY\")\n",
    "GOOGLE_CSE_CX = os.getenv(\"GOOGLE_CSE_CX\")\n",
    "if not GOOGLE_API_KEY or not GOOGLE_CSE_CX:\n",
    "    raise EnvironmentError(\"GOOGLE_API_KEY and GOOGLE_CSE_CX must be set.\")\n",
    "\n",
    "client = OpenAI(api_key=OPENAI_API_KEY)\n",
    "\n",
    "ART_DIR = Path(\"artifacts\") / \"meeting_deep_dive\"\n",
    "ART_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Load inputs + entity\n",
    "# ------------------------------------------------------------\n",
    "def load_latest_json(pattern: str) -> tuple[dict, Path]:\n",
    "    files = sorted(ART_DIR.glob(pattern), key=lambda p: p.stat().st_mtime, reverse=True)\n",
    "    if not files:\n",
    "        raise FileNotFoundError(f\"No {pattern} found. Please run previous cells.\")\n",
    "    p = files[0]\n",
    "    return json.loads(p.read_text(encoding=\"utf-8\")), p\n",
    "\n",
    "try:\n",
    "    run_id = inputs[\"meta\"][\"run_id\"]\n",
    "except Exception:\n",
    "    inp, _ = load_latest_json(\"inputs_*.json\")\n",
    "    run_id = inp.get(\"meta\", {}).get(\"run_id\") or datetime.now(timezone.utc).strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "entity, _ = load_latest_json(\"entity_*.json\")\n",
    "startup_name = (entity.get(\"canonical_name\") or \"Unknown Startup\").strip()\n",
    "official_website = (entity.get(\"official_website\") or \"\").strip()\n",
    "if not official_website:\n",
    "    raise ValueError(\"official_website is missing. Run cell #2 first.\")\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Time window config (edit as needed)\n",
    "# ------------------------------------------------------------\n",
    "MONTHS_BACK = 4\n",
    "NOW_UTC = datetime.now(timezone.utc)\n",
    "\n",
    "# Approximate cutoff (30 days per month is enough for a \"recent scan\")\n",
    "CUTOFF_UTC = NOW_UTC - timedelta(days=30 * MONTHS_BACK)\n",
    "CUTOFF_STR = CUTOFF_UTC.strftime(\"%Y-%m-%d\")\n",
    "print(f\"Recent window: last {MONTHS_BACK} months (cutoff ~ {CUTOFF_STR} UTC)\")\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Knobs\n",
    "# ------------------------------------------------------------\n",
    "USER_AGENT = \"researchOSv2-bot/0.3 (+contact: internal-research)\"\n",
    "TIMEOUT_CONNECT = 8\n",
    "TIMEOUT_READ = 18\n",
    "SLEEP_SEC = 0.20\n",
    "\n",
    "MAX_OFFICIAL_PAGES = 40\n",
    "MAX_CSE_PAGES = 60\n",
    "MAX_TEXT_CHARS_STORE = 70000\n",
    "MAX_TEXT_CHARS_PROMPT = 9000\n",
    "MAX_PAGES_TO_LLM = 26\n",
    "\n",
    "# Stage fetch (prevents long hangs / makes debugging easier)\n",
    "CSE_FETCH_LIMIT_STAGE1 = 20   # first pass\n",
    "CSE_FETCH_LIMIT_STAGE2 = 50   # optional second pass\n",
    "\n",
    "RECENT_KEY_PATHS = [\"/news\", \"/press\", \"/blog\", \"/updates\", \"/research\"]\n",
    "RECENT_KEYWORDS = [\"news\", \"press\", \"blog\", \"update\", \"updates\", \"research\", \"announcement\", \"release\"]\n",
    "\n",
    "# Global hard timeout safety net\n",
    "socket.setdefaulttimeout(20)\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# URL / domain / robots helpers\n",
    "# ------------------------------------------------------------\n",
    "def normalize_url(url: str) -> str:\n",
    "    url = (url or \"\").strip()\n",
    "    if not url:\n",
    "        return \"\"\n",
    "    if not re.match(r\"^https?://\", url, flags=re.I):\n",
    "        url = \"https://\" + url\n",
    "    p = urlparse(url)\n",
    "    return p._replace(netloc=p.netloc.lower(), fragment=\"\").geturl()\n",
    "\n",
    "def get_domain(url: str) -> str:\n",
    "    return re.sub(r\"^www\\.\", \"\", urlparse(normalize_url(url)).netloc)\n",
    "\n",
    "def same_domain(a: str, b: str) -> bool:\n",
    "    da, db = get_domain(a), get_domain(b)\n",
    "    return da == db or da.endswith(\".\" + db) or db.endswith(\".\" + da)\n",
    "\n",
    "def is_asset_url(url: str) -> bool:\n",
    "    path = (urlparse(url).path or \"\").lower()\n",
    "    return bool(re.search(r\"\\.(pdf|jpg|jpeg|png|gif|svg|zip|mp4|mov)$\", path))\n",
    "\n",
    "def build_robot_parser(base_url: str) -> RobotFileParser | None:\n",
    "    rp = RobotFileParser()\n",
    "    rp.set_url(urljoin(normalize_url(base_url), \"/robots.txt\"))\n",
    "    try:\n",
    "        rp.read()\n",
    "        return rp\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "def robots_allows(rp: RobotFileParser | None, url: str) -> bool:\n",
    "    if rp is None:\n",
    "        p = (urlparse(url).path or \"/\").lower()\n",
    "        return p in [\"/\", \"\"] or any(k in p for k in RECENT_KEYWORDS)\n",
    "    try:\n",
    "        return rp.can_fetch(USER_AGENT, url)\n",
    "    except Exception:\n",
    "        return False\n",
    "\n",
    "_robot_cache: dict[str, RobotFileParser | None] = {}\n",
    "\n",
    "def get_rp_for_url(url: str) -> RobotFileParser | None:\n",
    "    dom = get_domain(url)\n",
    "    if dom in _robot_cache:\n",
    "        return _robot_cache[dom]\n",
    "    rp = build_robot_parser(url)\n",
    "    _robot_cache[dom] = rp\n",
    "    return rp\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Fetch + text extraction\n",
    "# ------------------------------------------------------------\n",
    "def fetch_html(url: str) -> tuple[int, str]:\n",
    "    r = requests.get(\n",
    "        url,\n",
    "        headers={\"User-Agent\": USER_AGENT},\n",
    "        timeout=(TIMEOUT_CONNECT, TIMEOUT_READ),\n",
    "        allow_redirects=True\n",
    "    )\n",
    "    return r.status_code, (r.text if r.ok else \"\")\n",
    "\n",
    "def html_to_text(html: str) -> str:\n",
    "    soup = BeautifulSoup(html, \"lxml\")\n",
    "    for tag in soup([\"script\", \"style\", \"noscript\", \"svg\"]):\n",
    "        tag.decompose()\n",
    "    node = soup.find(\"main\") or soup.find(\"article\") or (soup.body if soup.body else soup)\n",
    "    text = node.get_text(\"\\n\", strip=True)\n",
    "    text = re.sub(r\"\\n{3,}\", \"\\n\\n\", text)\n",
    "    return text.strip()\n",
    "\n",
    "def extract_links(base_url: str, html: str) -> list[str]:\n",
    "    soup = BeautifulSoup(html, \"lxml\")\n",
    "    out = []\n",
    "    for a in soup.find_all(\"a\", href=True):\n",
    "        href = a[\"href\"].strip()\n",
    "        if href.startswith(\"#\") or href.lower().startswith(\"mailto:\") or href.lower().startswith(\"tel:\"):\n",
    "            continue\n",
    "        u = normalize_url(urljoin(base_url, href))\n",
    "        if u.startswith((\"http://\", \"https://\")):\n",
    "            out.append(u)\n",
    "    seen, uniq = set(), []\n",
    "    for u in out:\n",
    "        if u not in seen:\n",
    "            seen.add(u)\n",
    "            uniq.append(u)\n",
    "    return uniq\n",
    "\n",
    "def score_recent_link(url: str) -> float:\n",
    "    p = (urlparse(url).path or \"/\").lower()\n",
    "    score = 0.0\n",
    "    for k in RECENT_KEYWORDS:\n",
    "        if k in p:\n",
    "            score += 1.0\n",
    "    if re.search(r\"/20\\d{2}/\\d{1,2}/\", p) or re.search(r\"/20\\d{2}/\", p):\n",
    "        score += 1.0\n",
    "    if is_asset_url(url):\n",
    "        score -= 4.0\n",
    "    if p.count(\"/\") >= 5:\n",
    "        score -= 0.3\n",
    "    return score\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Google CSE\n",
    "# ------------------------------------------------------------\n",
    "def google_cse_search(query: str, num: int = 5) -> list[dict]:\n",
    "    url = \"https://www.googleapis.com/customsearch/v1\"\n",
    "    params = {\"key\": GOOGLE_API_KEY, \"cx\": GOOGLE_CSE_CX, \"q\": query, \"num\": min(max(num, 1), 10)}\n",
    "    r = requests.get(url, params=params, timeout=30)\n",
    "    r.raise_for_status()\n",
    "    data = r.json()\n",
    "    items = data.get(\"items\", []) or []\n",
    "    return [{\n",
    "        \"title\": it.get(\"title\"),\n",
    "        \"link\": normalize_url(it.get(\"link\") or \"\"),\n",
    "        \"snippet\": it.get(\"snippet\"),\n",
    "        \"displayLink\": it.get(\"displayLink\"),\n",
    "    } for it in items]\n",
    "\n",
    "def cap_by_domain(urls: list[str], cap: int = 4) -> list[str]:\n",
    "    buckets = defaultdict(int)\n",
    "    out = []\n",
    "    for u in urls:\n",
    "        d = get_domain(u)\n",
    "        if buckets[d] < cap:\n",
    "            out.append(u)\n",
    "            buckets[d] += 1\n",
    "    return out\n",
    "\n",
    "def prefer_dated_urls(urls: list[str], top_n: int = 40) -> list[str]:\n",
    "    def score(u):\n",
    "        p = (urlparse(u).path or \"\").lower()\n",
    "        s = 0\n",
    "        if re.search(r\"/20\\d{2}/\\d{1,2}/\", p): s += 3\n",
    "        if re.search(r\"/20\\d{2}/\", p): s += 1\n",
    "        return s\n",
    "    return sorted(urls, key=score, reverse=True)[:top_n]\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Official targets (news/press/blog hubs + internal links)\n",
    "# ------------------------------------------------------------\n",
    "official_home = normalize_url(official_website)\n",
    "rp_official = build_robot_parser(official_home)\n",
    "\n",
    "status, html = fetch_html(official_home)\n",
    "time.sleep(SLEEP_SEC)\n",
    "if status >= 400 or not html:\n",
    "    raise RuntimeError(f\"Failed to fetch official homepage: HTTP {status}\")\n",
    "\n",
    "home_links = extract_links(official_home, html)\n",
    "internal_links = [l for l in home_links if same_domain(l, official_home) and not is_asset_url(l)]\n",
    "\n",
    "explicit = []\n",
    "for p in RECENT_KEY_PATHS:\n",
    "    u = normalize_url(urljoin(official_home, p))\n",
    "    if robots_allows(rp_official, u) and not is_asset_url(u):\n",
    "        explicit.append(u)\n",
    "\n",
    "candidates = list(dict.fromkeys([official_home] + explicit + internal_links))\n",
    "scored = [{\"url\": u, \"score\": score_recent_link(u)} for u in candidates]\n",
    "scored.sort(key=lambda x: x[\"score\"], reverse=True)\n",
    "\n",
    "official_targets = []\n",
    "for it in scored:\n",
    "    if len(official_targets) >= MAX_OFFICIAL_PAGES:\n",
    "        break\n",
    "    if it[\"score\"] < 0.2:\n",
    "        continue\n",
    "    if robots_allows(rp_official, it[\"url\"]):\n",
    "        official_targets.append(it[\"url\"])\n",
    "\n",
    "official_targets = list(dict.fromkeys(official_targets))\n",
    "print(f\"Official recent targets: {len(official_targets)}\")\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# CSE targets (recent-focused queries)\n",
    "# ------------------------------------------------------------\n",
    "# Google CSE doesn't guarantee strict date filters; we bias recall with month tokens\n",
    "month_tokens = []\n",
    "for i in range(MONTHS_BACK + 1):\n",
    "    d = NOW_UTC - timedelta(days=30 * i)\n",
    "    month_tokens.append(d.strftime(\"%Y-%m\"))\n",
    "\n",
    "official_domain = get_domain(official_home)\n",
    "\n",
    "queries = [\n",
    "    f'\"{startup_name}\" site:{official_domain} (news OR press OR blog OR update OR research)',\n",
    "    f'\"{startup_name}\" (press release OR announcement OR launched OR partnership OR funding) {month_tokens[0]}',\n",
    "    f'\"{startup_name}\" (プレスリリース OR 発表 OR リリース OR 提携 OR 資金調達) {month_tokens[0]}',\n",
    "]\n",
    "for t in month_tokens:\n",
    "    queries.append(f'\"{startup_name}\" {t} (announcement OR press OR blog OR funding OR partnership OR research)')\n",
    "    queries.append(f'\"{startup_name}\" {t} (プレスリリース OR 発表 OR リリース OR 提携 OR 資金調達 OR 研究)')\n",
    "\n",
    "cse_items = []\n",
    "for q in queries:\n",
    "    try:\n",
    "        cse_items.extend(google_cse_search(q, num=5))\n",
    "        time.sleep(0.2)\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "seen = set()\n",
    "cse_links = []\n",
    "for it in cse_items:\n",
    "    link = it.get(\"link\")\n",
    "    if link and link not in seen and link.startswith((\"http://\", \"https://\")) and not is_asset_url(link):\n",
    "        seen.add(link)\n",
    "        cse_links.append(link)\n",
    "\n",
    "# De-bias: cap domains + prefer dated URLs\n",
    "cse_links = cap_by_domain(cse_links, cap=4)\n",
    "cse_links = prefer_dated_urls(cse_links, top_n=MAX_CSE_PAGES)\n",
    "print(f\"CSE recent targets: {len(cse_links)}\")\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Fetch + store raw pages (JSONL) with progress + timeouts\n",
    "# ------------------------------------------------------------\n",
    "RAW_PATH = ART_DIR / f\"recent_raw_pages_{run_id}.jsonl\"\n",
    "RAW_PATH.write_text(\"\", encoding=\"utf-8\")\n",
    "\n",
    "def append_jsonl(path: Path, obj: dict):\n",
    "    with path.open(\"a\", encoding=\"utf-8\") as f:\n",
    "        f.write(json.dumps(obj, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "def collect_pages(urls: list[str], source_type: str, limit: int | None = None):\n",
    "    fetched = 0\n",
    "    urls2 = urls[:limit] if limit else urls\n",
    "\n",
    "    for i, u in enumerate(urls2, start=1):\n",
    "        u = normalize_url(u)\n",
    "        if not u or is_asset_url(u):\n",
    "            continue\n",
    "\n",
    "        print(f\"[{source_type}] {i}/{len(urls2)} fetching: {u}\")\n",
    "\n",
    "        rp = rp_official if source_type == \"official\" else get_rp_for_url(u)\n",
    "        allowed = robots_allows(rp, u)\n",
    "\n",
    "        if not allowed:\n",
    "            append_jsonl(RAW_PATH, {\n",
    "                \"url\": u,\n",
    "                \"source_type\": source_type,\n",
    "                \"domain\": get_domain(u),\n",
    "                \"fetched_at_utc\": datetime.now(timezone.utc).isoformat(),\n",
    "                \"http_status\": None,\n",
    "                \"robots_allowed\": False,\n",
    "                \"text\": \"\",\n",
    "                \"notes\": \"Skipped due to robots.txt (or conservative fallback).\",\n",
    "            })\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            code, html = fetch_html(u)\n",
    "            time.sleep(SLEEP_SEC)\n",
    "            text = html_to_text(html) if html else \"\"\n",
    "            text = text[:MAX_TEXT_CHARS_STORE]\n",
    "\n",
    "            append_jsonl(RAW_PATH, {\n",
    "                \"url\": u,\n",
    "                \"source_type\": source_type,\n",
    "                \"domain\": get_domain(u),\n",
    "                \"fetched_at_utc\": datetime.now(timezone.utc).isoformat(),\n",
    "                \"http_status\": code,\n",
    "                \"robots_allowed\": True,\n",
    "                \"text\": text,\n",
    "                \"text_char_len\": len(text),\n",
    "            })\n",
    "            fetched += 1\n",
    "\n",
    "        except Exception as e:\n",
    "            append_jsonl(RAW_PATH, {\n",
    "                \"url\": u,\n",
    "                \"source_type\": source_type,\n",
    "                \"domain\": get_domain(u),\n",
    "                \"fetched_at_utc\": datetime.now(timezone.utc).isoformat(),\n",
    "                \"http_status\": None,\n",
    "                \"robots_allowed\": True,\n",
    "                \"text\": \"\",\n",
    "                \"notes\": f\"Fetch error: {type(e).__name__}: {str(e)}\",\n",
    "            })\n",
    "            print(f\"  -> skip (error): {type(e).__name__}: {e}\")\n",
    "\n",
    "    print(f\"Done fetching {source_type}: saved {fetched} pages (errors/blocked recorded).\")\n",
    "\n",
    "# Stage fetching (so it never \"hangs\" silently)\n",
    "collect_pages(official_targets, \"official\")\n",
    "collect_pages(cse_links, \"cse\", limit=CSE_FETCH_LIMIT_STAGE1)\n",
    "\n",
    "print(f\"✅ Recent raw pages saved: {RAW_PATH.as_posix()}\")\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Load raw pages + shortlist to LLM (favor pages that look recent)\n",
    "# ------------------------------------------------------------\n",
    "def load_raw_pages(path: Path) -> list[dict]:\n",
    "    rows = []\n",
    "    for line in path.read_text(encoding=\"utf-8\").splitlines():\n",
    "        if line.strip():\n",
    "            rows.append(json.loads(line))\n",
    "    return rows\n",
    "\n",
    "raw_pages = load_raw_pages(RAW_PATH)\n",
    "\n",
    "usable = [\n",
    "    r for r in raw_pages\n",
    "    if r.get(\"robots_allowed\")\n",
    "    and isinstance(r.get(\"http_status\"), int) and r[\"http_status\"] < 400\n",
    "    and (r.get(\"text\") or \"\").strip()\n",
    "]\n",
    "\n",
    "def looks_recent(text: str) -> bool:\n",
    "    if not text:\n",
    "        return False\n",
    "    # weak heuristics; LLM will do actual date extraction\n",
    "    return bool(re.search(r\"\\b(2026|2025)\\b\", text))\n",
    "\n",
    "def score_page_for_recent(r):\n",
    "    url = r.get(\"url\") or \"\"\n",
    "    text = r.get(\"text\") or \"\"\n",
    "    p = (urlparse(url).path or \"\").lower()\n",
    "    s = 0\n",
    "    for k in RECENT_KEYWORDS:\n",
    "        if k in p:\n",
    "            s += 2\n",
    "    if re.search(r\"/20\\d{2}/\\d{1,2}/\", p) or re.search(r\"/20\\d{2}/\", p):\n",
    "        s += 2\n",
    "    if looks_recent(text):\n",
    "        s += 1\n",
    "    s += min(len(text), 20000) / 8000\n",
    "    if r.get(\"source_type\") == \"official\":\n",
    "        s += 2\n",
    "    return s\n",
    "\n",
    "usable_sorted = sorted(usable, key=score_page_for_recent, reverse=True)\n",
    "shortlist = usable_sorted[:MAX_PAGES_TO_LLM]\n",
    "print(f\"Usable pages: {len(usable)} | Shortlist to LLM: {len(shortlist)}\")\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Robust JSON parsing helper\n",
    "# ------------------------------------------------------------\n",
    "def _extract_json_object(text: str) -> str:\n",
    "    if not text:\n",
    "        return \"\"\n",
    "    t = text.strip()\n",
    "    m = re.search(r\"```(?:json)?\\s*(\\{.*?\\})\\s*```\", t, flags=re.DOTALL | re.IGNORECASE)\n",
    "    if m:\n",
    "        return m.group(1).strip()\n",
    "    start, end = t.find(\"{\"), t.rfind(\"}\")\n",
    "    if start != -1 and end != -1 and end > start:\n",
    "        return t[start:end+1].strip()\n",
    "    return \"\"\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# OpenAI: timeline extraction (events + content index)\n",
    "# ------------------------------------------------------------\n",
    "def openai_extract_recent_timeline(pages: list[dict], cutoff_date: str) -> dict:\n",
    "    system = (\n",
    "        \"You are a research analyst assistant. \"\n",
    "        \"From the provided webpage texts, extract a timeline of recent changes in the last few months. \"\n",
    "        \"CRITICAL: Always produce a non-empty content_index if there are relevant pages. \"\n",
    "        \"Prefer events on/after the cutoff date, but if date is missing, keep as month-level or unknown date with lower confidence. \"\n",
    "        \"Do not invent dates. Use only what appears in the text. \"\n",
    "        \"Return a single JSON object only (no markdown).\"\n",
    "    )\n",
    "\n",
    "    schema = {\n",
    "        \"cutoff_date\": \"string (YYYY-MM-DD)\",\n",
    "        \"timeline_events\": [\n",
    "            {\n",
    "                \"date\": \"string|null (YYYY-MM-DD if known, else YYYY-MM or null)\",\n",
    "                \"event_type\": \"product|research|partnership|customer|funding|hiring|org_change|policy|other\",\n",
    "                \"headline\": \"string\",\n",
    "                \"details\": \"string\",\n",
    "                \"evidence_url\": \"string\",\n",
    "                \"source_type\": \"official|cse\",\n",
    "                \"confidence\": \"0..1\"\n",
    "            }\n",
    "        ],\n",
    "        \"content_index\": [\n",
    "            {\n",
    "                \"content_type\": \"press|blog|research|article|interview|event_page|other\",\n",
    "                \"title\": \"string|null\",\n",
    "                \"publisher_or_platform\": \"string|null\",\n",
    "                \"date\": \"string|null\",\n",
    "                \"url\": \"string\",\n",
    "                \"what_it_contains\": \"string\",\n",
    "                \"relevance\": \"0..1\",\n",
    "                \"confidence\": \"0..1\"\n",
    "            }\n",
    "        ],\n",
    "        \"notes\": {\"data_gaps\": \"array of strings\", \"conflicts\": \"array of strings\"},\n",
    "        \"confidence\": {\"overall\": \"0..1\", \"rationale\": \"string\"}\n",
    "    }\n",
    "\n",
    "    bundle = {\n",
    "        \"startup_name\": startup_name,\n",
    "        \"official_website\": official_home,\n",
    "        \"cutoff_date\": cutoff_date,\n",
    "        \"pages\": [\n",
    "            {\n",
    "                \"url\": p[\"url\"],\n",
    "                \"source_type\": p.get(\"source_type\"),\n",
    "                \"domain\": p.get(\"domain\"),\n",
    "                \"text\": (p.get(\"text\") or \"\")[:MAX_TEXT_CHARS_PROMPT],\n",
    "            } for p in pages\n",
    "        ],\n",
    "        \"schema\": schema,\n",
    "        \"instructions\": [\n",
    "            \"Extract events that are likely in the last few months.\",\n",
    "            \"If a page is older but referenced as context, keep it out of timeline_events (or mark low relevance).\",\n",
    "            \"For content_index, include at least 10 items if available.\"\n",
    "        ]\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        resp = client.chat.completions.create(\n",
    "            model=\"gpt-4o-mini\",\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": system},\n",
    "                {\"role\": \"user\", \"content\": json.dumps(bundle, ensure_ascii=False)},\n",
    "            ],\n",
    "            temperature=0.0,\n",
    "            response_format={\"type\": \"json_object\"},\n",
    "        )\n",
    "        return json.loads((resp.choices[0].message.content or \"\").strip())\n",
    "    except Exception:\n",
    "        resp2 = client.chat.completions.create(\n",
    "            model=\"gpt-4o-mini\",\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": system},\n",
    "                {\"role\": \"user\", \"content\": json.dumps(bundle, ensure_ascii=False)},\n",
    "            ],\n",
    "            temperature=0.0,\n",
    "        )\n",
    "        raw = resp2.choices[0].message.content or \"\"\n",
    "        candidate = _extract_json_object(raw)\n",
    "        if candidate:\n",
    "            return json.loads(candidate)\n",
    "        raise RuntimeError(\"Model did not return a JSON object.\")\n",
    "\n",
    "extracted = openai_extract_recent_timeline(shortlist, cutoff_date=CUTOFF_STR)\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Save JSON + DataFrames\n",
    "# ------------------------------------------------------------\n",
    "JSON_PATH = ART_DIR / f\"recent_timeline_extraction_{run_id}.json\"\n",
    "JSON_PATH.write_text(json.dumps(extracted, ensure_ascii=False, indent=2), encoding=\"utf-8\")\n",
    "\n",
    "events = extracted.get(\"timeline_events\", []) or []\n",
    "content = extracted.get(\"content_index\", []) or []\n",
    "\n",
    "df_events = pd.DataFrame(events)\n",
    "df_content = pd.DataFrame(content)\n",
    "\n",
    "if not df_events.empty:\n",
    "    df_events.insert(0, \"run_id\", run_id)\n",
    "if not df_content.empty:\n",
    "    df_content.insert(0, \"run_id\", run_id)\n",
    "\n",
    "EVENTS_CSV = ART_DIR / f\"recent_timeline_{run_id}.csv\"\n",
    "CONTENT_CSV = ART_DIR / f\"recent_content_index_{run_id}.csv\"\n",
    "\n",
    "df_events.to_csv(EVENTS_CSV, index=False)\n",
    "df_content.to_csv(CONTENT_CSV, index=False)\n",
    "\n",
    "print(\"✅ Recent Changes Timeline complete\")\n",
    "print(f\"- Raw pages: {RAW_PATH.as_posix()}\")\n",
    "print(f\"- JSON: {JSON_PATH.as_posix()}\")\n",
    "print(f\"- Timeline CSV: {EVENTS_CSV.as_posix()}\")\n",
    "print(f\"- Content CSV: {CONTENT_CSV.as_posix()}\")\n",
    "\n",
    "# Optional: sort events by date (best-effort)\n",
    "if not df_events.empty and \"date\" in df_events.columns:\n",
    "    df_events_sorted = df_events.copy()\n",
    "    df_events_sorted[\"date_sort\"] = df_events_sorted[\"date\"].fillna(\"\").astype(str)\n",
    "    df_events_sorted = df_events_sorted.sort_values(\"date_sort\", ascending=False).drop(columns=[\"date_sort\"])\n",
    "    display(df_events_sorted.head(50))\n",
    "else:\n",
    "    display(df_events)\n",
    "\n",
    "display(df_content.head(30) if not df_content.empty else df_content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9af8b58c-4759-4ae0-abb6-024360bd8540",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run_id: 20260107_225628\n",
      "startup_name: Sakana AI\n",
      "official_website: https://sakana.ai/series-a\n",
      "✅ Evidence map saved: artifacts/meeting_deep_dive/integrated_evidence_map_20260107_225628.csv\n",
      "✅ Integrated insights JSON saved: artifacts/meeting_deep_dive/integrated_insights_20260107_225628.json\n",
      "✅ Integrated insights MD saved: artifacts/meeting_deep_dive/integrated_insights_20260107_225628.md\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>startup_name</th>\n",
       "      <th>confidence_overall</th>\n",
       "      <th>summary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Sakana AI</td>\n",
       "      <td>0.85</td>\n",
       "      <td>Sakana AI is a Tokyo-based startup focused on ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  startup_name  confidence_overall  \\\n",
       "0    Sakana AI                0.85   \n",
       "\n",
       "                                             summary  \n",
       "0  Sakana AI is a Tokyo-based startup focused on ...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top takeaways:\n",
      "- Sakana AI raised approximately $200M in its Series A funding round (https://sakana.ai/series-a/).\n",
      "- Sakana AI raised $135M in its Series B funding round at a valuation of $2.635 billion (https://siliconangle.com/2025/11/17/sakana-ai-lands-135m-2-635b-valuation-accelerate-frontier-research-applied-ai-japan/).\n",
      "- The company has developed the AI Scientist, capable of fully automated scientific discovery (https://sakana.ai/ai-scientist/).\n",
      "- Sakana AI's technology is being applied in the banking sector to automate decision-making processes (https://sakana.ai/mufg-bank/).\n",
      "\n",
      "Top diligence priorities:\n",
      "- 1. Validate the performance and adoption of the AI Scientist in real-world applications.\n",
      "- 2. Assess the competitive landscape and Sakana AI's differentiation strategy.\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# 11. Integrated Insights (Synthesis across all sections)\n",
    "# ============================================================\n",
    "# Goal:\n",
    "# - Integrate outputs from sections #3–#10 into a single, inspection-ready insight pack:\n",
    "#   - What is confidently true (facts with evidence)\n",
    "#   - What is likely (high-confidence hypotheses)\n",
    "#   - What is uncertain / missing (gaps + conflicting signals)\n",
    "#   - Implications for the meeting (what to probe, what to validate)\n",
    "#\n",
    "# Approach:\n",
    "# - Load latest artifacts (JSON/CSV) from prior sections (if available)\n",
    "# - Build a compact \"evidence bundle\" to send to OpenAI\n",
    "# - Ask the model to produce:\n",
    "#   1) integrated_summary (1–2 paragraphs)\n",
    "#   2) key_takeaways (bullets)\n",
    "#   3) strengths / risks (bullets)\n",
    "#   4) hypotheses_to_test (bullets)\n",
    "#   5) due_diligence_priorities (ranked)\n",
    "#   6) contradictions & data gaps\n",
    "#   7) evidence_map (top URLs by theme)\n",
    "#\n",
    "# Outputs:\n",
    "# - integrated_insights_<run_id>.json\n",
    "# - integrated_insights_<run_id>.md (optional human-readable)\n",
    "# - integrated_evidence_map_<run_id>.csv\n",
    "\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "import glob\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from datetime import datetime, timezone\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Global configuration\n",
    "# ------------------------------------------------------------\n",
    "load_dotenv(\"env.txt\")\n",
    "\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "if not OPENAI_API_KEY:\n",
    "    raise EnvironmentError(\"OPENAI_API_KEY is not set.\")\n",
    "\n",
    "client = OpenAI(api_key=OPENAI_API_KEY)\n",
    "\n",
    "ART_DIR = Path(\"artifacts\") / \"meeting_deep_dive\"\n",
    "ART_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Helpers: find latest artifacts\n",
    "# ------------------------------------------------------------\n",
    "def latest_path(pattern: str) -> Path | None:\n",
    "    paths = sorted(ART_DIR.glob(pattern), key=lambda p: p.stat().st_mtime, reverse=True)\n",
    "    return paths[0] if paths else None\n",
    "\n",
    "def safe_read_json(path: Path | None) -> dict | None:\n",
    "    if not path or not path.exists():\n",
    "        return None\n",
    "    return json.loads(path.read_text(encoding=\"utf-8\"))\n",
    "\n",
    "def safe_read_csv(path: Path | None) -> pd.DataFrame | None:\n",
    "    if not path or not path.exists():\n",
    "        return None\n",
    "    try:\n",
    "        return pd.read_csv(path)\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "def df_head_records(df: pd.DataFrame | None, n: int = 30) -> list[dict]:\n",
    "    if df is None or df.empty:\n",
    "        return []\n",
    "    return df.head(n).to_dict(orient=\"records\")\n",
    "\n",
    "def normalize_url(u: str) -> str:\n",
    "    u = (u or \"\").strip()\n",
    "    u = re.sub(r\"#.*$\", \"\", u)\n",
    "    return u\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Determine run_id + core entity\n",
    "# ------------------------------------------------------------\n",
    "inputs_path = latest_path(\"inputs_*.json\")\n",
    "entity_path = latest_path(\"entity_*.json\")\n",
    "\n",
    "inputs = safe_read_json(inputs_path) or {}\n",
    "entity = safe_read_json(entity_path) or {}\n",
    "\n",
    "run_id = (inputs.get(\"meta\", {}) or {}).get(\"run_id\") or datetime.now(timezone.utc).strftime(\"%Y%m%d_%H%M%S\")\n",
    "startup_name = (entity.get(\"canonical_name\") or inputs.get(\"startup_name\") or \"Unknown Startup\").strip()\n",
    "official_website = (entity.get(\"official_website\") or \"\").strip()\n",
    "\n",
    "print(\"run_id:\", run_id)\n",
    "print(\"startup_name:\", startup_name)\n",
    "print(\"official_website:\", official_website)\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Load prior artifacts (best-effort)\n",
    "# ------------------------------------------------------------\n",
    "# 3) Company Basics\n",
    "company_extraction = safe_read_json(latest_path(\"company_extraction_*.json\")) or {}\n",
    "df_company_claims = safe_read_csv(latest_path(\"company_claims_*.csv\"))\n",
    "df_company_content = safe_read_csv(latest_path(\"company_content_index_*.csv\"))\n",
    "\n",
    "# 4) Key People\n",
    "people_extraction = safe_read_json(latest_path(\"people_extraction_*.json\")) or {}\n",
    "df_people_directory = safe_read_csv(latest_path(\"people_directory_*.csv\"))\n",
    "# 5) Meeting Person Deep Dive\n",
    "deep_dive = safe_read_json(latest_path(\"meeting_person_deep_dive_*.json\")) or {}\n",
    "df_people_profiles = safe_read_csv(latest_path(\"people_profiles_*.csv\"))\n",
    "df_people_content = safe_read_csv(latest_path(\"people_content_index_*.csv\"))\n",
    "\n",
    "# 6) Business & Product\n",
    "bp = safe_read_json(latest_path(\"business_product_extraction_*.json\")) or {}\n",
    "df_bp_claims = safe_read_csv(latest_path(\"business_product_claims_*.csv\"))\n",
    "df_bp_content = safe_read_csv(latest_path(\"business_product_content_index_*.csv\"))\n",
    "\n",
    "# 7) Customer & Market\n",
    "mk = safe_read_json(latest_path(\"market_extraction_*.json\")) or {}\n",
    "df_mk_claims = safe_read_csv(latest_path(\"market_claims_*.csv\"))\n",
    "df_mk_content = safe_read_csv(latest_path(\"market_content_index_*.csv\"))\n",
    "\n",
    "# 8) Competitive\n",
    "cp = safe_read_json(latest_path(\"competitive_extraction_*.json\")) or {}\n",
    "df_cp_competitors = safe_read_csv(latest_path(\"competitor_table_*.csv\"))\n",
    "df_cp_claims = safe_read_csv(latest_path(\"competitive_claims_*.csv\"))\n",
    "df_cp_content = safe_read_csv(latest_path(\"competitive_content_index_*.csv\"))\n",
    "\n",
    "# 9) Funding\n",
    "fd = safe_read_json(latest_path(\"funding_extraction_*.json\")) or {}\n",
    "df_fd_events = safe_read_csv(latest_path(\"funding_events_*.csv\"))\n",
    "df_fd_claims = safe_read_csv(latest_path(\"funding_claims_*.csv\"))\n",
    "df_fd_content = safe_read_csv(latest_path(\"funding_content_index_*.csv\"))\n",
    "\n",
    "# 10) Recent Timeline\n",
    "recent = safe_read_json(latest_path(\"recent_timeline_extraction_*.json\")) or {}\n",
    "df_recent_timeline = safe_read_csv(latest_path(\"recent_timeline_*.csv\"))\n",
    "df_recent_content = safe_read_csv(latest_path(\"recent_content_index_*.csv\"))\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Build an \"evidence map\" (top URLs grouped by theme)\n",
    "# ------------------------------------------------------------\n",
    "def collect_urls(df: pd.DataFrame | None, cols: list[str], limit: int = 50) -> list[str]:\n",
    "    if df is None or df.empty:\n",
    "        return []\n",
    "    urls = []\n",
    "    for c in cols:\n",
    "        if c in df.columns:\n",
    "            urls.extend([normalize_url(x) for x in df[c].dropna().astype(str).tolist()])\n",
    "    # unique preserve order\n",
    "    seen, out = set(), []\n",
    "    for u in urls:\n",
    "        if u and u.startswith(\"http\") and u not in seen:\n",
    "            seen.add(u)\n",
    "            out.append(u)\n",
    "        if len(out) >= limit:\n",
    "            break\n",
    "    return out\n",
    "\n",
    "evidence_map = {\n",
    "    \"company_basics\": collect_urls(df_company_claims, [\"evidence_url\"], 30) + collect_urls(df_company_content, [\"url\"], 30),\n",
    "    \"people\": collect_urls(df_people_content, [\"url\"], 30),\n",
    "    \"business_product\": collect_urls(df_bp_claims, [\"evidence_url\"], 30) + collect_urls(df_bp_content, [\"url\"], 30),\n",
    "    \"market\": collect_urls(df_mk_claims, [\"evidence_url\"], 30) + collect_urls(df_mk_content, [\"url\"], 30),\n",
    "    \"competition\": collect_urls(df_cp_claims, [\"evidence_url\"], 30) + collect_urls(df_cp_content, [\"url\"], 30),\n",
    "    \"funding\": collect_urls(df_fd_claims, [\"evidence_url\"], 30) + collect_urls(df_fd_content, [\"url\"], 30),\n",
    "    \"recent_changes\": collect_urls(df_recent_timeline, [\"evidence_url\"], 30) + collect_urls(df_recent_content, [\"url\"], 30),\n",
    "}\n",
    "# dedupe within each theme\n",
    "for k, v in evidence_map.items():\n",
    "    seen, out = set(), []\n",
    "    for u in v:\n",
    "        if u not in seen:\n",
    "            seen.add(u)\n",
    "            out.append(u)\n",
    "    evidence_map[k] = out[:40]\n",
    "\n",
    "df_evidence_map = pd.DataFrame(\n",
    "    [{\"theme\": theme, \"url\": url} for theme, urls in evidence_map.items() for url in urls]\n",
    ")\n",
    "\n",
    "EVIDENCE_CSV = ART_DIR / f\"integrated_evidence_map_{run_id}.csv\"\n",
    "df_evidence_map.to_csv(EVIDENCE_CSV, index=False)\n",
    "print(\"✅ Evidence map saved:\", EVIDENCE_CSV.as_posix())\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Build the LLM bundle (keep compact to avoid token blowups)\n",
    "# ------------------------------------------------------------\n",
    "bundle = {\n",
    "    \"startup_name\": startup_name,\n",
    "    \"official_website\": official_website,\n",
    "    \"inputs\": {\n",
    "        \"meeting_person_name\": (inputs.get(\"meeting_person_name\") if isinstance(inputs, dict) else None),\n",
    "        \"meeting_context\": (inputs.get(\"meeting_context\") if isinstance(inputs, dict) else None),\n",
    "        \"your_org_context\": (inputs.get(\"your_org_context\") if isinstance(inputs, dict) else None),\n",
    "    },\n",
    "    \"section_snapshots\": {\n",
    "        \"company_basics\": {\n",
    "            \"summary\": company_extraction.get(\"company_summary\") if isinstance(company_extraction, dict) else None,\n",
    "            \"claims_head\": df_head_records(df_company_claims, 25),\n",
    "        },\n",
    "        \"people\": {\n",
    "            \"meeting_deep_dive\": deep_dive.get(\"meeting_person_insights\") if isinstance(deep_dive, dict) else None,\n",
    "            \"profiles_head\": df_head_records(df_people_profiles, 12),\n",
    "            \"content_head\": df_head_records(df_people_content, 15),\n",
    "        },\n",
    "        \"business_product\": {\n",
    "            \"summary\": (bp.get(\"business_product_summary\") if isinstance(bp, dict) else None),\n",
    "            \"claims_head\": df_head_records(df_bp_claims, 25),\n",
    "        },\n",
    "        \"market\": {\n",
    "            \"summary\": (mk.get(\"market_summary\") if isinstance(mk, dict) else None),\n",
    "            \"claims_head\": df_head_records(df_mk_claims, 25),\n",
    "        },\n",
    "        \"competition\": {\n",
    "            \"summary\": (cp.get(\"competitive_summary\") if isinstance(cp, dict) else None),\n",
    "            \"competitors_head\": df_head_records(df_cp_competitors, 20),\n",
    "            \"claims_head\": df_head_records(df_cp_claims, 20),\n",
    "        },\n",
    "        \"funding\": {\n",
    "            \"summary\": (fd.get(\"funding_summary\") if isinstance(fd, dict) else None),\n",
    "            \"events_head\": df_head_records(df_fd_events, 15),\n",
    "            \"claims_head\": df_head_records(df_fd_claims, 20),\n",
    "        },\n",
    "        \"recent_changes\": {\n",
    "            \"cutoff_date\": recent.get(\"cutoff_date\") if isinstance(recent, dict) else None,\n",
    "            \"timeline_head\": df_head_records(df_recent_timeline, 25),\n",
    "        },\n",
    "    },\n",
    "    \"evidence_map\": evidence_map,\n",
    "}\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# OpenAI: Integrated synthesis (facts vs hypotheses)\n",
    "# ------------------------------------------------------------\n",
    "def _extract_json_object(text: str) -> str:\n",
    "    if not text:\n",
    "        return \"\"\n",
    "    t = text.strip()\n",
    "    m = re.search(r\"```(?:json)?\\s*(\\{.*?\\})\\s*```\", t, flags=re.DOTALL | re.IGNORECASE)\n",
    "    if m:\n",
    "        return m.group(1).strip()\n",
    "    start, end = t.find(\"{\"), t.rfind(\"}\")\n",
    "    if start != -1 and end != -1 and end > start:\n",
    "        return t[start:end+1].strip()\n",
    "    return \"\"\n",
    "\n",
    "def openai_integrated_insights(bundle: dict) -> dict:\n",
    "    system = (\n",
    "        \"You are a diligence analyst assistant. \"\n",
    "        \"Synthesize the provided multi-section evidence bundle into integrated insights for a startup meeting.\\n\\n\"\n",
    "        \"CRITICAL RULES:\\n\"\n",
    "        \"- Separate facts (with URLs) from hypotheses.\\n\"\n",
    "        \"- Never invent details (no made-up customers, funding numbers, or competitive claims).\\n\"\n",
    "        \"- If evidence is thin or contradictory, call it out.\\n\"\n",
    "        \"- Prefer concise, decision-useful writing.\\n\"\n",
    "        \"Return a single JSON object only (no markdown).\"\n",
    "    )\n",
    "\n",
    "    schema = {\n",
    "        \"integrated_summary\": \"string (1-2 short paragraphs)\",\n",
    "        \"key_takeaways\": [\"array of strings (facts-first)\"],\n",
    "        \"strengths\": [\"array of strings\"],\n",
    "        \"risks_watchouts\": [\"array of strings\"],\n",
    "        \"hypotheses_to_test\": [\"array of strings\"],\n",
    "        \"due_diligence_priorities\": [\n",
    "            {\"priority\": \"1..10\", \"item\": \"string\", \"why\": \"string\", \"evidence_urls\": \"array of strings\"}\n",
    "        ],\n",
    "        \"contradictions\": [\n",
    "            {\"topic\": \"string\", \"conflicting_points\": \"array of strings\", \"evidence_urls\": \"array of strings\"}\n",
    "        ],\n",
    "        \"data_gaps\": [\n",
    "            {\"topic\": \"string\", \"what_is_missing\": \"string\", \"how_to_verify\": \"string\"}\n",
    "        ],\n",
    "        \"meeting_implications\": {\n",
    "            \"what_to_probe\": [\"array of strings\"],\n",
    "            \"what_to_validate_fast\": [\"array of strings\"],\n",
    "            \"what_to_defer\": [\"array of strings\"]\n",
    "        },\n",
    "        \"evidence_map_top\": [\n",
    "            {\"theme\": \"string\", \"top_urls\": \"array of strings\", \"why_these\": \"string\"}\n",
    "        ],\n",
    "        \"confidence\": {\"overall\": \"0..1\", \"rationale\": \"string\"}\n",
    "    }\n",
    "\n",
    "    payload = {\"schema\": schema, \"bundle\": bundle}\n",
    "\n",
    "    try:\n",
    "        resp = client.chat.completions.create(\n",
    "            model=\"gpt-4o-mini\",\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": system},\n",
    "                {\"role\": \"user\", \"content\": json.dumps(payload, ensure_ascii=False)},\n",
    "            ],\n",
    "            temperature=0.2,\n",
    "            response_format={\"type\": \"json_object\"},\n",
    "        )\n",
    "        return json.loads((resp.choices[0].message.content or \"\").strip())\n",
    "    except Exception:\n",
    "        resp2 = client.chat.completions.create(\n",
    "            model=\"gpt-4o-mini\",\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": system},\n",
    "                {\"role\": \"user\", \"content\": json.dumps(payload, ensure_ascii=False)},\n",
    "            ],\n",
    "            temperature=0.2,\n",
    "        )\n",
    "        raw = resp2.choices[0].message.content or \"\"\n",
    "        candidate = _extract_json_object(raw)\n",
    "        if candidate:\n",
    "            return json.loads(candidate)\n",
    "        raise RuntimeError(\"Model did not return a JSON object.\")\n",
    "\n",
    "integrated = openai_integrated_insights(bundle)\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Save outputs\n",
    "# ------------------------------------------------------------\n",
    "JSON_PATH = ART_DIR / f\"integrated_insights_{run_id}.json\"\n",
    "JSON_PATH.write_text(json.dumps(integrated, ensure_ascii=False, indent=2), encoding=\"utf-8\")\n",
    "print(\"✅ Integrated insights JSON saved:\", JSON_PATH.as_posix())\n",
    "\n",
    "# Optional: human-readable markdown\n",
    "def to_md(x: dict) -> str:\n",
    "    lines = []\n",
    "    lines.append(f\"# Integrated Insights: {startup_name}\")\n",
    "    lines.append(\"\")\n",
    "    lines.append(\"## Integrated Summary\")\n",
    "    lines.append(x.get(\"integrated_summary\", \"\"))\n",
    "    lines.append(\"\")\n",
    "    lines.append(\"## Key Takeaways\")\n",
    "    for b in x.get(\"key_takeaways\", [])[:12]:\n",
    "        lines.append(f\"- {b}\")\n",
    "    lines.append(\"\")\n",
    "    lines.append(\"## Strengths\")\n",
    "    for b in x.get(\"strengths\", [])[:12]:\n",
    "        lines.append(f\"- {b}\")\n",
    "    lines.append(\"\")\n",
    "    lines.append(\"## Risks / Watchouts\")\n",
    "    for b in x.get(\"risks_watchouts\", [])[:12]:\n",
    "        lines.append(f\"- {b}\")\n",
    "    lines.append(\"\")\n",
    "    lines.append(\"## Hypotheses to Test\")\n",
    "    for b in x.get(\"hypotheses_to_test\", [])[:12]:\n",
    "        lines.append(f\"- {b}\")\n",
    "    lines.append(\"\")\n",
    "    lines.append(\"## Due Diligence Priorities\")\n",
    "    for r in x.get(\"due_diligence_priorities\", [])[:10]:\n",
    "        lines.append(f\"- **{r.get('priority')}** {r.get('item')} — {r.get('why')}\")\n",
    "        urls = r.get(\"evidence_urls\") or []\n",
    "        if urls:\n",
    "            for u in urls[:3]:\n",
    "                lines.append(f\"  - {u}\")\n",
    "    lines.append(\"\")\n",
    "    lines.append(\"## Contradictions\")\n",
    "    for c in x.get(\"contradictions\", [])[:10]:\n",
    "        lines.append(f\"- **{c.get('topic')}**\")\n",
    "        for p in c.get(\"conflicting_points\", [])[:4]:\n",
    "            lines.append(f\"  - {p}\")\n",
    "    lines.append(\"\")\n",
    "    lines.append(\"## Data Gaps\")\n",
    "    for g in x.get(\"data_gaps\", [])[:12]:\n",
    "        lines.append(f\"- **{g.get('topic')}**: {g.get('what_is_missing')}\")\n",
    "        lines.append(f\"  - Verify: {g.get('how_to_verify')}\")\n",
    "    lines.append(\"\")\n",
    "    return \"\\n\".join(lines)\n",
    "\n",
    "MD_PATH = ART_DIR / f\"integrated_insights_{run_id}.md\"\n",
    "MD_PATH.write_text(to_md(integrated), encoding=\"utf-8\")\n",
    "print(\"✅ Integrated insights MD saved:\", MD_PATH.as_posix())\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Display\n",
    "# ------------------------------------------------------------\n",
    "display(pd.DataFrame([{\n",
    "    \"startup_name\": startup_name,\n",
    "    \"confidence_overall\": (integrated.get(\"confidence\", {}) or {}).get(\"overall\"),\n",
    "    \"summary\": integrated.get(\"integrated_summary\", \"\")[:240] + (\"...\" if len(integrated.get(\"integrated_summary\", \"\")) > 240 else \"\")\n",
    "}]))\n",
    "\n",
    "print(\"\\nTop takeaways:\")\n",
    "for b in integrated.get(\"key_takeaways\", [])[:8]:\n",
    "    print(\"-\", b)\n",
    "\n",
    "print(\"\\nTop diligence priorities:\")\n",
    "for r in integrated.get(\"due_diligence_priorities\", [])[:6]:\n",
    "    print(f\"- {r.get('priority')}. {r.get('item')}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0351641a-f058-425f-bc3a-0652299eac10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run_id: 20260107_225628\n",
      "startup_name: Sakana AI\n",
      "integrated_summary_len: 364\n",
      "takeaways_len: 4\n",
      "dd_priorities_len: 2\n",
      "top_pre_read_urls: 15\n",
      "⚠️ Invalid framing output: Framing output missing required keys. Likely you saved payload (schema/bundle) instead of model output, or the model returned an unexpected structure.\n",
      "Retrying once with stronger constraints...\n",
      "✅ Meeting context framing JSON saved: artifacts/meeting_deep_dive/meeting_context_framing_20260107_225628.json\n",
      "✅ Meeting context framing MD saved: artifacts/meeting_deep_dive/meeting_context_framing_20260107_225628.md\n",
      "\n",
      "objective: Evaluate the potential of Sakana AI for investment and strategic partnership opportunities.\n",
      "30-sec: Sakana AI, a leading Tokyo-based startup, is pioneering nature-inspired AI technologies with significant funding backing. With its innovative AI Scientist and applications in sectors like banking, it represents a compelling investment opportunity. This meeting aims to evaluate its market potential and strategic fit for our portfolio.\n",
      "agenda items: 5\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# 12. Meeting Context Framing (Fixed: save the MODEL OUTPUT, not the payload)\n",
    "# ============================================================\n",
    "\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from datetime import datetime, timezone\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Global configuration\n",
    "# ------------------------------------------------------------\n",
    "load_dotenv(\"env.txt\")\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "if not OPENAI_API_KEY:\n",
    "    raise EnvironmentError(\"OPENAI_API_KEY is not set.\")\n",
    "client = OpenAI(api_key=OPENAI_API_KEY)\n",
    "\n",
    "ART_DIR = Path(\"artifacts\") / \"meeting_deep_dive\"\n",
    "ART_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Helpers\n",
    "# ------------------------------------------------------------\n",
    "def latest_path(pattern: str) -> Path | None:\n",
    "    paths = sorted(ART_DIR.glob(pattern), key=lambda p: p.stat().st_mtime, reverse=True)\n",
    "    return paths[0] if paths else None\n",
    "\n",
    "def safe_read_json(path: Path | None) -> dict:\n",
    "    if not path or not path.exists():\n",
    "        return {}\n",
    "    return json.loads(path.read_text(encoding=\"utf-8\"))\n",
    "\n",
    "def safe_read_text(path: Path | None) -> str:\n",
    "    if not path or not path.exists():\n",
    "        return \"\"\n",
    "    return path.read_text(encoding=\"utf-8\")\n",
    "\n",
    "def _extract_json_object(text: str) -> str:\n",
    "    if not text:\n",
    "        return \"\"\n",
    "    t = text.strip()\n",
    "    m = re.search(r\"```(?:json)?\\s*(\\{.*?\\})\\s*```\", t, flags=re.DOTALL | re.IGNORECASE)\n",
    "    if m:\n",
    "        return m.group(1).strip()\n",
    "    start, end = t.find(\"{\"), t.rfind(\"}\")\n",
    "    if start != -1 and end != -1 and end > start:\n",
    "        return t[start:end+1].strip()\n",
    "    return \"\"\n",
    "\n",
    "def validate_framing(obj: dict):\n",
    "    \"\"\"\n",
    "    Fail fast if we accidentally saved payload, or model returned wrong structure.\n",
    "    \"\"\"\n",
    "    if not isinstance(obj, dict):\n",
    "        raise ValueError(\"Framing output is not a dict.\")\n",
    "    if \"meeting_frame\" not in obj or \"narrative\" not in obj or \"suggested_agenda\" not in obj:\n",
    "        raise ValueError(\n",
    "            \"Framing output missing required keys. \"\n",
    "            \"Likely you saved payload (schema/bundle) instead of model output, \"\n",
    "            \"or the model returned an unexpected structure.\"\n",
    "        )\n",
    "    mf = obj.get(\"meeting_frame\") or {}\n",
    "    nar = obj.get(\"narrative\") or {}\n",
    "    if not (mf.get(\"meeting_objective\") or \"\").strip():\n",
    "        raise ValueError(\"meeting_frame.meeting_objective is empty.\")\n",
    "    if not (nar.get(\"thirty_second\") or \"\").strip():\n",
    "        raise ValueError(\"narrative.thirty_second is empty.\")\n",
    "    if len(obj.get(\"suggested_agenda\") or []) < 3:\n",
    "        raise ValueError(\"suggested_agenda has < 3 items.\")\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Load inputs / entity / integrated insights\n",
    "# ------------------------------------------------------------\n",
    "inputs = safe_read_json(latest_path(\"inputs_*.json\"))\n",
    "entity = safe_read_json(latest_path(\"entity_*.json\"))\n",
    "\n",
    "run_id = (inputs.get(\"meta\", {}) or {}).get(\"run_id\") or datetime.now(timezone.utc).strftime(\"%Y%m%d_%H%M%S\")\n",
    "startup_name = (entity.get(\"canonical_name\") or inputs.get(\"startup_name\") or \"Unknown Startup\").strip()\n",
    "\n",
    "meeting_person_name = inputs.get(\"meeting_person_name\")\n",
    "meeting_person_title = inputs.get(\"meeting_person_title\")\n",
    "meeting_context = inputs.get(\"meeting_context\") or \"\"\n",
    "your_org_context = inputs.get(\"your_org_context\") or \"\"\n",
    "\n",
    "integrated = safe_read_json(latest_path(\"integrated_insights_*.json\"))\n",
    "integrated_md = safe_read_text(latest_path(\"integrated_insights_*.md\"))\n",
    "\n",
    "# Evidence map -> top URLs\n",
    "top_urls = []\n",
    "ev_path = latest_path(\"integrated_evidence_map_*.csv\")\n",
    "if ev_path and ev_path.exists():\n",
    "    try:\n",
    "        df_ev = pd.read_csv(ev_path)\n",
    "        if \"url\" in df_ev.columns:\n",
    "            top_urls = df_ev[\"url\"].dropna().astype(str).head(15).tolist()\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "bundle = {\n",
    "    \"startup_name\": startup_name,\n",
    "    \"meeting_person\": {\"name\": meeting_person_name, \"title\": meeting_person_title},\n",
    "    \"meeting_context\": meeting_context,\n",
    "    \"your_org_context\": your_org_context,\n",
    "    \"integrated_insights\": integrated,\n",
    "    \"integrated_insights_md_excerpt\": integrated_md[:6000],\n",
    "    \"top_pre_read_urls\": top_urls\n",
    "}\n",
    "\n",
    "print(\"run_id:\", run_id)\n",
    "print(\"startup_name:\", startup_name)\n",
    "print(\"integrated_summary_len:\", len((integrated.get(\"integrated_summary\") or \"\")))\n",
    "print(\"takeaways_len:\", len(integrated.get(\"key_takeaways\", []) or []))\n",
    "print(\"dd_priorities_len:\", len(integrated.get(\"due_diligence_priorities\", []) or []))\n",
    "print(\"top_pre_read_urls:\", len(top_urls))\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# OpenAI: Meeting framing\n",
    "# ------------------------------------------------------------\n",
    "def openai_meeting_context_framing(bundle: dict, retry_mode: bool = False) -> dict:\n",
    "    system = (\n",
    "        \"You are a venture diligence assistant. Create a meeting-ready context framing using the provided insights.\\n\\n\"\n",
    "        \"NON-NEGOTIABLE RULES:\\n\"\n",
    "        \"- Do NOT return empty strings. If unknown, write 'UNKNOWN' and list assumptions.\\n\"\n",
    "        \"- Always produce: meeting_objective, 30-sec narrative, and at least 5 agenda items.\\n\"\n",
    "        \"- Separate facts vs hypotheses.\\n\"\n",
    "        \"- Do not invent numbers/customers/investors.\\n\"\n",
    "        \"Return a single JSON object only (no markdown).\"\n",
    "    )\n",
    "    if retry_mode:\n",
    "        system += (\n",
    "            \"\\n\\nRETRY MODE: previous output was invalid/empty. \"\n",
    "            \"You MUST fill every field with useful content. \"\n",
    "            \"If context is missing, assume a generic first meeting and label assumptions.\"\n",
    "        )\n",
    "\n",
    "    schema = {\n",
    "        \"meeting_frame\": {\n",
    "            \"meeting_objective\": \"string\",\n",
    "            \"why_now\": \"string\",\n",
    "            \"success_criteria\": [\"array of strings (>=3)\"],\n",
    "            \"key_themes\": [\"array of strings (>=4)\"],\n",
    "            \"facts_to_anchor\": [\"array of strings (>=4)\"],\n",
    "            \"hypotheses_to_validate\": [\"array of strings (>=4)\"],\n",
    "            \"risks_to_probe\": [\"array of strings (>=4)\"]\n",
    "        },\n",
    "        \"narrative\": {\n",
    "            \"thirty_second\": \"string\",\n",
    "            \"two_minute\": \"string\",\n",
    "            \"one_slide_version\": [\"array of bullet strings (>=5)\"]\n",
    "        },\n",
    "        \"suggested_agenda\": [\n",
    "            {\n",
    "                \"minutes\": \"integer\",\n",
    "                \"topic\": \"string\",\n",
    "                \"what_to_ask\": [\"array of strings (>=3)\"],\n",
    "                \"what_to_listen_for\": [\"array of strings (>=2)\"]\n",
    "            }\n",
    "        ],\n",
    "        \"role_plan\": [\n",
    "            {\"role\": \"string\", \"focus\": \"string\", \"top_questions\": [\"array of strings (>=3)\"]}\n",
    "        ],\n",
    "        \"pre_reads\": [\n",
    "            {\"title\": \"string|null\", \"url\": \"string\", \"why_read\": \"string\"}\n",
    "        ],\n",
    "        \"notes\": {\"assumptions\": [\"array of strings\"], \"data_gaps\": [\"array of strings\"]},\n",
    "        \"confidence\": {\"overall\": \"0..1\", \"rationale\": \"string\"}\n",
    "    }\n",
    "\n",
    "    payload = {\"schema\": schema, \"bundle\": bundle}\n",
    "\n",
    "    # NOTE: We RETURN the model output, NOT the payload.\n",
    "    try:\n",
    "        resp = client.chat.completions.create(\n",
    "            model=\"gpt-4o-mini\",\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": system},\n",
    "                {\"role\": \"user\", \"content\": json.dumps(payload, ensure_ascii=False)},\n",
    "            ],\n",
    "            temperature=0.3,\n",
    "            response_format={\"type\": \"json_object\"},\n",
    "        )\n",
    "        return json.loads((resp.choices[0].message.content or \"\").strip())\n",
    "    except Exception:\n",
    "        resp2 = client.chat.completions.create(\n",
    "            model=\"gpt-4o-mini\",\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": system},\n",
    "                {\"role\": \"user\", \"content\": json.dumps(payload, ensure_ascii=False)},\n",
    "            ],\n",
    "            temperature=0.3,\n",
    "        )\n",
    "        raw = resp2.choices[0].message.content or \"\"\n",
    "        candidate = _extract_json_object(raw)\n",
    "        if candidate:\n",
    "            return json.loads(candidate)\n",
    "        raise RuntimeError(\"Model did not return a JSON object.\")\n",
    "\n",
    "# 1st try\n",
    "framing = openai_meeting_context_framing(bundle, retry_mode=False)\n",
    "\n",
    "# Validate; if invalid, retry once with stronger instruction\n",
    "try:\n",
    "    validate_framing(framing)\n",
    "except Exception as e:\n",
    "    print(\"⚠️ Invalid framing output:\", e)\n",
    "    print(\"Retrying once with stronger constraints...\")\n",
    "    framing = openai_meeting_context_framing(bundle, retry_mode=True)\n",
    "    validate_framing(framing)\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Save outputs (MODEL OUTPUT)\n",
    "# ------------------------------------------------------------\n",
    "JSON_PATH = ART_DIR / f\"meeting_context_framing_{run_id}.json\"\n",
    "JSON_PATH.write_text(json.dumps(framing, ensure_ascii=False, indent=2), encoding=\"utf-8\")\n",
    "print(\"✅ Meeting context framing JSON saved:\", JSON_PATH.as_posix())\n",
    "\n",
    "# Optional MD\n",
    "def to_md(x: dict) -> str:\n",
    "    mf = x.get(\"meeting_frame\", {}) or {}\n",
    "    nar = x.get(\"narrative\", {}) or {}\n",
    "    agenda = x.get(\"suggested_agenda\", []) or []\n",
    "    roles = x.get(\"role_plan\", []) or []\n",
    "    pre = x.get(\"pre_reads\", []) or []\n",
    "    notes = x.get(\"notes\", {}) or {}\n",
    "\n",
    "    lines = []\n",
    "    lines.append(f\"# Meeting Context Framing: {startup_name}\\n\")\n",
    "    lines.append(\"## Objective\")\n",
    "    lines.append(mf.get(\"meeting_objective\", \"UNKNOWN\"))\n",
    "    lines.append(\"\\n## Talk Track (30s)\")\n",
    "    lines.append(nar.get(\"thirty_second\", \"UNKNOWN\"))\n",
    "    lines.append(\"\\n## Talk Track (2m)\")\n",
    "    lines.append(nar.get(\"two_minute\", \"UNKNOWN\"))\n",
    "\n",
    "    lines.append(\"\\n## Agenda\")\n",
    "    for a in agenda:\n",
    "        lines.append(f\"- **{a.get('minutes', 0)} min** — {a.get('topic','')}\")\n",
    "        for q in (a.get(\"what_to_ask\") or [])[:4]:\n",
    "            lines.append(f\"  - Ask: {q}\")\n",
    "        for l in (a.get(\"what_to_listen_for\") or [])[:3]:\n",
    "            lines.append(f\"  - Listen for: {l}\")\n",
    "\n",
    "    lines.append(\"\\n## Role Plan\")\n",
    "    for r in roles:\n",
    "        lines.append(f\"- **{r.get('role','')}**: {r.get('focus','')}\")\n",
    "        for q in (r.get(\"top_questions\") or [])[:3]:\n",
    "            lines.append(f\"  - {q}\")\n",
    "\n",
    "    lines.append(\"\\n## Pre-Reads\")\n",
    "    for p in pre[:12]:\n",
    "        lines.append(f\"- {p.get('title') or 'Pre-read'}\")\n",
    "        lines.append(f\"  - {p.get('url')}\")\n",
    "        lines.append(f\"  - Why: {p.get('why_read')}\")\n",
    "\n",
    "    if notes.get(\"assumptions\"):\n",
    "        lines.append(\"\\n## Assumptions\")\n",
    "        for a in notes.get(\"assumptions\", [])[:10]:\n",
    "            lines.append(f\"- {a}\")\n",
    "\n",
    "    if notes.get(\"data_gaps\"):\n",
    "        lines.append(\"\\n## Data Gaps\")\n",
    "        for g in notes.get(\"data_gaps\", [])[:12]:\n",
    "            lines.append(f\"- {g}\")\n",
    "\n",
    "    return \"\\n\".join(lines)\n",
    "\n",
    "MD_PATH = ART_DIR / f\"meeting_context_framing_{run_id}.md\"\n",
    "MD_PATH.write_text(to_md(framing), encoding=\"utf-8\")\n",
    "print(\"✅ Meeting context framing MD saved:\", MD_PATH.as_posix())\n",
    "\n",
    "# Quick display\n",
    "mf = framing.get(\"meeting_frame\", {}) or {}\n",
    "print(\"\\nobjective:\", mf.get(\"meeting_objective\"))\n",
    "print(\"30-sec:\", (framing.get(\"narrative\", {}) or {}).get(\"thirty_second\"))\n",
    "print(\"agenda items:\", len(framing.get(\"suggested_agenda\", []) or []))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5a07a71c-dc11-464f-a559-55415ffa88ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run_id: 20260107_225628\n",
      "startup_name: Sakana AI\n",
      "integrated_summary_len: 364\n",
      "takeaways_len: 4\n",
      "dd_priorities_len: 2\n",
      "top_pre_read_urls: 15\n",
      "⚠️ Invalid framing output: Framing output missing required keys. Likely you saved payload (schema/bundle) instead of model output, or the model returned an unexpected structure.\n",
      "Retrying once with stronger constraints...\n",
      "✅ Meeting context framing JSON saved: artifacts/meeting_deep_dive/meeting_context_framing_20260107_225628.json\n",
      "✅ Meeting context framing MD saved: artifacts/meeting_deep_dive/meeting_context_framing_20260107_225628.md\n",
      "\n",
      "objective: Evaluate the potential of Sakana AI for investment and partnership opportunities.\n",
      "30-sec: Sakana AI is a Tokyo-based startup revolutionizing AI with nature-inspired technologies, having raised significant funding and developed innovative solutions like the AI Scientist. This meeting aims to evaluate its market potential and investment opportunities.\n",
      "agenda items: 5\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# 12. Meeting Context Framing (Fixed: save the MODEL OUTPUT, not the payload)\n",
    "# ============================================================\n",
    "\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from datetime import datetime, timezone\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Global configuration\n",
    "# ------------------------------------------------------------\n",
    "load_dotenv(\"env.txt\")\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "if not OPENAI_API_KEY:\n",
    "    raise EnvironmentError(\"OPENAI_API_KEY is not set.\")\n",
    "client = OpenAI(api_key=OPENAI_API_KEY)\n",
    "\n",
    "ART_DIR = Path(\"artifacts\") / \"meeting_deep_dive\"\n",
    "ART_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Helpers\n",
    "# ------------------------------------------------------------\n",
    "def latest_path(pattern: str) -> Path | None:\n",
    "    paths = sorted(ART_DIR.glob(pattern), key=lambda p: p.stat().st_mtime, reverse=True)\n",
    "    return paths[0] if paths else None\n",
    "\n",
    "def safe_read_json(path: Path | None) -> dict:\n",
    "    if not path or not path.exists():\n",
    "        return {}\n",
    "    return json.loads(path.read_text(encoding=\"utf-8\"))\n",
    "\n",
    "def safe_read_text(path: Path | None) -> str:\n",
    "    if not path or not path.exists():\n",
    "        return \"\"\n",
    "    return path.read_text(encoding=\"utf-8\")\n",
    "\n",
    "def _extract_json_object(text: str) -> str:\n",
    "    if not text:\n",
    "        return \"\"\n",
    "    t = text.strip()\n",
    "    m = re.search(r\"```(?:json)?\\s*(\\{.*?\\})\\s*```\", t, flags=re.DOTALL | re.IGNORECASE)\n",
    "    if m:\n",
    "        return m.group(1).strip()\n",
    "    start, end = t.find(\"{\"), t.rfind(\"}\")\n",
    "    if start != -1 and end != -1 and end > start:\n",
    "        return t[start:end+1].strip()\n",
    "    return \"\"\n",
    "\n",
    "def validate_framing(obj: dict):\n",
    "    \"\"\"\n",
    "    Fail fast if we accidentally saved payload, or model returned wrong structure.\n",
    "    \"\"\"\n",
    "    if not isinstance(obj, dict):\n",
    "        raise ValueError(\"Framing output is not a dict.\")\n",
    "    if \"meeting_frame\" not in obj or \"narrative\" not in obj or \"suggested_agenda\" not in obj:\n",
    "        raise ValueError(\n",
    "            \"Framing output missing required keys. \"\n",
    "            \"Likely you saved payload (schema/bundle) instead of model output, \"\n",
    "            \"or the model returned an unexpected structure.\"\n",
    "        )\n",
    "    mf = obj.get(\"meeting_frame\") or {}\n",
    "    nar = obj.get(\"narrative\") or {}\n",
    "    if not (mf.get(\"meeting_objective\") or \"\").strip():\n",
    "        raise ValueError(\"meeting_frame.meeting_objective is empty.\")\n",
    "    if not (nar.get(\"thirty_second\") or \"\").strip():\n",
    "        raise ValueError(\"narrative.thirty_second is empty.\")\n",
    "    if len(obj.get(\"suggested_agenda\") or []) < 3:\n",
    "        raise ValueError(\"suggested_agenda has < 3 items.\")\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Load inputs / entity / integrated insights\n",
    "# ------------------------------------------------------------\n",
    "inputs = safe_read_json(latest_path(\"inputs_*.json\"))\n",
    "entity = safe_read_json(latest_path(\"entity_*.json\"))\n",
    "\n",
    "run_id = (inputs.get(\"meta\", {}) or {}).get(\"run_id\") or datetime.now(timezone.utc).strftime(\"%Y%m%d_%H%M%S\")\n",
    "startup_name = (entity.get(\"canonical_name\") or inputs.get(\"startup_name\") or \"Unknown Startup\").strip()\n",
    "\n",
    "meeting_person_name = inputs.get(\"meeting_person_name\")\n",
    "meeting_person_title = inputs.get(\"meeting_person_title\")\n",
    "meeting_context = inputs.get(\"meeting_context\") or \"\"\n",
    "your_org_context = inputs.get(\"your_org_context\") or \"\"\n",
    "\n",
    "integrated = safe_read_json(latest_path(\"integrated_insights_*.json\"))\n",
    "integrated_md = safe_read_text(latest_path(\"integrated_insights_*.md\"))\n",
    "\n",
    "# Evidence map -> top URLs\n",
    "top_urls = []\n",
    "ev_path = latest_path(\"integrated_evidence_map_*.csv\")\n",
    "if ev_path and ev_path.exists():\n",
    "    try:\n",
    "        df_ev = pd.read_csv(ev_path)\n",
    "        if \"url\" in df_ev.columns:\n",
    "            top_urls = df_ev[\"url\"].dropna().astype(str).head(15).tolist()\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "bundle = {\n",
    "    \"startup_name\": startup_name,\n",
    "    \"meeting_person\": {\"name\": meeting_person_name, \"title\": meeting_person_title},\n",
    "    \"meeting_context\": meeting_context,\n",
    "    \"your_org_context\": your_org_context,\n",
    "    \"integrated_insights\": integrated,\n",
    "    \"integrated_insights_md_excerpt\": integrated_md[:6000],\n",
    "    \"top_pre_read_urls\": top_urls\n",
    "}\n",
    "\n",
    "print(\"run_id:\", run_id)\n",
    "print(\"startup_name:\", startup_name)\n",
    "print(\"integrated_summary_len:\", len((integrated.get(\"integrated_summary\") or \"\")))\n",
    "print(\"takeaways_len:\", len(integrated.get(\"key_takeaways\", []) or []))\n",
    "print(\"dd_priorities_len:\", len(integrated.get(\"due_diligence_priorities\", []) or []))\n",
    "print(\"top_pre_read_urls:\", len(top_urls))\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# OpenAI: Meeting framing\n",
    "# ------------------------------------------------------------\n",
    "def openai_meeting_context_framing(bundle: dict, retry_mode: bool = False) -> dict:\n",
    "    system = (\n",
    "        \"You are a venture diligence assistant. Create a meeting-ready context framing using the provided insights.\\n\\n\"\n",
    "        \"NON-NEGOTIABLE RULES:\\n\"\n",
    "        \"- Do NOT return empty strings. If unknown, write 'UNKNOWN' and list assumptions.\\n\"\n",
    "        \"- Always produce: meeting_objective, 30-sec narrative, and at least 5 agenda items.\\n\"\n",
    "        \"- Separate facts vs hypotheses.\\n\"\n",
    "        \"- Do not invent numbers/customers/investors.\\n\"\n",
    "        \"Return a single JSON object only (no markdown).\"\n",
    "    )\n",
    "    if retry_mode:\n",
    "        system += (\n",
    "            \"\\n\\nRETRY MODE: previous output was invalid/empty. \"\n",
    "            \"You MUST fill every field with useful content. \"\n",
    "            \"If context is missing, assume a generic first meeting and label assumptions.\"\n",
    "        )\n",
    "\n",
    "    schema = {\n",
    "        \"meeting_frame\": {\n",
    "            \"meeting_objective\": \"string\",\n",
    "            \"why_now\": \"string\",\n",
    "            \"success_criteria\": [\"array of strings (>=3)\"],\n",
    "            \"key_themes\": [\"array of strings (>=4)\"],\n",
    "            \"facts_to_anchor\": [\"array of strings (>=4)\"],\n",
    "            \"hypotheses_to_validate\": [\"array of strings (>=4)\"],\n",
    "            \"risks_to_probe\": [\"array of strings (>=4)\"]\n",
    "        },\n",
    "        \"narrative\": {\n",
    "            \"thirty_second\": \"string\",\n",
    "            \"two_minute\": \"string\",\n",
    "            \"one_slide_version\": [\"array of bullet strings (>=5)\"]\n",
    "        },\n",
    "        \"suggested_agenda\": [\n",
    "            {\n",
    "                \"minutes\": \"integer\",\n",
    "                \"topic\": \"string\",\n",
    "                \"what_to_ask\": [\"array of strings (>=3)\"],\n",
    "                \"what_to_listen_for\": [\"array of strings (>=2)\"]\n",
    "            }\n",
    "        ],\n",
    "        \"role_plan\": [\n",
    "            {\"role\": \"string\", \"focus\": \"string\", \"top_questions\": [\"array of strings (>=3)\"]}\n",
    "        ],\n",
    "        \"pre_reads\": [\n",
    "            {\"title\": \"string|null\", \"url\": \"string\", \"why_read\": \"string\"}\n",
    "        ],\n",
    "        \"notes\": {\"assumptions\": [\"array of strings\"], \"data_gaps\": [\"array of strings\"]},\n",
    "        \"confidence\": {\"overall\": \"0..1\", \"rationale\": \"string\"}\n",
    "    }\n",
    "\n",
    "    payload = {\"schema\": schema, \"bundle\": bundle}\n",
    "\n",
    "    # NOTE: We RETURN the model output, NOT the payload.\n",
    "    try:\n",
    "        resp = client.chat.completions.create(\n",
    "            model=\"gpt-4o-mini\",\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": system},\n",
    "                {\"role\": \"user\", \"content\": json.dumps(payload, ensure_ascii=False)},\n",
    "            ],\n",
    "            temperature=0.3,\n",
    "            response_format={\"type\": \"json_object\"},\n",
    "        )\n",
    "        return json.loads((resp.choices[0].message.content or \"\").strip())\n",
    "    except Exception:\n",
    "        resp2 = client.chat.completions.create(\n",
    "            model=\"gpt-4o-mini\",\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": system},\n",
    "                {\"role\": \"user\", \"content\": json.dumps(payload, ensure_ascii=False)},\n",
    "            ],\n",
    "            temperature=0.3,\n",
    "        )\n",
    "        raw = resp2.choices[0].message.content or \"\"\n",
    "        candidate = _extract_json_object(raw)\n",
    "        if candidate:\n",
    "            return json.loads(candidate)\n",
    "        raise RuntimeError(\"Model did not return a JSON object.\")\n",
    "\n",
    "# 1st try\n",
    "framing = openai_meeting_context_framing(bundle, retry_mode=False)\n",
    "\n",
    "# Validate; if invalid, retry once with stronger instruction\n",
    "try:\n",
    "    validate_framing(framing)\n",
    "except Exception as e:\n",
    "    print(\"⚠️ Invalid framing output:\", e)\n",
    "    print(\"Retrying once with stronger constraints...\")\n",
    "    framing = openai_meeting_context_framing(bundle, retry_mode=True)\n",
    "    validate_framing(framing)\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Save outputs (MODEL OUTPUT)\n",
    "# ------------------------------------------------------------\n",
    "JSON_PATH = ART_DIR / f\"meeting_context_framing_{run_id}.json\"\n",
    "JSON_PATH.write_text(json.dumps(framing, ensure_ascii=False, indent=2), encoding=\"utf-8\")\n",
    "print(\"✅ Meeting context framing JSON saved:\", JSON_PATH.as_posix())\n",
    "\n",
    "# Optional MD\n",
    "def to_md(x: dict) -> str:\n",
    "    mf = x.get(\"meeting_frame\", {}) or {}\n",
    "    nar = x.get(\"narrative\", {}) or {}\n",
    "    agenda = x.get(\"suggested_agenda\", []) or []\n",
    "    roles = x.get(\"role_plan\", []) or []\n",
    "    pre = x.get(\"pre_reads\", []) or []\n",
    "    notes = x.get(\"notes\", {}) or {}\n",
    "\n",
    "    lines = []\n",
    "    lines.append(f\"# Meeting Context Framing: {startup_name}\\n\")\n",
    "    lines.append(\"## Objective\")\n",
    "    lines.append(mf.get(\"meeting_objective\", \"UNKNOWN\"))\n",
    "    lines.append(\"\\n## Talk Track (30s)\")\n",
    "    lines.append(nar.get(\"thirty_second\", \"UNKNOWN\"))\n",
    "    lines.append(\"\\n## Talk Track (2m)\")\n",
    "    lines.append(nar.get(\"two_minute\", \"UNKNOWN\"))\n",
    "\n",
    "    lines.append(\"\\n## Agenda\")\n",
    "    for a in agenda:\n",
    "        lines.append(f\"- **{a.get('minutes', 0)} min** — {a.get('topic','')}\")\n",
    "        for q in (a.get(\"what_to_ask\") or [])[:4]:\n",
    "            lines.append(f\"  - Ask: {q}\")\n",
    "        for l in (a.get(\"what_to_listen_for\") or [])[:3]:\n",
    "            lines.append(f\"  - Listen for: {l}\")\n",
    "\n",
    "    lines.append(\"\\n## Role Plan\")\n",
    "    for r in roles:\n",
    "        lines.append(f\"- **{r.get('role','')}**: {r.get('focus','')}\")\n",
    "        for q in (r.get(\"top_questions\") or [])[:3]:\n",
    "            lines.append(f\"  - {q}\")\n",
    "\n",
    "    lines.append(\"\\n## Pre-Reads\")\n",
    "    for p in pre[:12]:\n",
    "        lines.append(f\"- {p.get('title') or 'Pre-read'}\")\n",
    "        lines.append(f\"  - {p.get('url')}\")\n",
    "        lines.append(f\"  - Why: {p.get('why_read')}\")\n",
    "\n",
    "    if notes.get(\"assumptions\"):\n",
    "        lines.append(\"\\n## Assumptions\")\n",
    "        for a in notes.get(\"assumptions\", [])[:10]:\n",
    "            lines.append(f\"- {a}\")\n",
    "\n",
    "    if notes.get(\"data_gaps\"):\n",
    "        lines.append(\"\\n## Data Gaps\")\n",
    "        for g in notes.get(\"data_gaps\", [])[:12]:\n",
    "            lines.append(f\"- {g}\")\n",
    "\n",
    "    return \"\\n\".join(lines)\n",
    "\n",
    "MD_PATH = ART_DIR / f\"meeting_context_framing_{run_id}.md\"\n",
    "MD_PATH.write_text(to_md(framing), encoding=\"utf-8\")\n",
    "print(\"✅ Meeting context framing MD saved:\", MD_PATH.as_posix())\n",
    "\n",
    "# Quick display\n",
    "mf = framing.get(\"meeting_frame\", {}) or {}\n",
    "print(\"\\nobjective:\", mf.get(\"meeting_objective\"))\n",
    "print(\"30-sec:\", (framing.get(\"narrative\", {}) or {}).get(\"thirty_second\"))\n",
    "print(\"agenda items:\", len(framing.get(\"suggested_agenda\", []) or []))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bb57b655-2588-4649-a198-a754549a3474",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run_id: 20260107_225628\n",
      "startup_name: Sakana AI\n",
      "evidence_urls: 25\n",
      "✅ Top meeting questions JSON saved: artifacts/meeting_deep_dive/top_meeting_questions_20260107_225628.json\n",
      "✅ Raw model response saved: artifacts/meeting_deep_dive/top_meeting_questions_raw_20260107_225628.txt\n",
      "✅ Top meeting questions MD saved: artifacts/meeting_deep_dive/top_meeting_questions_20260107_225628.md\n",
      "✅ Top meeting questions CSV saved: artifacts/meeting_deep_dive/top_meeting_questions_20260107_225628.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>run_id</th>\n",
       "      <th>rank</th>\n",
       "      <th>theme</th>\n",
       "      <th>question</th>\n",
       "      <th>why_it_matters</th>\n",
       "      <th>what_a_strong_answer_looks_like</th>\n",
       "      <th>strong_answer_signals</th>\n",
       "      <th>red_flags</th>\n",
       "      <th>suggested_followups</th>\n",
       "      <th>evidence_urls</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20260107_225628</td>\n",
       "      <td>1</td>\n",
       "      <td>product</td>\n",
       "      <td>Can you provide specific metrics on the perfor...</td>\n",
       "      <td>Understanding the effectiveness and real-world...</td>\n",
       "      <td>Detailed metrics showing user adoption rates, ...</td>\n",
       "      <td>High user adoption rates | Positive customer f...</td>\n",
       "      <td>Lack of data on performance | Negative custome...</td>\n",
       "      <td>What specific sectors have seen the most succe...</td>\n",
       "      <td>https://sakana.ai/ai-scientist/</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20260107_225628</td>\n",
       "      <td>2</td>\n",
       "      <td>market</td>\n",
       "      <td>How do you differentiate Sakana AI from establ...</td>\n",
       "      <td>Identifying Sakana AI's unique value propositi...</td>\n",
       "      <td>Clear articulation of unique features, advanta...</td>\n",
       "      <td>Unique technology features | Clear differentia...</td>\n",
       "      <td>Vague differentiation | Inability to articulat...</td>\n",
       "      <td>What specific features do customers prefer? | ...</td>\n",
       "      <td>https://promptloop.com/directory/what-does-sak...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20260107_225628</td>\n",
       "      <td>3</td>\n",
       "      <td>team</td>\n",
       "      <td>What is the background and expertise of the te...</td>\n",
       "      <td>The team's expertise and experience are critic...</td>\n",
       "      <td>A diverse team with relevant experience in AI,...</td>\n",
       "      <td>Experienced leadership | Diverse skill sets | ...</td>\n",
       "      <td>Lack of relevant experience | High turnover in...</td>\n",
       "      <td>What are the team's previous successes? | How ...</td>\n",
       "      <td>https://www.japantimes.co.jp/business/2025/09/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>20260107_225628</td>\n",
       "      <td>4</td>\n",
       "      <td>funding</td>\n",
       "      <td>How do you plan to utilize the recent $135 mil...</td>\n",
       "      <td>Understanding the allocation of funds will pro...</td>\n",
       "      <td>A clear plan detailing how funds will be used ...</td>\n",
       "      <td>Specific allocation of funds | Clear growth st...</td>\n",
       "      <td>Vague plans for fund utilization | Lack of str...</td>\n",
       "      <td>What are the key milestones you aim to achieve...</td>\n",
       "      <td>https://siliconangle.com/2025/11/17/sakana-ai-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20260107_225628</td>\n",
       "      <td>5</td>\n",
       "      <td>risks</td>\n",
       "      <td>What are the primary risks you foresee in scal...</td>\n",
       "      <td>Identifying potential risks will help assess t...</td>\n",
       "      <td>A comprehensive understanding of market risks,...</td>\n",
       "      <td>Clear risk identification | Proactive risk man...</td>\n",
       "      <td>Lack of risk awareness | No contingency plans ...</td>\n",
       "      <td>How do you plan to address these risks? | What...</td>\n",
       "      <td>https://www.retailbankerinternational.com/news...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            run_id  rank    theme  \\\n",
       "0  20260107_225628     1  product   \n",
       "1  20260107_225628     2   market   \n",
       "2  20260107_225628     3     team   \n",
       "3  20260107_225628     4  funding   \n",
       "4  20260107_225628     5    risks   \n",
       "\n",
       "                                            question  \\\n",
       "0  Can you provide specific metrics on the perfor...   \n",
       "1  How do you differentiate Sakana AI from establ...   \n",
       "2  What is the background and expertise of the te...   \n",
       "3  How do you plan to utilize the recent $135 mil...   \n",
       "4  What are the primary risks you foresee in scal...   \n",
       "\n",
       "                                      why_it_matters  \\\n",
       "0  Understanding the effectiveness and real-world...   \n",
       "1  Identifying Sakana AI's unique value propositi...   \n",
       "2  The team's expertise and experience are critic...   \n",
       "3  Understanding the allocation of funds will pro...   \n",
       "4  Identifying potential risks will help assess t...   \n",
       "\n",
       "                     what_a_strong_answer_looks_like  \\\n",
       "0  Detailed metrics showing user adoption rates, ...   \n",
       "1  Clear articulation of unique features, advanta...   \n",
       "2  A diverse team with relevant experience in AI,...   \n",
       "3  A clear plan detailing how funds will be used ...   \n",
       "4  A comprehensive understanding of market risks,...   \n",
       "\n",
       "                               strong_answer_signals  \\\n",
       "0  High user adoption rates | Positive customer f...   \n",
       "1  Unique technology features | Clear differentia...   \n",
       "2  Experienced leadership | Diverse skill sets | ...   \n",
       "3  Specific allocation of funds | Clear growth st...   \n",
       "4  Clear risk identification | Proactive risk man...   \n",
       "\n",
       "                                           red_flags  \\\n",
       "0  Lack of data on performance | Negative custome...   \n",
       "1  Vague differentiation | Inability to articulat...   \n",
       "2  Lack of relevant experience | High turnover in...   \n",
       "3  Vague plans for fund utilization | Lack of str...   \n",
       "4  Lack of risk awareness | No contingency plans ...   \n",
       "\n",
       "                                 suggested_followups  \\\n",
       "0  What specific sectors have seen the most succe...   \n",
       "1  What specific features do customers prefer? | ...   \n",
       "2  What are the team's previous successes? | How ...   \n",
       "3  What are the key milestones you aim to achieve...   \n",
       "4  How do you plan to address these risks? | What...   \n",
       "\n",
       "                                       evidence_urls  \n",
       "0                    https://sakana.ai/ai-scientist/  \n",
       "1  https://promptloop.com/directory/what-does-sak...  \n",
       "2  https://www.japantimes.co.jp/business/2025/09/...  \n",
       "3  https://siliconangle.com/2025/11/17/sakana-ai-...  \n",
       "4  https://www.retailbankerinternational.com/news...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ============================================================\n",
    "# 13. Top 5 Meeting Questions (Robust + Evidence-linked)\n",
    "# ============================================================\n",
    "# Fixes:\n",
    "# - Do NOT send {\"schema\":..., \"bundle\":...} as user JSON (model may echo it back)\n",
    "# - Send only the bundle, with a strict system schema description\n",
    "# - Save raw model response for debugging\n",
    "# - Retry cascade: gpt-4o-mini -> (stronger prompt) -> gpt-4o\n",
    "# - Key salvage: accept alternative keys like \"questions\" and normalize to \"top_questions\"\n",
    "#\n",
    "# Outputs:\n",
    "# - top_meeting_questions_<run_id>.json\n",
    "# - top_meeting_questions_<run_id>.md\n",
    "# - top_meeting_questions_<run_id>.csv\n",
    "# - top_meeting_questions_raw_<run_id>.txt (debug)\n",
    "\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from datetime import datetime, timezone\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Global configuration\n",
    "# ------------------------------------------------------------\n",
    "load_dotenv(\"env.txt\")\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "if not OPENAI_API_KEY:\n",
    "    raise EnvironmentError(\"OPENAI_API_KEY is not set.\")\n",
    "client = OpenAI(api_key=OPENAI_API_KEY)\n",
    "\n",
    "ART_DIR = Path(\"artifacts\") / \"meeting_deep_dive\"\n",
    "ART_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Helpers\n",
    "# ------------------------------------------------------------\n",
    "def latest_path(pattern: str) -> Path | None:\n",
    "    paths = sorted(ART_DIR.glob(pattern), key=lambda p: p.stat().st_mtime, reverse=True)\n",
    "    return paths[0] if paths else None\n",
    "\n",
    "def safe_read_json(path: Path | None) -> dict:\n",
    "    if not path or not path.exists():\n",
    "        return {}\n",
    "    return json.loads(path.read_text(encoding=\"utf-8\"))\n",
    "\n",
    "def safe_read_text(path: Path | None) -> str:\n",
    "    if not path or not path.exists():\n",
    "        return \"\"\n",
    "    return path.read_text(encoding=\"utf-8\")\n",
    "\n",
    "def _extract_json_object(text: str) -> str:\n",
    "    if not text:\n",
    "        return \"\"\n",
    "    t = text.strip()\n",
    "    m = re.search(r\"```(?:json)?\\s*(\\{.*?\\})\\s*```\", t, flags=re.DOTALL | re.IGNORECASE)\n",
    "    if m:\n",
    "        return m.group(1).strip()\n",
    "    start, end = t.find(\"{\"), t.rfind(\"}\")\n",
    "    if start != -1 and end != -1 and end > start:\n",
    "        return t[start:end+1].strip()\n",
    "    return \"\"\n",
    "\n",
    "def normalize_questions_obj(obj: dict) -> dict:\n",
    "    \"\"\"\n",
    "    Normalize common variants into {\"top_questions\": [...], \"notes\": {...}, \"confidence\": {...}}\n",
    "    \"\"\"\n",
    "    if not isinstance(obj, dict):\n",
    "        return {}\n",
    "\n",
    "    # If model echoed input\n",
    "    if set(obj.keys()) == {\"schema\", \"bundle\"}:\n",
    "        return {}  # treat as invalid\n",
    "\n",
    "    if \"top_questions\" in obj and isinstance(obj[\"top_questions\"], list):\n",
    "        return obj\n",
    "\n",
    "    # Salvage alternative keys\n",
    "    for k in [\"questions\", \"meeting_questions\", \"top5_questions\", \"top_5_questions\"]:\n",
    "        if k in obj and isinstance(obj[k], list):\n",
    "            obj[\"top_questions\"] = obj[k]\n",
    "            break\n",
    "\n",
    "    # Sometimes nested\n",
    "    if \"result\" in obj and isinstance(obj[\"result\"], dict):\n",
    "        inner = obj[\"result\"]\n",
    "        if \"top_questions\" in inner and isinstance(inner[\"top_questions\"], list):\n",
    "            return inner\n",
    "        for k in [\"questions\", \"meeting_questions\"]:\n",
    "            if k in inner and isinstance(inner[k], list):\n",
    "                inner[\"top_questions\"] = inner[k]\n",
    "                return inner\n",
    "\n",
    "    return obj if \"top_questions\" in obj else {}\n",
    "\n",
    "def validate_questions(obj: dict):\n",
    "    if \"top_questions\" not in obj:\n",
    "        raise ValueError(\"Missing top_questions in output.\")\n",
    "    qs = obj.get(\"top_questions\") or []\n",
    "    if len(qs) != 5:\n",
    "        raise ValueError(f\"Expected exactly 5 questions, got {len(qs)}\")\n",
    "    for i, q in enumerate(qs, start=1):\n",
    "        if not (q.get(\"question\") or \"\").strip():\n",
    "            raise ValueError(f\"Question {i} missing question text.\")\n",
    "        if not (q.get(\"why_it_matters\") or \"\").strip():\n",
    "            raise ValueError(f\"Question {i} missing why_it_matters.\")\n",
    "        if not (q.get(\"strong_answer_signals\") or []):\n",
    "            raise ValueError(f\"Question {i} missing strong_answer_signals.\")\n",
    "        if not (q.get(\"red_flags\") or []):\n",
    "            raise ValueError(f\"Question {i} missing red_flags.\")\n",
    "        ev = q.get(\"evidence_urls\") or []\n",
    "        if not isinstance(ev, list) or len(ev) < 1:\n",
    "            raise ValueError(f\"Question {i} missing evidence_urls (need >=1).\")\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Load inputs / integrated / framing / evidence\n",
    "# ------------------------------------------------------------\n",
    "inputs = safe_read_json(latest_path(\"inputs_*.json\"))\n",
    "entity = safe_read_json(latest_path(\"entity_*.json\"))\n",
    "integrated = safe_read_json(latest_path(\"integrated_insights_*.json\"))\n",
    "framing = safe_read_json(latest_path(\"meeting_context_framing_*.json\"))\n",
    "\n",
    "integrated_md = safe_read_text(latest_path(\"integrated_insights_*.md\"))\n",
    "framing_md = safe_read_text(latest_path(\"meeting_context_framing_*.md\"))\n",
    "\n",
    "run_id = (inputs.get(\"meta\", {}) or {}).get(\"run_id\") or datetime.now(timezone.utc).strftime(\"%Y%m%d_%H%M%S\")\n",
    "startup_name = (entity.get(\"canonical_name\") or inputs.get(\"startup_name\") or \"Unknown Startup\").strip()\n",
    "\n",
    "# Evidence URLs\n",
    "top_urls = []\n",
    "ev_path = latest_path(\"integrated_evidence_map_*.csv\")\n",
    "if ev_path and ev_path.exists():\n",
    "    try:\n",
    "        df_ev = pd.read_csv(ev_path)\n",
    "        if \"url\" in df_ev.columns:\n",
    "            top_urls = df_ev[\"url\"].dropna().astype(str).head(25).tolist()\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "bundle = {\n",
    "    \"startup_name\": startup_name,\n",
    "    \"meeting_person\": {\"name\": inputs.get(\"meeting_person_name\"), \"title\": inputs.get(\"meeting_person_title\")},\n",
    "    \"meeting_context\": inputs.get(\"meeting_context\"),\n",
    "    \"your_org_context\": inputs.get(\"your_org_context\"),\n",
    "    \"integrated_insights_json\": integrated,\n",
    "    \"meeting_context_framing_json\": framing,\n",
    "    \"integrated_md_excerpt\": integrated_md[:5000],\n",
    "    \"framing_md_excerpt\": framing_md[:3500],\n",
    "    \"allowed_evidence_urls\": top_urls[:25],   # IMPORTANT: model must only use these URLs\n",
    "}\n",
    "\n",
    "print(\"run_id:\", run_id)\n",
    "print(\"startup_name:\", startup_name)\n",
    "print(\"evidence_urls:\", len(top_urls))\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# OpenAI: Top 5 questions\n",
    "# ------------------------------------------------------------\n",
    "SYSTEM_BASE = \"\"\"\n",
    "You are a venture diligence assistant.\n",
    "\n",
    "TASK:\n",
    "Generate the Top 5 meeting questions for an initial meeting, using the provided bundle (integrated insights + meeting framing).\n",
    "\n",
    "OUTPUT FORMAT (STRICT JSON):\n",
    "Return a single JSON object with EXACTLY these top-level keys:\n",
    "{\n",
    "  \"top_questions\": [ ... exactly 5 items ... ],\n",
    "  \"notes\": { \"assumptions\": [...], \"open_points\": [...] },\n",
    "  \"confidence\": { \"overall\": 0..1, \"rationale\": \"...\" }\n",
    "}\n",
    "\n",
    "Each item in \"top_questions\" MUST have:\n",
    "{\n",
    "  \"rank\": 1..5,\n",
    "  \"theme\": \"product|market|gtm|moat|team|funding|other\",\n",
    "  \"question\": \"...\",\n",
    "  \"why_it_matters\": \"...\",\n",
    "  \"what_a_strong_answer_looks_like\": \"...\",\n",
    "  \"strong_answer_signals\": [\"...\", \"...\", \"...\", \"...\"],\n",
    "  \"red_flags\": [\"...\", \"...\", \"...\", \"...\"],\n",
    "  \"suggested_followups\": [\"...\", \"...\", \"...\"],\n",
    "  \"evidence_urls\": [\"<URL from allowed_evidence_urls>\", \"...\"]\n",
    "}\n",
    "\n",
    "NON-NEGOTIABLE RULES:\n",
    "- Produce exactly 5 questions ranked 1..5.\n",
    "- DO NOT invent facts or URLs.\n",
    "- evidence_urls must be chosen ONLY from bundle.allowed_evidence_urls.\n",
    "- Do not include any keys other than: top_questions, notes, confidence.\n",
    "- Avoid vague questions; each should be askable in the room.\n",
    "\"\"\".strip()\n",
    "\n",
    "SYSTEM_STRONG = (SYSTEM_BASE + \"\"\"\n",
    "\n",
    "RETRY MODE:\n",
    "- Your previous output was invalid.\n",
    "- You MUST comply with the exact JSON schema above.\n",
    "- If information is missing, write assumptions in notes.assumptions instead of leaving blanks.\n",
    "\"\"\").strip()\n",
    "\n",
    "def call_model(bundle: dict, model: str, strong: bool = False) -> tuple[dict, str]:\n",
    "    system = SYSTEM_STRONG if strong else SYSTEM_BASE\n",
    "    user = json.dumps(bundle, ensure_ascii=False)\n",
    "\n",
    "    try:\n",
    "        resp = client.chat.completions.create(\n",
    "            model=model,\n",
    "            messages=[{\"role\": \"system\", \"content\": system},\n",
    "                      {\"role\": \"user\", \"content\": user}],\n",
    "            temperature=0.15,\n",
    "            response_format={\"type\": \"json_object\"},\n",
    "        )\n",
    "        raw = (resp.choices[0].message.content or \"\").strip()\n",
    "        out = json.loads(raw) if raw else {}\n",
    "        return out, raw\n",
    "    except Exception:\n",
    "        # fallback: best-effort extraction\n",
    "        resp2 = client.chat.completions.create(\n",
    "            model=model,\n",
    "            messages=[{\"role\": \"system\", \"content\": system},\n",
    "                      {\"role\": \"user\", \"content\": user}],\n",
    "            temperature=0.15,\n",
    "        )\n",
    "        raw = (resp2.choices[0].message.content or \"\").strip()\n",
    "        candidate = _extract_json_object(raw)\n",
    "        out = json.loads(candidate) if candidate else {}\n",
    "        return out, raw\n",
    "\n",
    "RAW_TXT = ART_DIR / f\"top_meeting_questions_raw_{run_id}.txt\"\n",
    "\n",
    "# Try 1: mini\n",
    "out, raw = call_model(bundle, model=\"gpt-4o-mini\", strong=False)\n",
    "RAW_TXT.write_text(raw, encoding=\"utf-8\")\n",
    "out = normalize_questions_obj(out)\n",
    "\n",
    "# Try 2: mini (strong)\n",
    "if not out:\n",
    "    print(\"⚠️ Invalid output (no top_questions) -> retry with stronger prompt...\")\n",
    "    out2, raw2 = call_model(bundle, model=\"gpt-4o-mini\", strong=True)\n",
    "    RAW_TXT.write_text(raw2, encoding=\"utf-8\")\n",
    "    out = normalize_questions_obj(out2)\n",
    "\n",
    "# Try 3: gpt-4o (strong) if still invalid\n",
    "if not out:\n",
    "    print(\"⚠️ Still invalid -> retry with gpt-4o (strong)...\")\n",
    "    out3, raw3 = call_model(bundle, model=\"gpt-4o\", strong=True)\n",
    "    RAW_TXT.write_text(raw3, encoding=\"utf-8\")\n",
    "    out = normalize_questions_obj(out3)\n",
    "\n",
    "# Final validation\n",
    "validate_questions(out)\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Save outputs\n",
    "# ------------------------------------------------------------\n",
    "JSON_PATH = ART_DIR / f\"top_meeting_questions_{run_id}.json\"\n",
    "JSON_PATH.write_text(json.dumps(out, ensure_ascii=False, indent=2), encoding=\"utf-8\")\n",
    "print(\"✅ Top meeting questions JSON saved:\", JSON_PATH.as_posix())\n",
    "print(\"✅ Raw model response saved:\", RAW_TXT.as_posix())\n",
    "\n",
    "# Markdown\n",
    "def to_md(x: dict) -> str:\n",
    "    qs = x.get(\"top_questions\") or []\n",
    "    notes = x.get(\"notes\", {}) or {}\n",
    "    lines = []\n",
    "    lines.append(f\"# Top 5 Meeting Questions: {startup_name}\\n\")\n",
    "    for q in sorted(qs, key=lambda z: int(z.get(\"rank\", 999))):\n",
    "        lines.append(f\"## {q.get('rank')}. {q.get('theme')}\")\n",
    "        lines.append(f\"**Q:** {q.get('question')}\\n\")\n",
    "        lines.append(f\"**Why it matters:** {q.get('why_it_matters')}\\n\")\n",
    "        lines.append(f\"**What a strong answer looks like:** {q.get('what_a_strong_answer_looks_like')}\\n\")\n",
    "        lines.append(\"**Strong answer signals:**\")\n",
    "        for s in (q.get(\"strong_answer_signals\") or [])[:8]:\n",
    "            lines.append(f\"- {s}\")\n",
    "        lines.append(\"\\n**Red flags:**\")\n",
    "        for s in (q.get(\"red_flags\") or [])[:8]:\n",
    "            lines.append(f\"- {s}\")\n",
    "        lines.append(\"\\n**Suggested follow-ups:**\")\n",
    "        for s in (q.get(\"suggested_followups\") or [])[:6]:\n",
    "            lines.append(f\"- {s}\")\n",
    "        lines.append(\"\\n**Evidence URLs:**\")\n",
    "        for u in (q.get(\"evidence_urls\") or [])[:5]:\n",
    "            lines.append(f\"- {u}\")\n",
    "        lines.append(\"\\n---\\n\")\n",
    "\n",
    "    if notes.get(\"assumptions\"):\n",
    "        lines.append(\"## Assumptions\")\n",
    "        for a in notes.get(\"assumptions\", [])[:12]:\n",
    "            lines.append(f\"- {a}\")\n",
    "        lines.append(\"\")\n",
    "    if notes.get(\"open_points\"):\n",
    "        lines.append(\"## Open Points\")\n",
    "        for a in notes.get(\"open_points\", [])[:12]:\n",
    "            lines.append(f\"- {a}\")\n",
    "        lines.append(\"\")\n",
    "    return \"\\n\".join(lines)\n",
    "\n",
    "MD_PATH = ART_DIR / f\"top_meeting_questions_{run_id}.md\"\n",
    "MD_PATH.write_text(to_md(out), encoding=\"utf-8\")\n",
    "print(\"✅ Top meeting questions MD saved:\", MD_PATH.as_posix())\n",
    "\n",
    "# CSV\n",
    "rows = []\n",
    "for q in out.get(\"top_questions\", []) or []:\n",
    "    rows.append({\n",
    "        \"run_id\": run_id,\n",
    "        \"rank\": q.get(\"rank\"),\n",
    "        \"theme\": q.get(\"theme\"),\n",
    "        \"question\": q.get(\"question\"),\n",
    "        \"why_it_matters\": q.get(\"why_it_matters\"),\n",
    "        \"what_a_strong_answer_looks_like\": q.get(\"what_a_strong_answer_looks_like\"),\n",
    "        \"strong_answer_signals\": \" | \".join(q.get(\"strong_answer_signals\") or []),\n",
    "        \"red_flags\": \" | \".join(q.get(\"red_flags\") or []),\n",
    "        \"suggested_followups\": \" | \".join(q.get(\"suggested_followups\") or []),\n",
    "        \"evidence_urls\": \" | \".join(q.get(\"evidence_urls\") or []),\n",
    "    })\n",
    "\n",
    "df_q = pd.DataFrame(rows).sort_values(\"rank\")\n",
    "CSV_PATH = ART_DIR / f\"top_meeting_questions_{run_id}.csv\"\n",
    "df_q.to_csv(CSV_PATH, index=False)\n",
    "print(\"✅ Top meeting questions CSV saved:\", CSV_PATH.as_posix())\n",
    "\n",
    "display(df_q)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9816ffcd-d980-40b4-a0bc-f5f9d32d7889",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run_id: 20260107_225628\n",
      "startup_name: Sakana AI\n",
      "✅ NG questions & watchouts JSON saved: artifacts/meeting_deep_dive/ng_questions_watchouts_20260107_225628.json\n",
      "✅ Raw model response saved: artifacts/meeting_deep_dive/ng_questions_watchouts_raw_20260107_225628.txt\n",
      "✅ NG questions & watchouts MD saved: artifacts/meeting_deep_dive/ng_questions_watchouts_20260107_225628.md\n",
      "✅ CSV saved: artifacts/meeting_deep_dive/ng_questions_20260107_225628.csv and artifacts/meeting_deep_dive/watchouts_20260107_225628.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>run_id</th>\n",
       "      <th>category</th>\n",
       "      <th>ng_question</th>\n",
       "      <th>why_bad</th>\n",
       "      <th>better_alternative</th>\n",
       "      <th>when_to_use_instead</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20260107_225628</td>\n",
       "      <td>bias/leading</td>\n",
       "      <td>Isn't Sakana AI the best AI company in Japan?</td>\n",
       "      <td>This question is leading and biases the respon...</td>\n",
       "      <td>How do you perceive Sakana AI's position in th...</td>\n",
       "      <td>Use when seeking an unbiased perspective on ma...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20260107_225628</td>\n",
       "      <td>too_broad</td>\n",
       "      <td>Can you tell us everything about your technology?</td>\n",
       "      <td>This question is too broad and may overwhelm t...</td>\n",
       "      <td>Can you explain the key features of the AI Sci...</td>\n",
       "      <td>Use when you want focused information on speci...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20260107_225628</td>\n",
       "      <td>too_sensitive</td>\n",
       "      <td>What are your company's weaknesses?</td>\n",
       "      <td>This question may put the respondent on the de...</td>\n",
       "      <td>What challenges have you faced in your growth ...</td>\n",
       "      <td>Use when you want to understand challenges wit...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>20260107_225628</td>\n",
       "      <td>confidentiality</td>\n",
       "      <td>Can you disclose your client list?</td>\n",
       "      <td>This question may breach confidentiality and t...</td>\n",
       "      <td>Can you share examples of industries you are w...</td>\n",
       "      <td>Use when seeking to understand market applicat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20260107_225628</td>\n",
       "      <td>premature_pricing</td>\n",
       "      <td>What is the price of your product?</td>\n",
       "      <td>This question may be premature if the value pr...</td>\n",
       "      <td>How do you determine the value of your product...</td>\n",
       "      <td>Use when you want to understand pricing strate...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>20260107_225628</td>\n",
       "      <td>other</td>\n",
       "      <td>Why should we invest in you?</td>\n",
       "      <td>This question may put pressure on the responde...</td>\n",
       "      <td>What unique value does Sakana AI bring to pote...</td>\n",
       "      <td>Use when you want to elicit a thoughtful respo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>20260107_225628</td>\n",
       "      <td>too_broad</td>\n",
       "      <td>What do you think about AI?</td>\n",
       "      <td>This question is too broad and lacks focus.</td>\n",
       "      <td>What trends do you see in the AI industry that...</td>\n",
       "      <td>Use when seeking insights on industry trends r...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>20260107_225628</td>\n",
       "      <td>bias/leading</td>\n",
       "      <td>Isn't your technology superior to competitors?</td>\n",
       "      <td>This question is leading and may not elicit ho...</td>\n",
       "      <td>How do you compare your technology to that of ...</td>\n",
       "      <td>Use when seeking a balanced comparison.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>20260107_225628</td>\n",
       "      <td>too_sensitive</td>\n",
       "      <td>Are you worried about your competition?</td>\n",
       "      <td>This question may make the respondent uncomfor...</td>\n",
       "      <td>What strategies do you have in place to addres...</td>\n",
       "      <td>Use when you want to discuss competition in a ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>20260107_225628</td>\n",
       "      <td>premature_pricing</td>\n",
       "      <td>What is your projected revenue for next year?</td>\n",
       "      <td>This question may be too early in the discussi...</td>\n",
       "      <td>What are your growth expectations for the upco...</td>\n",
       "      <td>Use when discussing future plans without focus...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            run_id           category  \\\n",
       "0  20260107_225628       bias/leading   \n",
       "1  20260107_225628          too_broad   \n",
       "2  20260107_225628      too_sensitive   \n",
       "3  20260107_225628    confidentiality   \n",
       "4  20260107_225628  premature_pricing   \n",
       "5  20260107_225628              other   \n",
       "6  20260107_225628          too_broad   \n",
       "7  20260107_225628       bias/leading   \n",
       "8  20260107_225628      too_sensitive   \n",
       "9  20260107_225628  premature_pricing   \n",
       "\n",
       "                                         ng_question  \\\n",
       "0      Isn't Sakana AI the best AI company in Japan?   \n",
       "1  Can you tell us everything about your technology?   \n",
       "2                What are your company's weaknesses?   \n",
       "3                 Can you disclose your client list?   \n",
       "4                 What is the price of your product?   \n",
       "5                       Why should we invest in you?   \n",
       "6                        What do you think about AI?   \n",
       "7     Isn't your technology superior to competitors?   \n",
       "8            Are you worried about your competition?   \n",
       "9      What is your projected revenue for next year?   \n",
       "\n",
       "                                             why_bad  \\\n",
       "0  This question is leading and biases the respon...   \n",
       "1  This question is too broad and may overwhelm t...   \n",
       "2  This question may put the respondent on the de...   \n",
       "3  This question may breach confidentiality and t...   \n",
       "4  This question may be premature if the value pr...   \n",
       "5  This question may put pressure on the responde...   \n",
       "6        This question is too broad and lacks focus.   \n",
       "7  This question is leading and may not elicit ho...   \n",
       "8  This question may make the respondent uncomfor...   \n",
       "9  This question may be too early in the discussi...   \n",
       "\n",
       "                                  better_alternative  \\\n",
       "0  How do you perceive Sakana AI's position in th...   \n",
       "1  Can you explain the key features of the AI Sci...   \n",
       "2  What challenges have you faced in your growth ...   \n",
       "3  Can you share examples of industries you are w...   \n",
       "4  How do you determine the value of your product...   \n",
       "5  What unique value does Sakana AI bring to pote...   \n",
       "6  What trends do you see in the AI industry that...   \n",
       "7  How do you compare your technology to that of ...   \n",
       "8  What strategies do you have in place to addres...   \n",
       "9  What are your growth expectations for the upco...   \n",
       "\n",
       "                                 when_to_use_instead  \n",
       "0  Use when seeking an unbiased perspective on ma...  \n",
       "1  Use when you want focused information on speci...  \n",
       "2  Use when you want to understand challenges wit...  \n",
       "3  Use when seeking to understand market applicat...  \n",
       "4  Use when you want to understand pricing strate...  \n",
       "5  Use when you want to elicit a thoughtful respo...  \n",
       "6  Use when seeking insights on industry trends r...  \n",
       "7            Use when seeking a balanced comparison.  \n",
       "8  Use when you want to discuss competition in a ...  \n",
       "9  Use when discussing future plans without focus...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>run_id</th>\n",
       "      <th>signal</th>\n",
       "      <th>why_it_matters</th>\n",
       "      <th>what_it_might_indicate</th>\n",
       "      <th>follow_up_question</th>\n",
       "      <th>severity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20260107_225628</td>\n",
       "      <td>Vague responses about technology capabilities.</td>\n",
       "      <td>Indicates a lack of clarity or confidence in t...</td>\n",
       "      <td>Potential weaknesses in the technology or its ...</td>\n",
       "      <td>Can you provide more specific examples of how ...</td>\n",
       "      <td>high</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20260107_225628</td>\n",
       "      <td>Overemphasis on funding without clear growth s...</td>\n",
       "      <td>May suggest reliance on funding rather than su...</td>\n",
       "      <td>Lack of a solid plan for utilizing funds effec...</td>\n",
       "      <td>How do you plan to allocate the recent funding...</td>\n",
       "      <td>medium</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20260107_225628</td>\n",
       "      <td>Mention of challenges in customer acquisition.</td>\n",
       "      <td>Highlights potential barriers to growth and ma...</td>\n",
       "      <td>Possible issues with product-market fit or com...</td>\n",
       "      <td>What specific challenges have you encountered ...</td>\n",
       "      <td>high</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>20260107_225628</td>\n",
       "      <td>Lack of competitive differentiation.</td>\n",
       "      <td>Indicates potential vulnerability in a crowded...</td>\n",
       "      <td>Unclear value proposition compared to competit...</td>\n",
       "      <td>What specific features set your product apart ...</td>\n",
       "      <td>high</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20260107_225628</td>\n",
       "      <td>Heavy reliance on the Japanese market.</td>\n",
       "      <td>Limits growth potential and exposes the compan...</td>\n",
       "      <td>Lack of international expansion strategy.</td>\n",
       "      <td>What are your plans for expanding beyond the J...</td>\n",
       "      <td>medium</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>20260107_225628</td>\n",
       "      <td>Unclear metrics on customer satisfaction.</td>\n",
       "      <td>Lack of data can hinder understanding of produ...</td>\n",
       "      <td>Potential issues with product adoption or perf...</td>\n",
       "      <td>How do you measure customer satisfaction with ...</td>\n",
       "      <td>high</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>20260107_225628</td>\n",
       "      <td>Defensive tone when discussing competition.</td>\n",
       "      <td>May indicate insecurity about their market pos...</td>\n",
       "      <td>Concerns about competitive threats and market ...</td>\n",
       "      <td>How do you plan to address competitive threats...</td>\n",
       "      <td>medium</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>20260107_225628</td>\n",
       "      <td>Lack of clarity on regulatory challenges.</td>\n",
       "      <td>Regulatory hurdles can significantly impact bu...</td>\n",
       "      <td>Unpreparedness for compliance and operational ...</td>\n",
       "      <td>What regulatory challenges do you anticipate i...</td>\n",
       "      <td>high</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>20260107_225628</td>\n",
       "      <td>Overly optimistic projections without backing ...</td>\n",
       "      <td>Can indicate a disconnect between expectations...</td>\n",
       "      <td>Potential issues with planning and forecasting.</td>\n",
       "      <td>What data supports your growth projections?</td>\n",
       "      <td>medium</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>20260107_225628</td>\n",
       "      <td>Inconsistent information about funding history.</td>\n",
       "      <td>Inconsistencies can raise red flags about tran...</td>\n",
       "      <td>Possible mismanagement or misunderstanding of ...</td>\n",
       "      <td>Can you clarify the details of your funding ro...</td>\n",
       "      <td>high</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>20260107_225628</td>\n",
       "      <td>Reluctance to discuss future plans.</td>\n",
       "      <td>May indicate uncertainty or lack of a clear vi...</td>\n",
       "      <td>Potential issues with strategic planning.</td>\n",
       "      <td>What are your key milestones for the next year?</td>\n",
       "      <td>medium</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>20260107_225628</td>\n",
       "      <td>Generalized statements about market trends.</td>\n",
       "      <td>Lack of specificity can indicate a superficial...</td>\n",
       "      <td>Weakness in market analysis and strategy.</td>\n",
       "      <td>Can you provide specific trends that impact yo...</td>\n",
       "      <td>medium</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             run_id                                             signal  \\\n",
       "0   20260107_225628     Vague responses about technology capabilities.   \n",
       "1   20260107_225628  Overemphasis on funding without clear growth s...   \n",
       "2   20260107_225628     Mention of challenges in customer acquisition.   \n",
       "3   20260107_225628               Lack of competitive differentiation.   \n",
       "4   20260107_225628             Heavy reliance on the Japanese market.   \n",
       "5   20260107_225628          Unclear metrics on customer satisfaction.   \n",
       "6   20260107_225628        Defensive tone when discussing competition.   \n",
       "7   20260107_225628          Lack of clarity on regulatory challenges.   \n",
       "8   20260107_225628  Overly optimistic projections without backing ...   \n",
       "9   20260107_225628    Inconsistent information about funding history.   \n",
       "10  20260107_225628                Reluctance to discuss future plans.   \n",
       "11  20260107_225628        Generalized statements about market trends.   \n",
       "\n",
       "                                       why_it_matters  \\\n",
       "0   Indicates a lack of clarity or confidence in t...   \n",
       "1   May suggest reliance on funding rather than su...   \n",
       "2   Highlights potential barriers to growth and ma...   \n",
       "3   Indicates potential vulnerability in a crowded...   \n",
       "4   Limits growth potential and exposes the compan...   \n",
       "5   Lack of data can hinder understanding of produ...   \n",
       "6   May indicate insecurity about their market pos...   \n",
       "7   Regulatory hurdles can significantly impact bu...   \n",
       "8   Can indicate a disconnect between expectations...   \n",
       "9   Inconsistencies can raise red flags about tran...   \n",
       "10  May indicate uncertainty or lack of a clear vi...   \n",
       "11  Lack of specificity can indicate a superficial...   \n",
       "\n",
       "                               what_it_might_indicate  \\\n",
       "0   Potential weaknesses in the technology or its ...   \n",
       "1   Lack of a solid plan for utilizing funds effec...   \n",
       "2   Possible issues with product-market fit or com...   \n",
       "3   Unclear value proposition compared to competit...   \n",
       "4           Lack of international expansion strategy.   \n",
       "5   Potential issues with product adoption or perf...   \n",
       "6   Concerns about competitive threats and market ...   \n",
       "7   Unpreparedness for compliance and operational ...   \n",
       "8     Potential issues with planning and forecasting.   \n",
       "9   Possible mismanagement or misunderstanding of ...   \n",
       "10          Potential issues with strategic planning.   \n",
       "11          Weakness in market analysis and strategy.   \n",
       "\n",
       "                                   follow_up_question severity  \n",
       "0   Can you provide more specific examples of how ...     high  \n",
       "1   How do you plan to allocate the recent funding...   medium  \n",
       "2   What specific challenges have you encountered ...     high  \n",
       "3   What specific features set your product apart ...     high  \n",
       "4   What are your plans for expanding beyond the J...   medium  \n",
       "5   How do you measure customer satisfaction with ...     high  \n",
       "6   How do you plan to address competitive threats...   medium  \n",
       "7   What regulatory challenges do you anticipate i...     high  \n",
       "8         What data supports your growth projections?   medium  \n",
       "9   Can you clarify the details of your funding ro...     high  \n",
       "10    What are your key milestones for the next year?   medium  \n",
       "11  Can you provide specific trends that impact yo...   medium  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ============================================================\n",
    "# 14. NG Questions & Watchouts (Do/Don't list + Risk cues)\n",
    "# ============================================================\n",
    "# Goal:\n",
    "# - Generate \"NG questions\" (questions to avoid) and \"watchouts\" for the meeting, based on:\n",
    "#   - Integrated Insights (#11)\n",
    "#   - Meeting Context Framing (#12)\n",
    "#   - Top 5 Meeting Questions (#13)\n",
    "# - Output should help the team avoid:\n",
    "#   - asking poorly framed / biased / overly leading questions\n",
    "#   - triggering defensiveness or confidentiality issues\n",
    "#   - missing key risk cues (watchouts) during the conversation\n",
    "#\n",
    "# Deliverables:\n",
    "# 1) NG Questions (bad questions) with \"why bad\" + \"better alternative\"\n",
    "# 2) Watchouts (things to listen for) with \"what it might indicate\" + \"follow-up\"\n",
    "# 3) Tone / posture guidance (how to ask, sequencing, pacing)\n",
    "#\n",
    "# Outputs:\n",
    "# - ng_questions_watchouts_<run_id>.json\n",
    "# - ng_questions_watchouts_<run_id>.md\n",
    "# - ng_questions_watchouts_<run_id>.csv (optional)\n",
    "# - ng_questions_watchouts_raw_<run_id>.txt (debug)\n",
    "\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from datetime import datetime, timezone\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Global configuration\n",
    "# ------------------------------------------------------------\n",
    "load_dotenv(\"env.txt\")\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "if not OPENAI_API_KEY:\n",
    "    raise EnvironmentError(\"OPENAI_API_KEY is not set.\")\n",
    "client = OpenAI(api_key=OPENAI_API_KEY)\n",
    "\n",
    "ART_DIR = Path(\"artifacts\") / \"meeting_deep_dive\"\n",
    "ART_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Helpers\n",
    "# ------------------------------------------------------------\n",
    "def latest_path(pattern: str) -> Path | None:\n",
    "    paths = sorted(ART_DIR.glob(pattern), key=lambda p: p.stat().st_mtime, reverse=True)\n",
    "    return paths[0] if paths else None\n",
    "\n",
    "def safe_read_json(path: Path | None) -> dict:\n",
    "    if not path or not path.exists():\n",
    "        return {}\n",
    "    return json.loads(path.read_text(encoding=\"utf-8\"))\n",
    "\n",
    "def safe_read_text(path: Path | None) -> str:\n",
    "    if not path or not path.exists():\n",
    "        return \"\"\n",
    "    return path.read_text(encoding=\"utf-8\")\n",
    "\n",
    "def _extract_json_object(text: str) -> str:\n",
    "    if not text:\n",
    "        return \"\"\n",
    "    t = text.strip()\n",
    "    m = re.search(r\"```(?:json)?\\s*(\\{.*?\\})\\s*```\", t, flags=re.DOTALL | re.IGNORECASE)\n",
    "    if m:\n",
    "        return m.group(1).strip()\n",
    "    start, end = t.find(\"{\"), t.rfind(\"}\")\n",
    "    if start != -1 and end != -1 and end > start:\n",
    "        return t[start:end+1].strip()\n",
    "    return \"\"\n",
    "\n",
    "def normalize_obj(obj: dict) -> dict:\n",
    "    \"\"\"\n",
    "    Normalize common failure mode where model echoes input or uses alternate keys.\n",
    "    \"\"\"\n",
    "    if not isinstance(obj, dict):\n",
    "        return {}\n",
    "    if set(obj.keys()) == {\"schema\", \"bundle\"}:\n",
    "        return {}\n",
    "    # Alternate top-level\n",
    "    if \"ng_questions\" not in obj:\n",
    "        for k in [\"bad_questions\", \"do_not_ask\", \"dont_ask_questions\"]:\n",
    "            if k in obj and isinstance(obj[k], list):\n",
    "                obj[\"ng_questions\"] = obj[k]\n",
    "                break\n",
    "    if \"watchouts\" not in obj:\n",
    "        for k in [\"red_flags\", \"risk_cues\", \"things_to_watch\"]:\n",
    "            if k in obj and isinstance(obj[k], list):\n",
    "                obj[\"watchouts\"] = obj[k]\n",
    "                break\n",
    "    return obj\n",
    "\n",
    "def validate_out(obj: dict):\n",
    "    if \"ng_questions\" not in obj or \"watchouts\" not in obj:\n",
    "        raise ValueError(\"Missing ng_questions or watchouts.\")\n",
    "    if len(obj.get(\"ng_questions\") or []) < 8:\n",
    "        raise ValueError(\"Need at least 8 NG questions.\")\n",
    "    if len(obj.get(\"watchouts\") or []) < 10:\n",
    "        raise ValueError(\"Need at least 10 watchouts.\")\n",
    "    if \"tone_guidance\" not in obj:\n",
    "        raise ValueError(\"Missing tone_guidance.\")\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Load inputs / integrated / framing / top questions / evidence\n",
    "# ------------------------------------------------------------\n",
    "inputs = safe_read_json(latest_path(\"inputs_*.json\"))\n",
    "entity = safe_read_json(latest_path(\"entity_*.json\"))\n",
    "integrated = safe_read_json(latest_path(\"integrated_insights_*.json\"))\n",
    "framing = safe_read_json(latest_path(\"meeting_context_framing_*.json\"))\n",
    "topq = safe_read_json(latest_path(\"top_meeting_questions_*.json\"))\n",
    "\n",
    "integrated_md = safe_read_text(latest_path(\"integrated_insights_*.md\"))\n",
    "framing_md = safe_read_text(latest_path(\"meeting_context_framing_*.md\"))\n",
    "topq_md = safe_read_text(latest_path(\"top_meeting_questions_*.md\"))\n",
    "\n",
    "run_id = (inputs.get(\"meta\", {}) or {}).get(\"run_id\") or datetime.now(timezone.utc).strftime(\"%Y%m%d_%H%M%S\")\n",
    "startup_name = (entity.get(\"canonical_name\") or inputs.get(\"startup_name\") or \"Unknown Startup\").strip()\n",
    "\n",
    "top_urls = []\n",
    "ev_path = latest_path(\"integrated_evidence_map_*.csv\")\n",
    "if ev_path and ev_path.exists():\n",
    "    try:\n",
    "        df_ev = pd.read_csv(ev_path)\n",
    "        if \"url\" in df_ev.columns:\n",
    "            top_urls = df_ev[\"url\"].dropna().astype(str).head(30).tolist()\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "bundle = {\n",
    "    \"startup_name\": startup_name,\n",
    "    \"meeting_person\": {\"name\": inputs.get(\"meeting_person_name\"), \"title\": inputs.get(\"meeting_person_title\")},\n",
    "    \"meeting_context\": inputs.get(\"meeting_context\"),\n",
    "    \"your_org_context\": inputs.get(\"your_org_context\"),\n",
    "    \"integrated_insights_json\": integrated,\n",
    "    \"meeting_context_framing_json\": framing,\n",
    "    \"top_meeting_questions_json\": topq,\n",
    "    \"integrated_md_excerpt\": integrated_md[:4500],\n",
    "    \"framing_md_excerpt\": framing_md[:3500],\n",
    "    \"top_questions_md_excerpt\": topq_md[:3000],\n",
    "    \"allowed_evidence_urls\": top_urls[:30],\n",
    "}\n",
    "\n",
    "print(\"run_id:\", run_id)\n",
    "print(\"startup_name:\", startup_name)\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# OpenAI: NG questions + watchouts\n",
    "# ------------------------------------------------------------\n",
    "SYSTEM_BASE = \"\"\"\n",
    "You are a venture diligence assistant.\n",
    "\n",
    "TASK:\n",
    "Using the provided bundle (integrated insights + meeting framing + top questions),\n",
    "produce (A) NG questions to avoid and (B) watchouts to listen for during the meeting.\n",
    "\n",
    "OUTPUT FORMAT (STRICT JSON):\n",
    "Return a single JSON object with EXACTLY these top-level keys:\n",
    "{\n",
    "  \"ng_questions\": [...],\n",
    "  \"watchouts\": [...],\n",
    "  \"tone_guidance\": {...},\n",
    "  \"confidence\": {...}\n",
    "}\n",
    "\n",
    "NG QUESTIONS REQUIREMENTS:\n",
    "- Provide 10 items.\n",
    "- Each item:\n",
    "{\n",
    "  \"category\": \"bias/leading|too_broad|too_sensitive|confidentiality|premature_pricing|other\",\n",
    "  \"ng_question\": \"string\",\n",
    "  \"why_bad\": \"string\",\n",
    "  \"better_alternative\": \"string\",\n",
    "  \"when_to_use_instead\": \"string\"\n",
    "}\n",
    "\n",
    "WATCHOUTS REQUIREMENTS:\n",
    "- Provide 12 items.\n",
    "- Each item:\n",
    "{\n",
    "  \"signal\": \"string (what you might hear/see)\",\n",
    "  \"why_it_matters\": \"string\",\n",
    "  \"what_it_might_indicate\": \"string\",\n",
    "  \"follow_up_question\": \"string\",\n",
    "  \"severity\": \"low|medium|high\"\n",
    "}\n",
    "\n",
    "TONE GUIDANCE REQUIREMENTS:\n",
    "{\n",
    "  \"posture\": [\"array of strings\"],\n",
    "  \"sequencing\": [\"array of strings (how to order topics)\"],\n",
    "  \"phrasing_patterns\": [\"array of strings (templates)\"],\n",
    "  \"things_to_avoid\": [\"array of strings\"]\n",
    "}\n",
    "\n",
    "NON-NEGOTIABLE RULES:\n",
    "- Do not invent facts.\n",
    "- Keep it practical and meeting-usable.\n",
    "- Do not include any other top-level keys.\n",
    "\"\"\".strip()\n",
    "\n",
    "SYSTEM_STRONG = (SYSTEM_BASE + \"\"\"\n",
    "\n",
    "RETRY MODE:\n",
    "- Your previous output was invalid.\n",
    "- You MUST comply with the exact JSON schema above.\n",
    "- Do NOT output \"schema\" or \"bundle\".\n",
    "\"\"\").strip()\n",
    "\n",
    "def call_model(bundle: dict, model: str, strong: bool = False) -> tuple[dict, str]:\n",
    "    system = SYSTEM_STRONG if strong else SYSTEM_BASE\n",
    "    user = json.dumps(bundle, ensure_ascii=False)\n",
    "    try:\n",
    "        resp = client.chat.completions.create(\n",
    "            model=model,\n",
    "            messages=[{\"role\": \"system\", \"content\": system},\n",
    "                      {\"role\": \"user\", \"content\": user}],\n",
    "            temperature=0.2,\n",
    "            response_format={\"type\": \"json_object\"},\n",
    "        )\n",
    "        raw = (resp.choices[0].message.content or \"\").strip()\n",
    "        out = json.loads(raw) if raw else {}\n",
    "        return out, raw\n",
    "    except Exception:\n",
    "        resp2 = client.chat.completions.create(\n",
    "            model=model,\n",
    "            messages=[{\"role\": \"system\", \"content\": system},\n",
    "                      {\"role\": \"user\", \"content\": user}],\n",
    "            temperature=0.2,\n",
    "        )\n",
    "        raw = (resp2.choices[0].message.content or \"\").strip()\n",
    "        candidate = _extract_json_object(raw)\n",
    "        out = json.loads(candidate) if candidate else {}\n",
    "        return out, raw\n",
    "\n",
    "RAW_TXT = ART_DIR / f\"ng_questions_watchouts_raw_{run_id}.txt\"\n",
    "\n",
    "# Try 1: mini\n",
    "out, raw = call_model(bundle, model=\"gpt-4o-mini\", strong=False)\n",
    "RAW_TXT.write_text(raw, encoding=\"utf-8\")\n",
    "out = normalize_obj(out)\n",
    "\n",
    "# Try 2: mini (strong)\n",
    "if not out or \"ng_questions\" not in out:\n",
    "    print(\"⚠️ Invalid output -> retry with stronger prompt...\")\n",
    "    out2, raw2 = call_model(bundle, model=\"gpt-4o-mini\", strong=True)\n",
    "    RAW_TXT.write_text(raw2, encoding=\"utf-8\")\n",
    "    out = normalize_obj(out2)\n",
    "\n",
    "# Try 3: gpt-4o (strong) if still invalid\n",
    "if not out or \"ng_questions\" not in out:\n",
    "    print(\"⚠️ Still invalid -> retry with gpt-4o (strong)...\")\n",
    "    out3, raw3 = call_model(bundle, model=\"gpt-4o\", strong=True)\n",
    "    RAW_TXT.write_text(raw3, encoding=\"utf-8\")\n",
    "    out = normalize_obj(out3)\n",
    "\n",
    "validate_out(out)\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Save outputs\n",
    "# ------------------------------------------------------------\n",
    "JSON_PATH = ART_DIR / f\"ng_questions_watchouts_{run_id}.json\"\n",
    "JSON_PATH.write_text(json.dumps(out, ensure_ascii=False, indent=2), encoding=\"utf-8\")\n",
    "print(\"✅ NG questions & watchouts JSON saved:\", JSON_PATH.as_posix())\n",
    "print(\"✅ Raw model response saved:\", RAW_TXT.as_posix())\n",
    "\n",
    "# Markdown\n",
    "def to_md(x: dict) -> str:\n",
    "    lines = []\n",
    "    lines.append(f\"# NG Questions & Watchouts: {startup_name}\\n\")\n",
    "\n",
    "    lines.append(\"## NG Questions (Do not ask like this)\\n\")\n",
    "    for i, q in enumerate((x.get(\"ng_questions\") or [])[:10], start=1):\n",
    "        lines.append(f\"### {i}. {q.get('category')}\")\n",
    "        lines.append(f\"- **NG:** {q.get('ng_question')}\")\n",
    "        lines.append(f\"- **Why bad:** {q.get('why_bad')}\")\n",
    "        lines.append(f\"- **Better:** {q.get('better_alternative')}\")\n",
    "        lines.append(f\"- **When to use instead:** {q.get('when_to_use_instead')}\")\n",
    "        lines.append(\"\")\n",
    "\n",
    "    lines.append(\"## Watchouts (Listen for these)\\n\")\n",
    "    for i, w in enumerate((x.get(\"watchouts\") or [])[:12], start=1):\n",
    "        lines.append(f\"### {i}. ({w.get('severity')}) {w.get('signal')}\")\n",
    "        lines.append(f\"- **Why it matters:** {w.get('why_it_matters')}\")\n",
    "        lines.append(f\"- **What it might indicate:** {w.get('what_it_might_indicate')}\")\n",
    "        lines.append(f\"- **Follow-up:** {w.get('follow_up_question')}\")\n",
    "        lines.append(\"\")\n",
    "\n",
    "    tg = x.get(\"tone_guidance\", {}) or {}\n",
    "    lines.append(\"## Tone / Posture Guidance\\n\")\n",
    "    lines.append(\"**Posture**\")\n",
    "    for b in (tg.get(\"posture\") or [])[:12]:\n",
    "        lines.append(f\"- {b}\")\n",
    "    lines.append(\"\\n**Sequencing**\")\n",
    "    for b in (tg.get(\"sequencing\") or [])[:12]:\n",
    "        lines.append(f\"- {b}\")\n",
    "    lines.append(\"\\n**Phrasing patterns**\")\n",
    "    for b in (tg.get(\"phrasing_patterns\") or [])[:12]:\n",
    "        lines.append(f\"- {b}\")\n",
    "    lines.append(\"\\n**Things to avoid**\")\n",
    "    for b in (tg.get(\"things_to_avoid\") or [])[:12]:\n",
    "        lines.append(f\"- {b}\")\n",
    "\n",
    "    lines.append(\"\\n\")\n",
    "    return \"\\n\".join(lines)\n",
    "\n",
    "MD_PATH = ART_DIR / f\"ng_questions_watchouts_{run_id}.md\"\n",
    "MD_PATH.write_text(to_md(out), encoding=\"utf-8\")\n",
    "print(\"✅ NG questions & watchouts MD saved:\", MD_PATH.as_posix())\n",
    "\n",
    "# Optional CSV exports\n",
    "df_ng = pd.DataFrame(out.get(\"ng_questions\") or [])\n",
    "df_w = pd.DataFrame(out.get(\"watchouts\") or [])\n",
    "\n",
    "if not df_ng.empty:\n",
    "    df_ng.insert(0, \"run_id\", run_id)\n",
    "if not df_w.empty:\n",
    "    df_w.insert(0, \"run_id\", run_id)\n",
    "\n",
    "CSV_NG = ART_DIR / f\"ng_questions_{run_id}.csv\"\n",
    "CSV_W = ART_DIR / f\"watchouts_{run_id}.csv\"\n",
    "\n",
    "df_ng.to_csv(CSV_NG, index=False)\n",
    "df_w.to_csv(CSV_W, index=False)\n",
    "\n",
    "print(\"✅ CSV saved:\", CSV_NG.as_posix(), \"and\", CSV_W.as_posix())\n",
    "\n",
    "display(df_ng.head(12))\n",
    "display(df_w.head(12))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9aa35496-0162-40fd-ba9c-6edebee8be14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ One-pager saved: artifacts/meeting_deep_dive/one_pager_20260107_225628.md\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "# Sakana AI — Initial Meeting Brief\n",
       "\n",
       "## Company Overview\n",
       "Sakana AI is a burgeoning artificial intelligence startup headquartered in Tokyo, Japan. The company is gaining significant attention for its innovative approach to AI development, which is inspired by natural processes. Sakana AI focuses on creating AI systems that mimic evolutionary and collective intelligence principles, aiming to develop more efficient and adaptive models. This approach is particularly timely as the demand for AI solutions tailored to specific cultural and linguistic contexts grows, especially in Japan. The company's recent success in securing substantial funding underscores its potential to lead in the AI sector, particularly within the Japanese market.\n",
       "\n",
       "## Team & Leadership\n",
       "The leadership team at Sakana AI is composed of highly credible figures in the AI and technology sectors. David Ha, the co-founder and CEO, brings a wealth of experience from his previous roles as a research scientist at Google Brain and a managing director at Goldman Sachs. His transition from finance to AI research highlights a unique blend of skills that are instrumental in steering Sakana AI's strategic direction. Llion Jones, the CTO, is renowned for his contributions to the transformer architecture, having co-authored the influential \"Attention is All You Need\" paper. His expertise in large-scale language models is pivotal to the company's technological advancements. Ren Ito, the COO, complements the team with his background in e-commerce and diplomacy, having served as the CEO of Mercari Europe and worked within Japan's Ministry of Foreign Affairs. This diverse leadership team is well-equipped to address the complex challenges of developing and deploying AI technologies.\n",
       "\n",
       "## Business, Product, and Technology\n",
       "Sakana AI is at the forefront of developing AI technologies that draw inspiration from natural phenomena. The company's flagship product, the AI Scientist, is designed to autonomously conduct scientific research, generate hypotheses, and produce peer-reviewed papers. This system leverages evolutionary algorithms to merge existing models, creating new, optimized AI systems that are both cost-effective and efficient. Additionally, Sakana AI employs the Continuous Thought Machine (CTM) to enhance reasoning capabilities through synchronized neuron dynamics. These technologies are distinctive due to their focus on reducing computational costs and their ability to autonomously generate scientific insights, positioning Sakana AI as a leader in nature-inspired AI development.\n",
       "\n",
       "## Customers, Market, and Go-To-Market\n",
       "Sakana AI targets a diverse range of customers, including financial institutions, government agencies, and manufacturing companies. The company's AI solutions are particularly appealing to sectors that require advanced decision-making capabilities and tailored AI models. In the Japanese market, Sakana AI is positioned as a leader in the emerging AI ecosystem, collaborating with major corporations and government entities. The company has established strategic partnerships with financial giants like MUFG Bank and Daiwa Securities, which facilitate the integration of AI into banking operations and asset management. Sakana AI's go-to-market strategy emphasizes collaboration with local enterprises and leveraging its nature-inspired technology to meet specific market needs.\n",
       "\n",
       "## Competitive Landscape\n",
       "In the competitive landscape, Sakana AI positions itself against global AI leaders such as OpenAI and Google DeepMind. While these companies have a broader global reach and established products, Sakana AI differentiates itself through its focus on nature-inspired intelligence and evolutionary algorithms. The company's emphasis on developing AI models tailored to Japan's linguistic and cultural environment provides a unique competitive edge. Additionally, Sakana AI's approach to AI safety and sustainability aligns with the growing demand for ethical AI solutions, positioning it favorably against competitors like Anthropic, which also prioritize AI safety.\n",
       "\n",
       "## Funding and Recent Developments\n",
       "Sakana AI has demonstrated robust financial growth, having raised approximately $347 million across multiple funding rounds. The company secured $135 million in its recent Series B round, achieving a post-money valuation of approximately $2.635 billion. Notable investors include Mitsubishi UFJ Financial Group, Khosla Ventures, and New Enterprise Associates, highlighting strong backing from both strategic and venture capital investors. Recent developments include Sakana AI's recognition as Japan's most valuable unicorn and its strategic shift towards expanding into defense and manufacturing sectors. These developments underscore the company's commitment to leveraging its innovative AI solutions to address diverse market needs while maintaining a strong focus on the Japanese market."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ============================================================\n",
    "# 15. One-Page Summary Generation (Integrated Narrative Memo)\n",
    "# ============================================================\n",
    "# Goal:\n",
    "# - Generate a narrative-style, one-page internal memo for an initial meeting.\n",
    "# - Use paragraph-based prose (no bullet points).\n",
    "# - Integrate findings from sections #3–#14.\n",
    "# - Clearly explain:\n",
    "#   - What the company is\n",
    "#   - Who is behind it\n",
    "#   - What it is building\n",
    "#   - Who it sells to\n",
    "#   - How it competes\n",
    "#   - How it is funded and what recently changed\n",
    "#\n",
    "# Output:\n",
    "# - one_pager_<run_id>.md\n",
    "# - Rendered Markdown in the notebook\n",
    "\n",
    "import os\n",
    "import json\n",
    "from pathlib import Path\n",
    "from datetime import datetime, timezone\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "from IPython.display import Markdown, display\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Config\n",
    "# ------------------------------------------------------------\n",
    "load_dotenv(\"env.txt\")\n",
    "\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "if not OPENAI_API_KEY:\n",
    "    raise EnvironmentError(\"OPENAI_API_KEY is not set.\")\n",
    "client = OpenAI(api_key=OPENAI_API_KEY)\n",
    "\n",
    "ART_DIR = Path(\"artifacts\") / \"meeting_deep_dive\"\n",
    "ART_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def latest_path(pattern: str):\n",
    "    paths = sorted(ART_DIR.glob(pattern), key=lambda p: p.stat().st_mtime, reverse=True)\n",
    "    return paths[0] if paths else None\n",
    "\n",
    "def safe_read_json(path):\n",
    "    if not path or not path.exists():\n",
    "        return {}\n",
    "    return json.loads(path.read_text(encoding=\"utf-8\"))\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Load artifacts (best-effort)\n",
    "# ------------------------------------------------------------\n",
    "inputs = safe_read_json(latest_path(\"inputs_*.json\"))\n",
    "entity = safe_read_json(latest_path(\"entity_*.json\"))\n",
    "\n",
    "bundle = {\n",
    "    \"startup_name\": entity.get(\"canonical_name\") or inputs.get(\"startup_name\"),\n",
    "    \"official_website\": entity.get(\"official_website\"),\n",
    "    \"meeting_person\": {\n",
    "        \"name\": inputs.get(\"meeting_person_name\"),\n",
    "        \"title\": inputs.get(\"meeting_person_title\"),\n",
    "    },\n",
    "    \"company_basics\": safe_read_json(latest_path(\"company_basics_*.json\")),\n",
    "    \"key_people\": safe_read_json(latest_path(\"people_extraction_*.json\")),\n",
    "    \"meeting_person_deep_dive\": safe_read_json(latest_path(\"meeting_person_deep_dive_*.json\")),\n",
    "    \"business_product\": safe_read_json(latest_path(\"business_product_extraction_*.json\")),\n",
    "    \"customer_market\": safe_read_json(latest_path(\"market_extraction_*.json\")),\n",
    "    \"competitive\": safe_read_json(latest_path(\"competitive_extraction_*.json\")),\n",
    "    \"funding\": safe_read_json(latest_path(\"funding_extraction_*.json\")),\n",
    "    \"recent_changes\": safe_read_json(latest_path(\"recent_timeline_extraction_*.json\")),\n",
    "    \"integrated_insights\": safe_read_json(latest_path(\"integrated_insights_*.json\")),\n",
    "}\n",
    "\n",
    "run_id = (inputs.get(\"meta\", {}) or {}).get(\"run_id\") or datetime.now(timezone.utc).strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# OpenAI: narrative one-pager\n",
    "# ------------------------------------------------------------\n",
    "SYSTEM_PROMPT = \"\"\"\n",
    "You are a venture capital research associate preparing a one-page internal memo\n",
    "for an initial meeting with a startup.\n",
    "\n",
    "Write in clear, professional prose.\n",
    "Do NOT use bullet points.\n",
    "Do NOT use labels such as \"UNKNOWN\" or \"TODO\".\n",
    "\n",
    "Structure the memo EXACTLY as follows, using Markdown headings:\n",
    "\n",
    "# {startup_name} — Initial Meeting Brief\n",
    "\n",
    "## Company Overview\n",
    "Explain what the company is, where it is based, what problem it is tackling,\n",
    "and why it is attracting attention now.\n",
    "\n",
    "## Team & Leadership\n",
    "Describe the founding and leadership team, their backgrounds, and why they are\n",
    "credible for this problem.\n",
    "\n",
    "## Business, Product, and Technology\n",
    "Explain what the company is building, how the technology works at a high level,\n",
    "and what makes it distinctive.\n",
    "\n",
    "## Customers, Market, and Go-To-Market\n",
    "Describe the target customers, market context, and how the company appears to\n",
    "approach adoption and distribution.\n",
    "\n",
    "## Competitive Landscape\n",
    "Explain who the company competes with and how it positions itself relative to\n",
    "large global players.\n",
    "\n",
    "## Funding and Recent Developments\n",
    "Summarize funding history, notable investors, and recent developments or\n",
    "strategic shifts.\n",
    "\n",
    "Writing guidelines:\n",
    "- Paragraphs only (no lists).\n",
    "- Be factual where possible, but naturally note areas where clarity is still emerging.\n",
    "- Maintain a neutral, analytical investment tone.\n",
    "- Length: roughly 900–1300 words.\n",
    "\n",
    "Output Markdown only.\n",
    "\"\"\".strip()\n",
    "\n",
    "resp = client.chat.completions.create(\n",
    "    model=\"gpt-4o\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": SYSTEM_PROMPT.format(\n",
    "            startup_name=bundle[\"startup_name\"] or \"The Company\"\n",
    "        )},\n",
    "        {\"role\": \"user\", \"content\": json.dumps(bundle, ensure_ascii=False)},\n",
    "    ],\n",
    "    temperature=0.25,\n",
    ")\n",
    "\n",
    "md = (resp.choices[0].message.content or \"\").strip()\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Save & display\n",
    "# ------------------------------------------------------------\n",
    "MD_PATH = ART_DIR / f\"one_pager_{run_id}.md\"\n",
    "MD_PATH.write_text(md, encoding=\"utf-8\")\n",
    "print(\"✅ One-pager saved:\", MD_PATH.as_posix())\n",
    "\n",
    "display(Markdown(md))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "096d8bd2-55a6-4ce1-bc5a-554c75c723cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Export manifest saved: artifacts/meeting_deep_dive/exports/manifest_20260107_225628.json\n",
      "- entity: 1 files\n",
      "- company_basics: 5 files\n",
      "- people: 5 files\n",
      "- meeting_person_deep_dive: 1 files\n",
      "- business_product: 5 files\n",
      "- market: 5 files\n",
      "- competitive: 5 files\n",
      "- funding: 6 files\n",
      "- recent_timeline: 4 files\n",
      "- integrated_insights: 2 files\n",
      "- meeting_context_framing: 2 files\n",
      "- top_meeting_questions: 4 files\n",
      "- ng_questions_watchouts: 3 files\n",
      "- one_pager: 3 files\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# 16. Export & Save Artifacts (Run-level manifest)\n",
    "# ============================================================\n",
    "# Purpose:\n",
    "# - Collect all key artifacts generated in this run\n",
    "# - Create a simple manifest for reuse (Notion / Drive / IC memo)\n",
    "# - No new analysis is performed in this step\n",
    "\n",
    "import json\n",
    "from pathlib import Path\n",
    "from datetime import datetime, timezone\n",
    "\n",
    "ART_DIR = Path(\"artifacts\") / \"meeting_deep_dive\"\n",
    "EXPORT_DIR = ART_DIR / \"exports\"\n",
    "EXPORT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "run_id = \"20260107_225628\"  # or read from inputs if you prefer\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Helper: collect existing artifacts\n",
    "# ------------------------------------------------------------\n",
    "def collect(patterns):\n",
    "    files = []\n",
    "    for pat in patterns:\n",
    "        files.extend(sorted(ART_DIR.glob(pat)))\n",
    "    return [p.as_posix() for p in files]\n",
    "\n",
    "manifest = {\n",
    "    \"run_id\": run_id,\n",
    "    \"exported_at_utc\": datetime.now(timezone.utc).isoformat(),\n",
    "    \"artifacts\": {\n",
    "        \"entity\": collect([\"entity_*.json\"]),\n",
    "        \"company_basics\": collect([\"company_*.*\"]),\n",
    "        \"people\": collect([\"people_*.*\"]),\n",
    "        \"meeting_person_deep_dive\": collect([\"meeting_person_deep_dive_*.*\"]),\n",
    "        \"business_product\": collect([\"business_product_*.*\"]),\n",
    "        \"market\": collect([\"market_*.*\"]),\n",
    "        \"competitive\": collect([\"competitive_*.*\"]),\n",
    "        \"funding\": collect([\"funding_*.*\"]),\n",
    "        \"recent_timeline\": collect([\"recent_*.*\"]),\n",
    "        \"integrated_insights\": collect([\"integrated_insights_*.*\"]),\n",
    "        \"meeting_context_framing\": collect([\"meeting_context_framing_*.*\"]),\n",
    "        \"top_meeting_questions\": collect([\"top_meeting_questions_*.*\"]),\n",
    "        \"ng_questions_watchouts\": collect([\"ng_questions_watchouts_*.*\"]),\n",
    "        \"one_pager\": collect([\"one_pager_*.*\"]),\n",
    "    }\n",
    "}\n",
    "\n",
    "MANIFEST_PATH = EXPORT_DIR / f\"manifest_{run_id}.json\"\n",
    "MANIFEST_PATH.write_text(json.dumps(manifest, ensure_ascii=False, indent=2), encoding=\"utf-8\")\n",
    "\n",
    "print(\"✅ Export manifest saved:\", MANIFEST_PATH.as_posix())\n",
    "\n",
    "# Optional: quick view\n",
    "for k, v in manifest[\"artifacts\"].items():\n",
    "    if v:\n",
    "        print(f\"- {k}: {len(v)} files\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf488e8b-773f-4d8e-82a4-152e000b7736",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71e57ee4-084f-4ddc-bdb0-11516984fdf3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62b0d9b4-3db3-4199-8d97-d7fa7c3200f3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6e5532c-ce92-4a2d-bc3c-92fac9163519",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4190f173-f3b5-4457-b824-8b4ecc2d5f48",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
