{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02c91bdb-855d-4c7f-ab59-913ae505a147",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 016 fetch_newsapi_weekly\n",
    "# ============================================================\n",
    "#\n",
    "# Overview\n",
    "# --------\n",
    "# This notebook fetches news articles from NewsAPI.org on a weekly basis\n",
    "# for a fixed set of 15 investment-relevant technology keywords.\n",
    "#\n",
    "# It is designed for recurring, automated execution (e.g., via GitHub Actions)\n",
    "# and produces raw, week-partitioned article datasets that serve as the\n",
    "# foundation for downstream aggregation and trend analysis.\n",
    "#\n",
    "# The notebook focuses exclusively on data collection and normalization.\n",
    "# Aggregation, normalization across weeks, and visualization are handled\n",
    "# in a separate downstream notebook.\n",
    "#\n",
    "#\n",
    "# Structure\n",
    "# ---------\n",
    "# Cell 0 : Purpose / Weekly Run Specification\n",
    "# Cell 1 : Imports & Global Configuration\n",
    "# Cell 2 : Target Week Calculation (automatic weekly time window)\n",
    "# Cell 3 : Load Keyword Dictionary (15 predefined keywords)\n",
    "# Cell 4 : Build NewsAPI Query Parameters\n",
    "# Cell 5 : Fetch Loop (pagination, retry, rate-limit handling)\n",
    "# Cell 6 : Normalize Raw API Responses\n",
    "# Cell 7 : Deduplicate Articles (URL-based)\n",
    "# Cell 8 : Save Raw Outputs (week-partitioned)\n",
    "# Cell 9 : Weekly Fetch Summary (lightweight report)\n",
    "#\n",
    "#\n",
    "# Notes\n",
    "# -----\n",
    "# - The execution target is the most recent *completed* week to ensure\n",
    "#   temporal consistency in weekly trend analysis.\n",
    "# - API credentials must be provided via environment variables\n",
    "#   (e.g., NEWSAPI_KEY); no secrets should be hard-coded.\n",
    "# - The output of this notebook is intentionally kept \"raw\":\n",
    "#   minimal transformation, maximal traceability to the original API responses.\n",
    "# - All outputs are partitioned by ISO year-week (YYYY-WW) to support\n",
    "#   incremental updates and reproducible backfills.\n",
    "# - When using the NewsAPI Developer plan, the /v2/everything endpoint is\n",
    "#   capped at 100 retrievable results per query; this notebook is designed\n",
    "#   to operate safely under that constraint.\n",
    "# - This notebook is safe to rerun for the same week; downstream logic\n",
    "#   should handle idempotency and deduplication.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2ed866e2-4c91-43de-a782-c6e32b7a73b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration loaded successfully.\n",
      "Raw data directory: /Users/yuetoya/Desktop/researchOS100-private/notebooks/data/raw/newsapi\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# Cell 1: Imports & Global Config\n",
    "# ============================================================\n",
    "\n",
    "# --- Standard library imports ---\n",
    "import os\n",
    "import time\n",
    "from datetime import datetime, timedelta, timezone\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Any\n",
    "\n",
    "# --- Third-party imports ---\n",
    "import requests\n",
    "import pandas as pd\n",
    "\n",
    "# --- Environment variables ---\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Explicitly load env.txt (instead of default .env)\n",
    "load_dotenv(\"env.txt\")\n",
    "NEWSAPI_KEY = os.getenv(\"NEWSAPI_KEY\")\n",
    "if NEWSAPI_KEY is None:\n",
    "    raise RuntimeError(\n",
    "        \"NEWSAPI_KEY is not set. Please provide it via an environment variable.\"\n",
    "    )\n",
    "\n",
    "# --- NewsAPI base configuration ---\n",
    "NEWSAPI_ENDPOINT = \"https://newsapi.org/v2/everything\"\n",
    "DEFAULT_LANGUAGE = \"en\"\n",
    "DEFAULT_PAGE_SIZE = 100          # Max allowed by NewsAPI\n",
    "DEFAULT_SORT_BY = \"publishedAt\"  # Ensures chronological ordering\n",
    "\n",
    "# --- Rate limit & retry configuration ---\n",
    "REQUEST_TIMEOUT_SEC = 30\n",
    "MAX_RETRIES = 3\n",
    "RETRY_SLEEP_SEC = 5              # Backoff for transient failures\n",
    "RATE_LIMIT_SLEEP_SEC = 1         # Small delay between requests (safety)\n",
    "\n",
    "# --- Project paths ---\n",
    "PROJECT_ROOT = Path(\".\").resolve()\n",
    "DATA_DIR = PROJECT_ROOT / \"data\"\n",
    "RAW_DATA_DIR = DATA_DIR / \"raw\" / \"newsapi\"\n",
    "\n",
    "# Ensure output directories exist\n",
    "RAW_DATA_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# --- Logging / display options ---\n",
    "pd.set_option(\"display.max_rows\", 100)\n",
    "pd.set_option(\"display.max_columns\", 50)\n",
    "\n",
    "print(\"Configuration loaded successfully.\")\n",
    "print(f\"Raw data directory: {RAW_DATA_DIR}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bf74f4c5-6481-4f9f-97cd-d35857dd471b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target week resolved successfully.\n",
      "ISO week        : 2025-W52\n",
      "Week start (UTC): 2025-12-22T00:00:00+00:00\n",
      "Week end   (UTC): 2025-12-28T23:59:59+00:00\n",
      "NewsAPI from    : 2025-12-22T00:00:00Z\n",
      "NewsAPI to      : 2025-12-28T23:59:59Z\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# Cell 2: Target Week (Automatic Weekly Time Window)\n",
    "# ============================================================\n",
    "\n",
    "# This notebook targets the most recent *completed* ISO week.\n",
    "# This avoids partial-week data and ensures consistency for\n",
    "# weekly trend aggregation.\n",
    "\n",
    "# Current time (UTC is used for consistency across environments)\n",
    "now_utc = datetime.now(timezone.utc)\n",
    "\n",
    "# Determine the start of the current ISO week (Monday 00:00 UTC)\n",
    "current_week_start = now_utc - timedelta(\n",
    "    days=now_utc.isoweekday() - 1,\n",
    "    hours=now_utc.hour,\n",
    "    minutes=now_utc.minute,\n",
    "    seconds=now_utc.second,\n",
    "    microseconds=now_utc.microsecond,\n",
    ")\n",
    "\n",
    "# The target week is the week immediately preceding the current week\n",
    "target_week_start = current_week_start - timedelta(weeks=1)\n",
    "target_week_end = current_week_start - timedelta(seconds=1)\n",
    "\n",
    "# ISO year-week for partitioning and logging\n",
    "iso_year, iso_week, _ = target_week_start.isocalendar()\n",
    "week_label = f\"{iso_year}-W{iso_week:02d}\"\n",
    "\n",
    "# Format dates for NewsAPI (ISO 8601)\n",
    "newsapi_from = target_week_start.strftime(\"%Y-%m-%dT%H:%M:%SZ\")\n",
    "newsapi_to = target_week_end.strftime(\"%Y-%m-%dT%H:%M:%SZ\")\n",
    "\n",
    "print(\"Target week resolved successfully.\")\n",
    "print(f\"ISO week        : {week_label}\")\n",
    "print(f\"Week start (UTC): {target_week_start.isoformat()}\")\n",
    "print(f\"Week end   (UTC): {target_week_end.isoformat()}\")\n",
    "print(f\"NewsAPI from    : {newsapi_from}\")\n",
    "print(f\"NewsAPI to      : {newsapi_to}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bf484710-ea1b-4d9d-ba42-d125049b2724",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Edit aliases if needed (comma-separated):\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a00af4df017940a3b307722ff75e5841",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Text(value='generative AI, GenAI', description='Generative AI', layout=Layout(width='85%'), placeholder='comma…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "53adb8810199406cb42d5a11e28e5059",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Text(value='AI agent, autonomous agent', description='AI Agents', layout=Layout(width='85%'), placeholder='com…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b93c5257581c446191a8f19e623f78a9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Text(value='robotics, autonomous robot, industrial robot, humanoid robot', description='Robotics', layout=Layo…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "06a73147ec584064b9f11290902ec4a5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Text(value='edge AI, on-device AI, on device AI', description='Edge AI', layout=Layout(width='85%'), placehold…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dcc038eb1798443f82f7f3c16b6ca53e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Text(value='digital health, health IT, healthcare IT', description='Digital Health', layout=Layout(width='85%'…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef49f49222b94ce786aff78e9be6b573",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Text(value='healthcare AI, medical AI, AI in healthcare', description='AI in Healthcare', layout=Layout(width=…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b948c4a67c79400787abe6324b186eda",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Text(value='precision medicine, personalized medicine', description='Precision Medicine', layout=Layout(width=…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9990292d52234702a9ed362d35f44478",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Text(value='clinical AI, clinical decision support, CDS', description='Clinical AI', layout=Layout(width='85%'…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c7b614b90e844d7a60d34dd7bf2835e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Text(value='climate tech, climate technology, cleantech', description='Climate Tech', layout=Layout(width='85%…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0bc89dc5397f42c684be393dda512af1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Text(value='carbon capture, CCS, carbon removal', description='Carbon Capture', layout=Layout(width='85%'), pl…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "11e31b4f3d3e4cc797006a5b2fa2376f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Text(value='energy storage, battery storage, grid storage', description='Energy Storage', layout=Layout(width=…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ecfeb6b67aa142a1bf03dee12e3becf1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Text(value='quantum computing, quantum technology', description='Quantum Computing', layout=Layout(width='85%'…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "095ee8885a594c7cb153e2441df35320",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Text(value='advanced materials, material science, materials science', description='Advanced Materials', layout…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2751aa31fdd64b319f4a00660ee7ad4b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Text(value='nuclear fusion, fusion energy', description='Nuclear Fusion', layout=Layout(width='85%'), placehol…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ab1a7bdd91ba4138ab4ff63635f423da",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Text(value='space technology, space startup, satellite, space launch', description='SpaceTech', layout=Layout(…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Final keyword dictionary (preview):\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>keyword_id</th>\n",
       "      <th>canonical</th>\n",
       "      <th>category</th>\n",
       "      <th>aliases</th>\n",
       "      <th>query</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ai_agents</td>\n",
       "      <td>AI Agents</td>\n",
       "      <td>AI</td>\n",
       "      <td>AI agent, autonomous agent</td>\n",
       "      <td>\"AI agent\" OR \"autonomous agent\"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>edge_ai</td>\n",
       "      <td>Edge AI</td>\n",
       "      <td>AI</td>\n",
       "      <td>edge AI, on-device AI, on device AI</td>\n",
       "      <td>\"edge AI\" OR \"on-device AI\" OR \"on device AI\"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>generative_ai</td>\n",
       "      <td>Generative AI</td>\n",
       "      <td>AI</td>\n",
       "      <td>generative AI, GenAI</td>\n",
       "      <td>\"generative AI\" OR \"GenAI\"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>robotics</td>\n",
       "      <td>Robotics</td>\n",
       "      <td>AI</td>\n",
       "      <td>robotics, autonomous robot, industrial robot, ...</td>\n",
       "      <td>\"robotics\" OR \"autonomous robot\" OR \"industria...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>carbon_capture</td>\n",
       "      <td>Carbon Capture</td>\n",
       "      <td>Climate</td>\n",
       "      <td>carbon capture, CCS, carbon removal</td>\n",
       "      <td>\"carbon capture\" OR \"CCS\" OR \"carbon removal\"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>climate_tech</td>\n",
       "      <td>Climate Tech</td>\n",
       "      <td>Climate</td>\n",
       "      <td>climate tech, climate technology, cleantech</td>\n",
       "      <td>\"climate tech\" OR \"climate technology\" OR \"cle...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>energy_storage</td>\n",
       "      <td>Energy Storage</td>\n",
       "      <td>Climate</td>\n",
       "      <td>energy storage, battery storage, grid storage</td>\n",
       "      <td>\"energy storage\" OR \"battery storage\" OR \"grid...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>advanced_materials</td>\n",
       "      <td>Advanced Materials</td>\n",
       "      <td>Deeptech</td>\n",
       "      <td>advanced materials, material science, material...</td>\n",
       "      <td>\"advanced materials\" OR \"material science\" OR ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>nuclear_fusion</td>\n",
       "      <td>Nuclear Fusion</td>\n",
       "      <td>Deeptech</td>\n",
       "      <td>nuclear fusion, fusion energy</td>\n",
       "      <td>\"nuclear fusion\" OR \"fusion energy\"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>quantum_computing</td>\n",
       "      <td>Quantum Computing</td>\n",
       "      <td>Deeptech</td>\n",
       "      <td>quantum computing, quantum technology</td>\n",
       "      <td>\"quantum computing\" OR \"quantum technology\"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>spacetech</td>\n",
       "      <td>SpaceTech</td>\n",
       "      <td>Deeptech</td>\n",
       "      <td>space technology, space startup, satellite, sp...</td>\n",
       "      <td>\"space technology\" OR \"space startup\" OR \"sate...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>ai_healthcare</td>\n",
       "      <td>AI in Healthcare</td>\n",
       "      <td>Healthtech</td>\n",
       "      <td>healthcare AI, medical AI, AI in healthcare</td>\n",
       "      <td>\"healthcare AI\" OR \"medical AI\" OR \"AI in heal...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>clinical_ai</td>\n",
       "      <td>Clinical AI</td>\n",
       "      <td>Healthtech</td>\n",
       "      <td>clinical AI, clinical decision support, CDS</td>\n",
       "      <td>\"clinical AI\" OR \"clinical decision support\" O...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>digital_health</td>\n",
       "      <td>Digital Health</td>\n",
       "      <td>Healthtech</td>\n",
       "      <td>digital health, health IT, healthcare IT</td>\n",
       "      <td>\"digital health\" OR \"health IT\" OR \"healthcare...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>precision_medicine</td>\n",
       "      <td>Precision Medicine</td>\n",
       "      <td>Healthtech</td>\n",
       "      <td>precision medicine, personalized medicine</td>\n",
       "      <td>\"precision medicine\" OR \"personalized medicine\"</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            keyword_id           canonical    category  \\\n",
       "0            ai_agents           AI Agents          AI   \n",
       "1              edge_ai             Edge AI          AI   \n",
       "2        generative_ai       Generative AI          AI   \n",
       "3             robotics            Robotics          AI   \n",
       "4       carbon_capture      Carbon Capture     Climate   \n",
       "5         climate_tech        Climate Tech     Climate   \n",
       "6       energy_storage      Energy Storage     Climate   \n",
       "7   advanced_materials  Advanced Materials    Deeptech   \n",
       "8       nuclear_fusion      Nuclear Fusion    Deeptech   \n",
       "9    quantum_computing   Quantum Computing    Deeptech   \n",
       "10           spacetech           SpaceTech    Deeptech   \n",
       "11       ai_healthcare    AI in Healthcare  Healthtech   \n",
       "12         clinical_ai         Clinical AI  Healthtech   \n",
       "13      digital_health      Digital Health  Healthtech   \n",
       "14  precision_medicine  Precision Medicine  Healthtech   \n",
       "\n",
       "                                              aliases  \\\n",
       "0                          AI agent, autonomous agent   \n",
       "1                 edge AI, on-device AI, on device AI   \n",
       "2                                generative AI, GenAI   \n",
       "3   robotics, autonomous robot, industrial robot, ...   \n",
       "4                 carbon capture, CCS, carbon removal   \n",
       "5         climate tech, climate technology, cleantech   \n",
       "6       energy storage, battery storage, grid storage   \n",
       "7   advanced materials, material science, material...   \n",
       "8                       nuclear fusion, fusion energy   \n",
       "9               quantum computing, quantum technology   \n",
       "10  space technology, space startup, satellite, sp...   \n",
       "11        healthcare AI, medical AI, AI in healthcare   \n",
       "12        clinical AI, clinical decision support, CDS   \n",
       "13           digital health, health IT, healthcare IT   \n",
       "14          precision medicine, personalized medicine   \n",
       "\n",
       "                                                query  \n",
       "0                    \"AI agent\" OR \"autonomous agent\"  \n",
       "1       \"edge AI\" OR \"on-device AI\" OR \"on device AI\"  \n",
       "2                          \"generative AI\" OR \"GenAI\"  \n",
       "3   \"robotics\" OR \"autonomous robot\" OR \"industria...  \n",
       "4       \"carbon capture\" OR \"CCS\" OR \"carbon removal\"  \n",
       "5   \"climate tech\" OR \"climate technology\" OR \"cle...  \n",
       "6   \"energy storage\" OR \"battery storage\" OR \"grid...  \n",
       "7   \"advanced materials\" OR \"material science\" OR ...  \n",
       "8                 \"nuclear fusion\" OR \"fusion energy\"  \n",
       "9         \"quantum computing\" OR \"quantum technology\"  \n",
       "10  \"space technology\" OR \"space startup\" OR \"sate...  \n",
       "11  \"healthcare AI\" OR \"medical AI\" OR \"AI in heal...  \n",
       "12  \"clinical AI\" OR \"clinical decision support\" O...  \n",
       "13  \"digital health\" OR \"health IT\" OR \"healthcare...  \n",
       "14    \"precision medicine\" OR \"personalized medicine\"  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Global exclusions (to be applied when building request params):\n",
      "['job', 'jobs', 'hiring', 'recruiting', 'careers']\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# Cell 3: Load Keyword Dictionary (15 keywords with editable aliases)\n",
    "# ============================================================\n",
    "\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# Default keyword definitions (canonical + category)\n",
    "# ------------------------------------------------------------------\n",
    "# NOTE:\n",
    "# - Canonical set is fixed (15 keywords).\n",
    "# - Aliases are editable via widgets (comma-separated).\n",
    "# - A NewsAPI query string is auto-generated from aliases.\n",
    "# - \"Foundation Models\" is replaced with \"Robotics\" per the latest decision.\n",
    "\n",
    "DEFAULT_KEYWORDS = [\n",
    "    # AI / Robotics (4)\n",
    "    {\"keyword_id\": \"generative_ai\", \"canonical\": \"Generative AI\", \"category\": \"AI\"},\n",
    "    {\"keyword_id\": \"ai_agents\", \"canonical\": \"AI Agents\", \"category\": \"AI\"},\n",
    "    {\"keyword_id\": \"robotics\", \"canonical\": \"Robotics\", \"category\": \"AI\"},\n",
    "    {\"keyword_id\": \"edge_ai\", \"canonical\": \"Edge AI\", \"category\": \"AI\"},\n",
    "\n",
    "    # Healthtech (4)\n",
    "    {\"keyword_id\": \"digital_health\", \"canonical\": \"Digital Health\", \"category\": \"Healthtech\"},\n",
    "    {\"keyword_id\": \"ai_healthcare\", \"canonical\": \"AI in Healthcare\", \"category\": \"Healthtech\"},\n",
    "    {\"keyword_id\": \"precision_medicine\", \"canonical\": \"Precision Medicine\", \"category\": \"Healthtech\"},\n",
    "    {\"keyword_id\": \"clinical_ai\", \"canonical\": \"Clinical AI\", \"category\": \"Healthtech\"},\n",
    "\n",
    "    # Climate (3)\n",
    "    {\"keyword_id\": \"climate_tech\", \"canonical\": \"Climate Tech\", \"category\": \"Climate\"},\n",
    "    {\"keyword_id\": \"carbon_capture\", \"canonical\": \"Carbon Capture\", \"category\": \"Climate\"},\n",
    "    {\"keyword_id\": \"energy_storage\", \"canonical\": \"Energy Storage\", \"category\": \"Climate\"},\n",
    "\n",
    "    # Deeptech / Frontier (4)\n",
    "    {\"keyword_id\": \"quantum_computing\", \"canonical\": \"Quantum Computing\", \"category\": \"Deeptech\"},\n",
    "    {\"keyword_id\": \"advanced_materials\", \"canonical\": \"Advanced Materials\", \"category\": \"Deeptech\"},\n",
    "    {\"keyword_id\": \"nuclear_fusion\", \"canonical\": \"Nuclear Fusion\", \"category\": \"Deeptech\"},\n",
    "    {\"keyword_id\": \"spacetech\", \"canonical\": \"SpaceTech\", \"category\": \"Deeptech\"},\n",
    "]\n",
    "\n",
    "keywords_df = pd.DataFrame(DEFAULT_KEYWORDS)\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# Default aliases (editable)\n",
    "# ------------------------------------------------------------------\n",
    "# Guidance:\n",
    "# - Keep aliases as common surface forms that appear in headlines.\n",
    "# - Avoid overly broad terms that create excessive noise.\n",
    "# - Add/remove aliases over time as the market vocabulary evolves.\n",
    "\n",
    "DEFAULT_ALIASES = {\n",
    "    # AI / Robotics\n",
    "    \"generative_ai\": \"generative AI, GenAI\",\n",
    "    \"ai_agents\": \"AI agent, autonomous agent\",\n",
    "    \"robotics\": \"robotics, autonomous robot, industrial robot, humanoid robot\",\n",
    "    \"edge_ai\": \"edge AI, on-device AI, on device AI\",\n",
    "\n",
    "    # Healthtech\n",
    "    \"digital_health\": \"digital health, health IT, healthcare IT\",\n",
    "    \"ai_healthcare\": \"healthcare AI, medical AI, AI in healthcare\",\n",
    "    \"precision_medicine\": \"precision medicine, personalized medicine\",\n",
    "    \"clinical_ai\": \"clinical AI, clinical decision support, CDS\",\n",
    "\n",
    "    # Climate\n",
    "    \"climate_tech\": \"climate tech, climate technology, cleantech\",\n",
    "    \"carbon_capture\": \"carbon capture, CCS, carbon removal\",\n",
    "    \"energy_storage\": \"energy storage, battery storage, grid storage\",\n",
    "\n",
    "    # Deeptech / Frontier\n",
    "    \"quantum_computing\": \"quantum computing, quantum technology\",\n",
    "    \"advanced_materials\": \"advanced materials, material science, materials science\",\n",
    "    \"nuclear_fusion\": \"nuclear fusion, fusion energy\",\n",
    "    \"spacetech\": \"space technology, space startup, satellite, space launch\",\n",
    "}\n",
    "\n",
    "# Optional global exclusions to reduce common noise (can be extended later).\n",
    "# These are applied in later cells when building request params.\n",
    "GLOBAL_EXCLUSIONS = [\n",
    "    \"job\", \"jobs\", \"hiring\", \"recruiting\", \"careers\"\n",
    "]\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# Widgets for alias editing\n",
    "# ------------------------------------------------------------------\n",
    "alias_widgets = {}\n",
    "\n",
    "print(\"Edit aliases if needed (comma-separated):\\n\")\n",
    "\n",
    "for row in keywords_df.itertuples(index=False):\n",
    "    w = widgets.Text(\n",
    "        value=DEFAULT_ALIASES.get(row.keyword_id, \"\"),\n",
    "        description=row.canonical,\n",
    "        layout=widgets.Layout(width=\"85%\"),\n",
    "        style={\"description_width\": \"220px\"},\n",
    "        placeholder=\"comma-separated aliases (e.g., term1, term2, term3)\"\n",
    "    )\n",
    "    alias_widgets[row.keyword_id] = w\n",
    "    display(w)\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# Build final keyword table (canonical + aliases + NewsAPI query)\n",
    "# ------------------------------------------------------------------\n",
    "\n",
    "def _build_or_query_from_aliases(aliases_csv: str) -> str:\n",
    "    \"\"\"\n",
    "    Convert a comma-separated alias string into a NewsAPI-friendly OR query:\n",
    "      term1, term2 -> \"term1\" OR \"term2\"\n",
    "\n",
    "    Returns an empty string if no aliases are provided.\n",
    "    \"\"\"\n",
    "    if aliases_csv is None:\n",
    "        return \"\"\n",
    "    terms = [t.strip() for t in str(aliases_csv).split(\",\") if t.strip()]\n",
    "    if not terms:\n",
    "        return \"\"\n",
    "    return \" OR \".join([f'\"{t}\"' for t in terms])\n",
    "\n",
    "def build_keyword_table() -> pd.DataFrame:\n",
    "    records = []\n",
    "    for row in keywords_df.itertuples(index=False):\n",
    "        aliases = alias_widgets[row.keyword_id].value\n",
    "        query = _build_or_query_from_aliases(aliases)\n",
    "\n",
    "        records.append({\n",
    "            \"keyword_id\": row.keyword_id,\n",
    "            \"canonical\": row.canonical,\n",
    "            \"category\": row.category,\n",
    "            \"aliases\": aliases,\n",
    "            \"query\": query\n",
    "        })\n",
    "    return pd.DataFrame(records)\n",
    "\n",
    "keywords_df_final = build_keyword_table()\n",
    "\n",
    "# Validate\n",
    "if keywords_df_final[\"query\"].isna().any() or (keywords_df_final[\"query\"].str.len() == 0).any():\n",
    "    empty = keywords_df_final[keywords_df_final[\"query\"].str.len() == 0][[\"keyword_id\", \"canonical\"]]\n",
    "    raise ValueError(\n",
    "        \"Some keywords have empty queries (aliases missing). \"\n",
    "        \"Please fill aliases for all keywords.\\n\"\n",
    "        f\"{empty}\"\n",
    "    )\n",
    "\n",
    "print(\"\\nFinal keyword dictionary (preview):\")\n",
    "display(\n",
    "    keywords_df_final[[\"keyword_id\", \"canonical\", \"category\", \"aliases\", \"query\"]]\n",
    "    .sort_values([\"category\", \"canonical\"])\n",
    "    .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "print(\"\\nGlobal exclusions (to be applied when building request params):\")\n",
    "print(GLOBAL_EXCLUSIONS)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "277f61f1-7bf7-4ac4-8366-b4e331a07b92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Built query parameters for 15 keywords.\n",
      "\n",
      "[Generative AI]\n",
      "Query : (\"generative AI\" OR \"GenAI\") AND NOT (job OR jobs OR hiring OR recruiting OR careers)\n",
      "From  : 2025-12-22T00:00:00Z\n",
      "To    : 2025-12-28T23:59:59Z\n",
      "--------------------------------------------------------------------------------\n",
      "[AI Agents]\n",
      "Query : (\"AI agent\" OR \"autonomous agent\") AND NOT (job OR jobs OR hiring OR recruiting OR careers)\n",
      "From  : 2025-12-22T00:00:00Z\n",
      "To    : 2025-12-28T23:59:59Z\n",
      "--------------------------------------------------------------------------------\n",
      "[Robotics]\n",
      "Query : (\"robotics\" OR \"autonomous robot\" OR \"industrial robot\" OR \"humanoid robot\") AND NOT (job OR jobs OR hiring OR recruiting OR careers)\n",
      "From  : 2025-12-22T00:00:00Z\n",
      "To    : 2025-12-28T23:59:59Z\n",
      "--------------------------------------------------------------------------------\n",
      "[Edge AI]\n",
      "Query : (\"edge AI\" OR \"on-device AI\" OR \"on device AI\") AND NOT (job OR jobs OR hiring OR recruiting OR careers)\n",
      "From  : 2025-12-22T00:00:00Z\n",
      "To    : 2025-12-28T23:59:59Z\n",
      "--------------------------------------------------------------------------------\n",
      "[Digital Health]\n",
      "Query : (\"digital health\" OR \"health IT\" OR \"healthcare IT\") AND NOT (job OR jobs OR hiring OR recruiting OR careers)\n",
      "From  : 2025-12-22T00:00:00Z\n",
      "To    : 2025-12-28T23:59:59Z\n",
      "--------------------------------------------------------------------------------\n",
      "[AI in Healthcare]\n",
      "Query : (\"healthcare AI\" OR \"medical AI\" OR \"AI in healthcare\") AND NOT (job OR jobs OR hiring OR recruiting OR careers)\n",
      "From  : 2025-12-22T00:00:00Z\n",
      "To    : 2025-12-28T23:59:59Z\n",
      "--------------------------------------------------------------------------------\n",
      "[Precision Medicine]\n",
      "Query : (\"precision medicine\" OR \"personalized medicine\") AND NOT (job OR jobs OR hiring OR recruiting OR careers)\n",
      "From  : 2025-12-22T00:00:00Z\n",
      "To    : 2025-12-28T23:59:59Z\n",
      "--------------------------------------------------------------------------------\n",
      "[Clinical AI]\n",
      "Query : (\"clinical AI\" OR \"clinical decision support\" OR \"CDS\") AND NOT (job OR jobs OR hiring OR recruiting OR careers)\n",
      "From  : 2025-12-22T00:00:00Z\n",
      "To    : 2025-12-28T23:59:59Z\n",
      "--------------------------------------------------------------------------------\n",
      "[Climate Tech]\n",
      "Query : (\"climate tech\" OR \"climate technology\" OR \"cleantech\") AND NOT (job OR jobs OR hiring OR recruiting OR careers)\n",
      "From  : 2025-12-22T00:00:00Z\n",
      "To    : 2025-12-28T23:59:59Z\n",
      "--------------------------------------------------------------------------------\n",
      "[Carbon Capture]\n",
      "Query : (\"carbon capture\" OR \"CCS\" OR \"carbon removal\") AND NOT (job OR jobs OR hiring OR recruiting OR careers)\n",
      "From  : 2025-12-22T00:00:00Z\n",
      "To    : 2025-12-28T23:59:59Z\n",
      "--------------------------------------------------------------------------------\n",
      "[Energy Storage]\n",
      "Query : (\"energy storage\" OR \"battery storage\" OR \"grid storage\") AND NOT (job OR jobs OR hiring OR recruiting OR careers)\n",
      "From  : 2025-12-22T00:00:00Z\n",
      "To    : 2025-12-28T23:59:59Z\n",
      "--------------------------------------------------------------------------------\n",
      "[Quantum Computing]\n",
      "Query : (\"quantum computing\" OR \"quantum technology\") AND NOT (job OR jobs OR hiring OR recruiting OR careers)\n",
      "From  : 2025-12-22T00:00:00Z\n",
      "To    : 2025-12-28T23:59:59Z\n",
      "--------------------------------------------------------------------------------\n",
      "[Advanced Materials]\n",
      "Query : (\"advanced materials\" OR \"material science\" OR \"materials science\") AND NOT (job OR jobs OR hiring OR recruiting OR careers)\n",
      "From  : 2025-12-22T00:00:00Z\n",
      "To    : 2025-12-28T23:59:59Z\n",
      "--------------------------------------------------------------------------------\n",
      "[Nuclear Fusion]\n",
      "Query : (\"nuclear fusion\" OR \"fusion energy\") AND NOT (job OR jobs OR hiring OR recruiting OR careers)\n",
      "From  : 2025-12-22T00:00:00Z\n",
      "To    : 2025-12-28T23:59:59Z\n",
      "--------------------------------------------------------------------------------\n",
      "[SpaceTech]\n",
      "Query : (\"space technology\" OR \"space startup\" OR \"satellite\" OR \"space launch\") AND NOT (job OR jobs OR hiring OR recruiting OR careers)\n",
      "From  : 2025-12-22T00:00:00Z\n",
      "To    : 2025-12-28T23:59:59Z\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# Cell 4: Build Query Params\n",
    "# ============================================================\n",
    "\n",
    "# This cell constructs NewsAPI request parameter dictionaries\n",
    "# for each keyword and the target weekly time window.\n",
    "#\n",
    "# The output of this cell is a list of request-ready parameter\n",
    "# objects that can be consumed directly by the fetch loop.\n",
    "\n",
    "from urllib.parse import urlencode\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# Helper: build full query string with global exclusions\n",
    "# ------------------------------------------------------------------\n",
    "\n",
    "def build_full_query(base_query: str, exclusions: list) -> str:\n",
    "    \"\"\"\n",
    "    Combine a base OR-query with global exclusion terms.\n",
    "\n",
    "    Example:\n",
    "      base_query = '\"robotics\" OR \"autonomous robot\"'\n",
    "      exclusions = [\"job\", \"hiring\"]\n",
    "\n",
    "      -> ('\"robotics\" OR \"autonomous robot\"') AND NOT (job OR hiring)\n",
    "    \"\"\"\n",
    "    if not exclusions:\n",
    "        return base_query\n",
    "\n",
    "    exclusion_clause = \" OR \".join(exclusions)\n",
    "    return f\"({base_query}) AND NOT ({exclusion_clause})\"\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# Build parameter sets for each keyword\n",
    "# ------------------------------------------------------------------\n",
    "\n",
    "query_params_list = []\n",
    "\n",
    "for row in keywords_df_final.itertuples(index=False):\n",
    "    if not row.query:\n",
    "        continue\n",
    "\n",
    "    full_query = build_full_query(\n",
    "        base_query=row.query,\n",
    "        exclusions=GLOBAL_EXCLUSIONS\n",
    "    )\n",
    "\n",
    "    params = {\n",
    "        \"q\": full_query,\n",
    "        \"from\": newsapi_from,\n",
    "        \"to\": newsapi_to,\n",
    "        \"language\": DEFAULT_LANGUAGE,\n",
    "        \"sortBy\": DEFAULT_SORT_BY,\n",
    "        \"pageSize\": DEFAULT_PAGE_SIZE,\n",
    "        # \"page\" will be added dynamically in the fetch loop\n",
    "    }\n",
    "\n",
    "    query_params_list.append({\n",
    "        \"keyword_id\": row.keyword_id,\n",
    "        \"canonical\": row.canonical,\n",
    "        \"category\": row.category,\n",
    "        \"params\": params\n",
    "    })\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# Sanity check / preview\n",
    "# ------------------------------------------------------------------\n",
    "\n",
    "print(f\"Built query parameters for {len(query_params_list)} keywords.\\n\")\n",
    "\n",
    "for item in query_params_list:\n",
    "    print(f\"[{item['canonical']}]\")\n",
    "    print(f\"Query : {item['params']['q']}\")\n",
    "    print(f\"From  : {item['params']['from']}\")\n",
    "    print(f\"To    : {item['params']['to']}\")\n",
    "    print(\"-\" * 80)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "079eef2a-61e7-4b39-8c5e-70eada576e40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting fetch for 15 keywords...\n",
      "Target week: 2025-W52 (2025-12-22T00:00:00Z -> 2025-12-28T23:59:59Z)\n",
      "\n",
      "[1/15] Fetching: Generative AI (AI)\n",
      "  totalResults=387 (capped to 100 for Developer plan) -> pages=1\n",
      "\n",
      "[2/15] Fetching: AI Agents (AI)\n",
      "  totalResults=245 (capped to 100 for Developer plan) -> pages=1\n",
      "\n",
      "[3/15] Fetching: Robotics (AI)\n",
      "  totalResults=275 (capped to 100 for Developer plan) -> pages=1\n",
      "\n",
      "[4/15] Fetching: Edge AI (AI)\n",
      "\n",
      "[5/15] Fetching: Digital Health (Healthtech)\n",
      "  totalResults=107 (capped to 100 for Developer plan) -> pages=1\n",
      "\n",
      "[6/15] Fetching: AI in Healthcare (Healthtech)\n",
      "\n",
      "[7/15] Fetching: Precision Medicine (Healthtech)\n",
      "\n",
      "[8/15] Fetching: Clinical AI (Healthtech)\n",
      "  totalResults=107 (capped to 100 for Developer plan) -> pages=1\n",
      "\n",
      "[9/15] Fetching: Climate Tech (Climate)\n",
      "\n",
      "[10/15] Fetching: Carbon Capture (Climate)\n",
      "\n",
      "[11/15] Fetching: Energy Storage (Climate)\n",
      "  totalResults=141 (capped to 100 for Developer plan) -> pages=1\n",
      "\n",
      "[12/15] Fetching: Quantum Computing (Deeptech)\n",
      "  totalResults=102 (capped to 100 for Developer plan) -> pages=1\n",
      "\n",
      "[13/15] Fetching: Advanced Materials (Deeptech)\n",
      "\n",
      "[14/15] Fetching: Nuclear Fusion (Deeptech)\n",
      "\n",
      "[15/15] Fetching: SpaceTech (Deeptech)\n",
      "  totalResults=585 (capped to 100 for Developer plan) -> pages=1\n",
      "\n",
      "Fetch completed.\n",
      "Total raw records collected: 1080\n",
      "\n",
      "Request log (tail):\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ts_utc</th>\n",
       "      <th>keyword_id</th>\n",
       "      <th>canonical</th>\n",
       "      <th>category</th>\n",
       "      <th>attempt</th>\n",
       "      <th>elapsed_sec</th>\n",
       "      <th>status_code</th>\n",
       "      <th>page</th>\n",
       "      <th>pageSize</th>\n",
       "      <th>event</th>\n",
       "      <th>totalResults</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2026-01-03T05:12:01.244751+00:00</td>\n",
       "      <td>generative_ai</td>\n",
       "      <td>Generative AI</td>\n",
       "      <td>AI</td>\n",
       "      <td>1</td>\n",
       "      <td>0.470</td>\n",
       "      <td>200</td>\n",
       "      <td>1</td>\n",
       "      <td>100</td>\n",
       "      <td>success</td>\n",
       "      <td>387</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2026-01-03T05:12:01.649324+00:00</td>\n",
       "      <td>ai_agents</td>\n",
       "      <td>AI Agents</td>\n",
       "      <td>AI</td>\n",
       "      <td>1</td>\n",
       "      <td>0.401</td>\n",
       "      <td>200</td>\n",
       "      <td>1</td>\n",
       "      <td>100</td>\n",
       "      <td>success</td>\n",
       "      <td>245</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2026-01-03T05:12:02.060351+00:00</td>\n",
       "      <td>robotics</td>\n",
       "      <td>Robotics</td>\n",
       "      <td>AI</td>\n",
       "      <td>1</td>\n",
       "      <td>0.408</td>\n",
       "      <td>200</td>\n",
       "      <td>1</td>\n",
       "      <td>100</td>\n",
       "      <td>success</td>\n",
       "      <td>275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2026-01-03T05:12:02.403733+00:00</td>\n",
       "      <td>edge_ai</td>\n",
       "      <td>Edge AI</td>\n",
       "      <td>AI</td>\n",
       "      <td>1</td>\n",
       "      <td>0.341</td>\n",
       "      <td>200</td>\n",
       "      <td>1</td>\n",
       "      <td>100</td>\n",
       "      <td>success</td>\n",
       "      <td>78</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2026-01-03T05:12:03.983435+00:00</td>\n",
       "      <td>digital_health</td>\n",
       "      <td>Digital Health</td>\n",
       "      <td>Healthtech</td>\n",
       "      <td>1</td>\n",
       "      <td>1.579</td>\n",
       "      <td>200</td>\n",
       "      <td>1</td>\n",
       "      <td>100</td>\n",
       "      <td>success</td>\n",
       "      <td>107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2026-01-03T05:12:04.297113+00:00</td>\n",
       "      <td>ai_healthcare</td>\n",
       "      <td>AI in Healthcare</td>\n",
       "      <td>Healthtech</td>\n",
       "      <td>1</td>\n",
       "      <td>0.310</td>\n",
       "      <td>200</td>\n",
       "      <td>1</td>\n",
       "      <td>100</td>\n",
       "      <td>success</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2026-01-03T05:12:04.599098+00:00</td>\n",
       "      <td>precision_medicine</td>\n",
       "      <td>Precision Medicine</td>\n",
       "      <td>Healthtech</td>\n",
       "      <td>1</td>\n",
       "      <td>0.300</td>\n",
       "      <td>200</td>\n",
       "      <td>1</td>\n",
       "      <td>100</td>\n",
       "      <td>success</td>\n",
       "      <td>34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2026-01-03T05:12:04.997018+00:00</td>\n",
       "      <td>clinical_ai</td>\n",
       "      <td>Clinical AI</td>\n",
       "      <td>Healthtech</td>\n",
       "      <td>1</td>\n",
       "      <td>0.396</td>\n",
       "      <td>200</td>\n",
       "      <td>1</td>\n",
       "      <td>100</td>\n",
       "      <td>success</td>\n",
       "      <td>107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2026-01-03T05:12:05.335922+00:00</td>\n",
       "      <td>climate_tech</td>\n",
       "      <td>Climate Tech</td>\n",
       "      <td>Climate</td>\n",
       "      <td>1</td>\n",
       "      <td>0.335</td>\n",
       "      <td>200</td>\n",
       "      <td>1</td>\n",
       "      <td>100</td>\n",
       "      <td>success</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2026-01-03T05:12:05.647940+00:00</td>\n",
       "      <td>carbon_capture</td>\n",
       "      <td>Carbon Capture</td>\n",
       "      <td>Climate</td>\n",
       "      <td>1</td>\n",
       "      <td>0.311</td>\n",
       "      <td>200</td>\n",
       "      <td>1</td>\n",
       "      <td>100</td>\n",
       "      <td>success</td>\n",
       "      <td>44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2026-01-03T05:12:06.060158+00:00</td>\n",
       "      <td>energy_storage</td>\n",
       "      <td>Energy Storage</td>\n",
       "      <td>Climate</td>\n",
       "      <td>1</td>\n",
       "      <td>0.410</td>\n",
       "      <td>200</td>\n",
       "      <td>1</td>\n",
       "      <td>100</td>\n",
       "      <td>success</td>\n",
       "      <td>141</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2026-01-03T05:12:06.496429+00:00</td>\n",
       "      <td>quantum_computing</td>\n",
       "      <td>Quantum Computing</td>\n",
       "      <td>Deeptech</td>\n",
       "      <td>1</td>\n",
       "      <td>0.435</td>\n",
       "      <td>200</td>\n",
       "      <td>1</td>\n",
       "      <td>100</td>\n",
       "      <td>success</td>\n",
       "      <td>102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2026-01-03T05:12:06.876772+00:00</td>\n",
       "      <td>advanced_materials</td>\n",
       "      <td>Advanced Materials</td>\n",
       "      <td>Deeptech</td>\n",
       "      <td>1</td>\n",
       "      <td>0.377</td>\n",
       "      <td>200</td>\n",
       "      <td>1</td>\n",
       "      <td>100</td>\n",
       "      <td>success</td>\n",
       "      <td>76</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>2026-01-03T05:12:07.180958+00:00</td>\n",
       "      <td>nuclear_fusion</td>\n",
       "      <td>Nuclear Fusion</td>\n",
       "      <td>Deeptech</td>\n",
       "      <td>1</td>\n",
       "      <td>0.302</td>\n",
       "      <td>200</td>\n",
       "      <td>1</td>\n",
       "      <td>100</td>\n",
       "      <td>success</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>2026-01-03T05:12:07.558227+00:00</td>\n",
       "      <td>spacetech</td>\n",
       "      <td>SpaceTech</td>\n",
       "      <td>Deeptech</td>\n",
       "      <td>1</td>\n",
       "      <td>0.376</td>\n",
       "      <td>200</td>\n",
       "      <td>1</td>\n",
       "      <td>100</td>\n",
       "      <td>success</td>\n",
       "      <td>585</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                              ts_utc          keyword_id           canonical  \\\n",
       "0   2026-01-03T05:12:01.244751+00:00       generative_ai       Generative AI   \n",
       "1   2026-01-03T05:12:01.649324+00:00           ai_agents           AI Agents   \n",
       "2   2026-01-03T05:12:02.060351+00:00            robotics            Robotics   \n",
       "3   2026-01-03T05:12:02.403733+00:00             edge_ai             Edge AI   \n",
       "4   2026-01-03T05:12:03.983435+00:00      digital_health      Digital Health   \n",
       "5   2026-01-03T05:12:04.297113+00:00       ai_healthcare    AI in Healthcare   \n",
       "6   2026-01-03T05:12:04.599098+00:00  precision_medicine  Precision Medicine   \n",
       "7   2026-01-03T05:12:04.997018+00:00         clinical_ai         Clinical AI   \n",
       "8   2026-01-03T05:12:05.335922+00:00        climate_tech        Climate Tech   \n",
       "9   2026-01-03T05:12:05.647940+00:00      carbon_capture      Carbon Capture   \n",
       "10  2026-01-03T05:12:06.060158+00:00      energy_storage      Energy Storage   \n",
       "11  2026-01-03T05:12:06.496429+00:00   quantum_computing   Quantum Computing   \n",
       "12  2026-01-03T05:12:06.876772+00:00  advanced_materials  Advanced Materials   \n",
       "13  2026-01-03T05:12:07.180958+00:00      nuclear_fusion      Nuclear Fusion   \n",
       "14  2026-01-03T05:12:07.558227+00:00           spacetech           SpaceTech   \n",
       "\n",
       "      category  attempt  elapsed_sec  status_code  page  pageSize    event  \\\n",
       "0           AI        1        0.470          200     1       100  success   \n",
       "1           AI        1        0.401          200     1       100  success   \n",
       "2           AI        1        0.408          200     1       100  success   \n",
       "3           AI        1        0.341          200     1       100  success   \n",
       "4   Healthtech        1        1.579          200     1       100  success   \n",
       "5   Healthtech        1        0.310          200     1       100  success   \n",
       "6   Healthtech        1        0.300          200     1       100  success   \n",
       "7   Healthtech        1        0.396          200     1       100  success   \n",
       "8      Climate        1        0.335          200     1       100  success   \n",
       "9      Climate        1        0.311          200     1       100  success   \n",
       "10     Climate        1        0.410          200     1       100  success   \n",
       "11    Deeptech        1        0.435          200     1       100  success   \n",
       "12    Deeptech        1        0.377          200     1       100  success   \n",
       "13    Deeptech        1        0.302          200     1       100  success   \n",
       "14    Deeptech        1        0.376          200     1       100  success   \n",
       "\n",
       "    totalResults  \n",
       "0            387  \n",
       "1            245  \n",
       "2            275  \n",
       "3             78  \n",
       "4            107  \n",
       "5             22  \n",
       "6             34  \n",
       "7            107  \n",
       "8             21  \n",
       "9             44  \n",
       "10           141  \n",
       "11           102  \n",
       "12            76  \n",
       "13            22  \n",
       "14           585  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ============================================================\n",
    "# Cell 5: Fetch Loop (Pagination + Retry)\n",
    "# ============================================================\n",
    "\n",
    "# This cell executes the weekly fetch against NewsAPI for each keyword.\n",
    "# Key behaviors:\n",
    "# - Pagination: iterate page=1..N where N is derived from totalResults\n",
    "# - Retry: retry transient failures (timeouts, 5xx) with backoff\n",
    "# - Rate-limit handling: if 429 is returned, sleep and retry\n",
    "#\n",
    "# Output:\n",
    "# - raw_records: list[dict] of article records enriched with keyword metadata\n",
    "# - fetch_log_df: per-request log for debugging and auditing\n",
    "\n",
    "# Developer plan cap: NewsAPI \"everything\" supports only up to 100 results in total.\n",
    "# If you are on a paid plan, set this to None.\n",
    "DEVELOPER_MAX_RESULTS = 100\n",
    "\n",
    "raw_records: List[Dict[str, Any]] = []\n",
    "request_logs: List[Dict[str, Any]] = []\n",
    "\n",
    "\n",
    "def _request_with_retry(\n",
    "    endpoint: str,\n",
    "    params: Dict[str, Any],\n",
    "    headers: Dict[str, str],\n",
    "    keyword_id: str,\n",
    "    canonical: str,\n",
    "    category: str,\n",
    ") -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Make a single NewsAPI request with retry logic.\n",
    "    Returns parsed JSON on success; raises RuntimeError on repeated failure.\n",
    "    \"\"\"\n",
    "    last_err = None\n",
    "\n",
    "    for attempt in range(1, MAX_RETRIES + 1):\n",
    "        t0 = time.time()\n",
    "        try:\n",
    "            resp = requests.get(\n",
    "                endpoint,\n",
    "                params=params,\n",
    "                headers=headers,\n",
    "                timeout=REQUEST_TIMEOUT_SEC,\n",
    "            )\n",
    "            elapsed = time.time() - t0\n",
    "\n",
    "            log_base = {\n",
    "                \"ts_utc\": datetime.now(timezone.utc).isoformat(),\n",
    "                \"keyword_id\": keyword_id,\n",
    "                \"canonical\": canonical,\n",
    "                \"category\": category,\n",
    "                \"attempt\": attempt,\n",
    "                \"elapsed_sec\": round(elapsed, 3),\n",
    "                \"status_code\": resp.status_code,\n",
    "                \"page\": params.get(\"page\"),\n",
    "                \"pageSize\": params.get(\"pageSize\"),\n",
    "            }\n",
    "\n",
    "            # Rate limit\n",
    "            if resp.status_code == 429:\n",
    "                request_logs.append({**log_base, \"event\": \"rate_limited\"})\n",
    "                time.sleep(RETRY_SLEEP_SEC * attempt)\n",
    "                continue\n",
    "\n",
    "            # Other non-200\n",
    "            if resp.status_code != 200:\n",
    "                request_logs.append(\n",
    "                    {**log_base, \"event\": \"http_error\", \"error_text\": resp.text[:500]}\n",
    "                )\n",
    "                # Retry on 5xx\n",
    "                if 500 <= resp.status_code < 600:\n",
    "                    time.sleep(RETRY_SLEEP_SEC * attempt)\n",
    "                    continue\n",
    "                # For 4xx (except 429), do not keep retrying\n",
    "                raise RuntimeError(f\"HTTP {resp.status_code}: {resp.text[:500]}\")\n",
    "\n",
    "            data = resp.json()\n",
    "            request_logs.append({**log_base, \"event\": \"success\", \"totalResults\": data.get(\"totalResults\")})\n",
    "            return data\n",
    "\n",
    "        except (requests.Timeout, requests.ConnectionError) as e:\n",
    "            elapsed = time.time() - t0\n",
    "            last_err = e\n",
    "            request_logs.append({\n",
    "                \"ts_utc\": datetime.now(timezone.utc).isoformat(),\n",
    "                \"keyword_id\": keyword_id,\n",
    "                \"canonical\": canonical,\n",
    "                \"category\": category,\n",
    "                \"attempt\": attempt,\n",
    "                \"elapsed_sec\": round(elapsed, 3),\n",
    "                \"status_code\": None,\n",
    "                \"page\": params.get(\"page\"),\n",
    "                \"pageSize\": params.get(\"pageSize\"),\n",
    "                \"event\": \"network_error\",\n",
    "                \"error_text\": repr(e)[:500],\n",
    "            })\n",
    "            time.sleep(RETRY_SLEEP_SEC * attempt)\n",
    "            continue\n",
    "\n",
    "    raise RuntimeError(\n",
    "        f\"Failed after {MAX_RETRIES} attempts for keyword={canonical} \"\n",
    "        f\"(last_err={repr(last_err)})\"\n",
    "    )\n",
    "\n",
    "\n",
    "headers = {\"X-Api-Key\": NEWSAPI_KEY}\n",
    "\n",
    "print(f\"Starting fetch for {len(query_params_list)} keywords...\")\n",
    "print(f\"Target week: {week_label} ({newsapi_from} -> {newsapi_to})\")\n",
    "\n",
    "for idx, item in enumerate(query_params_list, start=1):\n",
    "    keyword_id = item[\"keyword_id\"]\n",
    "    canonical = item[\"canonical\"]\n",
    "    category = item[\"category\"]\n",
    "\n",
    "    base_params = dict(item[\"params\"])  # copy\n",
    "    base_params[\"page\"] = 1\n",
    "\n",
    "    print(f\"\\n[{idx}/{len(query_params_list)}] Fetching: {canonical} ({category})\")\n",
    "\n",
    "    # --- First page (to get totalResults) ---\n",
    "    data = _request_with_retry(\n",
    "        endpoint=NEWSAPI_ENDPOINT,\n",
    "        params=base_params,\n",
    "        headers=headers,\n",
    "        keyword_id=keyword_id,\n",
    "        canonical=canonical,\n",
    "        category=category,\n",
    "    )\n",
    "\n",
    "    total_results = int(data.get(\"totalResults\", 0) or 0)\n",
    "    articles = data.get(\"articles\", []) or []\n",
    "\n",
    "    # Enrich and store\n",
    "    for a in articles:\n",
    "        raw_records.append({\n",
    "            \"keyword_id\": keyword_id,\n",
    "            \"canonical\": canonical,\n",
    "            \"category\": category,\n",
    "            \"week_label\": week_label,\n",
    "            \"fetched_at_utc\": datetime.now(timezone.utc).isoformat(),\n",
    "            \"query\": base_params.get(\"q\"),\n",
    "            **a\n",
    "        })\n",
    "\n",
    "    # --- Pagination ---\n",
    "    # NewsAPI returns up to pageSize results per page.\n",
    "    # total_pages = ceil(total_results / pageSize)\n",
    "    page_size = int(base_params[\"pageSize\"])\n",
    "\n",
    "    # NewsAPI Developer accounts are capped at 100 total results for /v2/everything.\n",
    "    # So even if totalResults is larger, we cannot fetch beyond page=1.\n",
    "    effective_total = total_results\n",
    "    capped = False\n",
    "    \n",
    "    if DEVELOPER_MAX_RESULTS is not None and total_results > DEVELOPER_MAX_RESULTS:\n",
    "        effective_total = DEVELOPER_MAX_RESULTS\n",
    "        capped = True\n",
    "    \n",
    "    total_pages = (effective_total + page_size - 1) // page_size\n",
    "    total_pages = max(total_pages, 1)\n",
    "    \n",
    "    if capped:\n",
    "        print(f\"  totalResults={total_results} (capped to {DEVELOPER_MAX_RESULTS} for Developer plan) -> pages={total_pages}\")\n",
    "    else:\n",
    "        if total_pages > 1:\n",
    "            print(f\"  totalResults={total_results} -> pages={total_pages}\")\n",
    "\n",
    "\n",
    "print(\"\\nFetch completed.\")\n",
    "print(f\"Total raw records collected: {len(raw_records)}\")\n",
    "\n",
    "fetch_log_df = pd.DataFrame(request_logs)\n",
    "print(\"\\nRequest log (tail):\")\n",
    "display(fetch_log_df.tail(20))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "22908e29-551c-4fa5-b056-07a1cd14db0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalization completed.\n",
      "Normalized rows: 1,080\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>week_label</th>\n",
       "      <th>keyword_id</th>\n",
       "      <th>canonical</th>\n",
       "      <th>category</th>\n",
       "      <th>fetched_at_utc</th>\n",
       "      <th>publishedAt</th>\n",
       "      <th>source_id</th>\n",
       "      <th>source_name</th>\n",
       "      <th>author</th>\n",
       "      <th>title</th>\n",
       "      <th>description</th>\n",
       "      <th>url</th>\n",
       "      <th>url_key</th>\n",
       "      <th>urlToImage</th>\n",
       "      <th>content</th>\n",
       "      <th>query</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2025-W52</td>\n",
       "      <td>generative_ai</td>\n",
       "      <td>Generative AI</td>\n",
       "      <td>AI</td>\n",
       "      <td>2026-01-03T05:12:01.247325+00:00</td>\n",
       "      <td>2025-12-28T23:20:06Z</td>\n",
       "      <td>None</td>\n",
       "      <td>pymnts.com</td>\n",
       "      <td>PYMNTS</td>\n",
       "      <td>Hotels Swap Online Travel Agents for AI Agents</td>\n",
       "      <td>Consumers are growing comfortable with letting...</td>\n",
       "      <td>https://www.pymnts.com/travel-payments/2025/ho...</td>\n",
       "      <td>https://www.pymnts.com/travel-payments/2025/ho...</td>\n",
       "      <td>https://www.pymnts.com/wp-content/uploads/2025...</td>\n",
       "      <td>Consumers are growing comfortable with letting...</td>\n",
       "      <td>(\"generative AI\" OR \"GenAI\") AND NOT (job OR j...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2025-W52</td>\n",
       "      <td>generative_ai</td>\n",
       "      <td>Generative AI</td>\n",
       "      <td>AI</td>\n",
       "      <td>2026-01-03T05:12:01.247350+00:00</td>\n",
       "      <td>2025-12-28T23:01:00Z</td>\n",
       "      <td>None</td>\n",
       "      <td>Diepresse.com</td>\n",
       "      <td>By Helmut Reisinger</td>\n",
       "      <td>Why Geopolitics and AI Are Now Core to Busines...</td>\n",
       "      <td>The digital landscape has become a central are...</td>\n",
       "      <td>https://www.diepresse.com/20407798/why-geopoli...</td>\n",
       "      <td>https://www.diepresse.com/20407798/why-geopoli...</td>\n",
       "      <td>https://img.diepresse.com/public/incoming/arhh...</td>\n",
       "      <td>The digital landscape has become a central are...</td>\n",
       "      <td>(\"generative AI\" OR \"GenAI\") AND NOT (job OR j...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2025-W52</td>\n",
       "      <td>generative_ai</td>\n",
       "      <td>Generative AI</td>\n",
       "      <td>AI</td>\n",
       "      <td>2026-01-03T05:12:01.247357+00:00</td>\n",
       "      <td>2025-12-28T22:00:00Z</td>\n",
       "      <td>None</td>\n",
       "      <td>OilPrice.com</td>\n",
       "      <td>Michael Kern</td>\n",
       "      <td>The AI Arms Race Is Cracking Open the Nuclear ...</td>\n",
       "      <td>We are seeing a violent collision between two ...</td>\n",
       "      <td>https://oilprice.com/Alternative-Energy/Nuclea...</td>\n",
       "      <td>https://oilprice.com/Alternative-Energy/Nuclea...</td>\n",
       "      <td>https://d32r1sh890xpii.cloudfront.net/article/...</td>\n",
       "      <td>Trump Media is entering the…\\r\\nThe UK's artif...</td>\n",
       "      <td>(\"generative AI\" OR \"GenAI\") AND NOT (job OR j...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2025-W52</td>\n",
       "      <td>generative_ai</td>\n",
       "      <td>Generative AI</td>\n",
       "      <td>AI</td>\n",
       "      <td>2026-01-03T05:12:01.247363+00:00</td>\n",
       "      <td>2025-12-28T21:31:47Z</td>\n",
       "      <td>None</td>\n",
       "      <td>How-To Geek</td>\n",
       "      <td>Sydney Butler</td>\n",
       "      <td>If Blender is so good, why isn’t Hollywood usi...</td>\n",
       "      <td>If Blender is so good, why isn’t Hollywood usi...</td>\n",
       "      <td>https://www.howtogeek.com/if-blender-is-so-goo...</td>\n",
       "      <td>https://www.howtogeek.com/if-blender-is-so-goo...</td>\n",
       "      <td>https://static0.howtogeekimages.com/wordpress/...</td>\n",
       "      <td>The movie Flow was made on a small budget, wit...</td>\n",
       "      <td>(\"generative AI\" OR \"GenAI\") AND NOT (job OR j...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2025-W52</td>\n",
       "      <td>generative_ai</td>\n",
       "      <td>Generative AI</td>\n",
       "      <td>AI</td>\n",
       "      <td>2026-01-03T05:12:01.247369+00:00</td>\n",
       "      <td>2025-12-28T21:00:00Z</td>\n",
       "      <td>None</td>\n",
       "      <td>Slashdot.org</td>\n",
       "      <td>EditorDavid</td>\n",
       "      <td>Did Tim Cook Post AI Slop in His Christmas Mes...</td>\n",
       "      <td>Artist Keith Thomson is a modern (and whimsica...</td>\n",
       "      <td>https://apple.slashdot.org/story/25/12/28/2048...</td>\n",
       "      <td>https://apple.slashdot.org/story/25/12/28/2048...</td>\n",
       "      <td>https://a.fsdn.com/sd/topics/ai_64.png</td>\n",
       "      <td>Artist Keith Thomson is a modern (and whimsica...</td>\n",
       "      <td>(\"generative AI\" OR \"GenAI\") AND NOT (job OR j...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2025-W52</td>\n",
       "      <td>generative_ai</td>\n",
       "      <td>Generative AI</td>\n",
       "      <td>AI</td>\n",
       "      <td>2026-01-03T05:12:01.247395+00:00</td>\n",
       "      <td>2025-12-28T19:28:39Z</td>\n",
       "      <td>None</td>\n",
       "      <td>Search Engine Journal</td>\n",
       "      <td>Roger Montti</td>\n",
       "      <td>Ahrefs Tested AI Misinformation, But Proved So...</td>\n",
       "      <td>Ahrefs research showed that biased inputs can ...</td>\n",
       "      <td>https://www.searchenginejournal.com/ahrefs-tes...</td>\n",
       "      <td>https://www.searchenginejournal.com/ahrefs-tes...</td>\n",
       "      <td>https://cdn.searchenginejournal.com/wp-content...</td>\n",
       "      <td>Ahrefs tested how AI systems behave when they’...</td>\n",
       "      <td>(\"generative AI\" OR \"GenAI\") AND NOT (job OR j...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2025-W52</td>\n",
       "      <td>generative_ai</td>\n",
       "      <td>Generative AI</td>\n",
       "      <td>AI</td>\n",
       "      <td>2026-01-03T05:12:01.247402+00:00</td>\n",
       "      <td>2025-12-28T17:17:24Z</td>\n",
       "      <td>None</td>\n",
       "      <td>Livemint</td>\n",
       "      <td>Kanishka Singharia</td>\n",
       "      <td>How ChatGPT became turning point for ex-Google...</td>\n",
       "      <td>Ex-Googlers closed a $2M a year startup to foc...</td>\n",
       "      <td>https://www.livemint.com/companies/people/how-...</td>\n",
       "      <td>https://www.livemint.com/companies/people/how-...</td>\n",
       "      <td>https://www.livemint.com/lm-img/img/2025/12/28...</td>\n",
       "      <td>Former Google employees, both aged 33, took a ...</td>\n",
       "      <td>(\"generative AI\" OR \"GenAI\") AND NOT (job OR j...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2025-W52</td>\n",
       "      <td>generative_ai</td>\n",
       "      <td>Generative AI</td>\n",
       "      <td>AI</td>\n",
       "      <td>2026-01-03T05:12:01.247414+00:00</td>\n",
       "      <td>2025-12-28T17:15:54Z</td>\n",
       "      <td>None</td>\n",
       "      <td>IPWatchdog.com</td>\n",
       "      <td>Eileen McDermott</td>\n",
       "      <td>A Year of Change, Transition and ‘Recalibratio...</td>\n",
       "      <td>Each year IPWatchdog surveys the IP community ...</td>\n",
       "      <td>https://ipwatchdog.com/2025/12/28/year-change-...</td>\n",
       "      <td>https://ipwatchdog.com/2025/12/28/year-change-...</td>\n",
       "      <td>https://ipwatchdog.com/wp-content/uploads/2025...</td>\n",
       "      <td>Taken together, 2025 was a year of recalibrati...</td>\n",
       "      <td>(\"generative AI\" OR \"GenAI\") AND NOT (job OR j...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2025-W52</td>\n",
       "      <td>generative_ai</td>\n",
       "      <td>Generative AI</td>\n",
       "      <td>AI</td>\n",
       "      <td>2026-01-03T05:12:01.247421+00:00</td>\n",
       "      <td>2025-12-28T17:05:39Z</td>\n",
       "      <td>None</td>\n",
       "      <td>GamesRadar+</td>\n",
       "      <td>Kaan Serin</td>\n",
       "      <td>Witchfire CEO says Divinity devs are \"definite...</td>\n",
       "      <td>Larian just got \"got a little bit unlucky\"</td>\n",
       "      <td>https://www.gamesradar.com/games/rpg/witchfire...</td>\n",
       "      <td>https://www.gamesradar.com/games/rpg/witchfire...</td>\n",
       "      <td>https://cdn.mos.cms.futurecdn.net/FPheaHQoGz5G...</td>\n",
       "      <td>The Astronauts CEO Adrian Chmielarz - develope...</td>\n",
       "      <td>(\"generative AI\" OR \"GenAI\") AND NOT (job OR j...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2025-W52</td>\n",
       "      <td>generative_ai</td>\n",
       "      <td>Generative AI</td>\n",
       "      <td>AI</td>\n",
       "      <td>2026-01-03T05:12:01.247428+00:00</td>\n",
       "      <td>2025-12-28T15:35:32Z</td>\n",
       "      <td>None</td>\n",
       "      <td>Tiktok.com</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>Small Business Marketing: A Complete Beginner'...</td>\n",
       "      <td>Marketing a business in 2025 requires an onlin...</td>\n",
       "      <td>https://ads.tiktok.com/business/en-US/guides/s...</td>\n",
       "      <td>https://ads.tiktok.com/business/en-US/guides/s...</td>\n",
       "      <td>https://sf16-website-login.neutral.ttwstatic.c...</td>\n",
       "      <td>Marketing a business in 2025 requires an onlin...</td>\n",
       "      <td>(\"generative AI\" OR \"GenAI\") AND NOT (job OR j...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  week_label     keyword_id      canonical category  \\\n",
       "0   2025-W52  generative_ai  Generative AI       AI   \n",
       "1   2025-W52  generative_ai  Generative AI       AI   \n",
       "2   2025-W52  generative_ai  Generative AI       AI   \n",
       "3   2025-W52  generative_ai  Generative AI       AI   \n",
       "4   2025-W52  generative_ai  Generative AI       AI   \n",
       "5   2025-W52  generative_ai  Generative AI       AI   \n",
       "6   2025-W52  generative_ai  Generative AI       AI   \n",
       "7   2025-W52  generative_ai  Generative AI       AI   \n",
       "8   2025-W52  generative_ai  Generative AI       AI   \n",
       "9   2025-W52  generative_ai  Generative AI       AI   \n",
       "\n",
       "                     fetched_at_utc           publishedAt source_id  \\\n",
       "0  2026-01-03T05:12:01.247325+00:00  2025-12-28T23:20:06Z      None   \n",
       "1  2026-01-03T05:12:01.247350+00:00  2025-12-28T23:01:00Z      None   \n",
       "2  2026-01-03T05:12:01.247357+00:00  2025-12-28T22:00:00Z      None   \n",
       "3  2026-01-03T05:12:01.247363+00:00  2025-12-28T21:31:47Z      None   \n",
       "4  2026-01-03T05:12:01.247369+00:00  2025-12-28T21:00:00Z      None   \n",
       "5  2026-01-03T05:12:01.247395+00:00  2025-12-28T19:28:39Z      None   \n",
       "6  2026-01-03T05:12:01.247402+00:00  2025-12-28T17:17:24Z      None   \n",
       "7  2026-01-03T05:12:01.247414+00:00  2025-12-28T17:15:54Z      None   \n",
       "8  2026-01-03T05:12:01.247421+00:00  2025-12-28T17:05:39Z      None   \n",
       "9  2026-01-03T05:12:01.247428+00:00  2025-12-28T15:35:32Z      None   \n",
       "\n",
       "             source_name               author  \\\n",
       "0             pymnts.com               PYMNTS   \n",
       "1          Diepresse.com  By Helmut Reisinger   \n",
       "2           OilPrice.com         Michael Kern   \n",
       "3            How-To Geek        Sydney Butler   \n",
       "4           Slashdot.org          EditorDavid   \n",
       "5  Search Engine Journal         Roger Montti   \n",
       "6               Livemint   Kanishka Singharia   \n",
       "7         IPWatchdog.com     Eileen McDermott   \n",
       "8            GamesRadar+           Kaan Serin   \n",
       "9             Tiktok.com                 <NA>   \n",
       "\n",
       "                                               title  \\\n",
       "0     Hotels Swap Online Travel Agents for AI Agents   \n",
       "1  Why Geopolitics and AI Are Now Core to Busines...   \n",
       "2  The AI Arms Race Is Cracking Open the Nuclear ...   \n",
       "3  If Blender is so good, why isn’t Hollywood usi...   \n",
       "4  Did Tim Cook Post AI Slop in His Christmas Mes...   \n",
       "5  Ahrefs Tested AI Misinformation, But Proved So...   \n",
       "6  How ChatGPT became turning point for ex-Google...   \n",
       "7  A Year of Change, Transition and ‘Recalibratio...   \n",
       "8  Witchfire CEO says Divinity devs are \"definite...   \n",
       "9  Small Business Marketing: A Complete Beginner'...   \n",
       "\n",
       "                                         description  \\\n",
       "0  Consumers are growing comfortable with letting...   \n",
       "1  The digital landscape has become a central are...   \n",
       "2  We are seeing a violent collision between two ...   \n",
       "3  If Blender is so good, why isn’t Hollywood usi...   \n",
       "4  Artist Keith Thomson is a modern (and whimsica...   \n",
       "5  Ahrefs research showed that biased inputs can ...   \n",
       "6  Ex-Googlers closed a $2M a year startup to foc...   \n",
       "7  Each year IPWatchdog surveys the IP community ...   \n",
       "8         Larian just got \"got a little bit unlucky\"   \n",
       "9  Marketing a business in 2025 requires an onlin...   \n",
       "\n",
       "                                                 url  \\\n",
       "0  https://www.pymnts.com/travel-payments/2025/ho...   \n",
       "1  https://www.diepresse.com/20407798/why-geopoli...   \n",
       "2  https://oilprice.com/Alternative-Energy/Nuclea...   \n",
       "3  https://www.howtogeek.com/if-blender-is-so-goo...   \n",
       "4  https://apple.slashdot.org/story/25/12/28/2048...   \n",
       "5  https://www.searchenginejournal.com/ahrefs-tes...   \n",
       "6  https://www.livemint.com/companies/people/how-...   \n",
       "7  https://ipwatchdog.com/2025/12/28/year-change-...   \n",
       "8  https://www.gamesradar.com/games/rpg/witchfire...   \n",
       "9  https://ads.tiktok.com/business/en-US/guides/s...   \n",
       "\n",
       "                                             url_key  \\\n",
       "0  https://www.pymnts.com/travel-payments/2025/ho...   \n",
       "1  https://www.diepresse.com/20407798/why-geopoli...   \n",
       "2  https://oilprice.com/Alternative-Energy/Nuclea...   \n",
       "3  https://www.howtogeek.com/if-blender-is-so-goo...   \n",
       "4  https://apple.slashdot.org/story/25/12/28/2048...   \n",
       "5  https://www.searchenginejournal.com/ahrefs-tes...   \n",
       "6  https://www.livemint.com/companies/people/how-...   \n",
       "7  https://ipwatchdog.com/2025/12/28/year-change-...   \n",
       "8  https://www.gamesradar.com/games/rpg/witchfire...   \n",
       "9  https://ads.tiktok.com/business/en-US/guides/s...   \n",
       "\n",
       "                                          urlToImage  \\\n",
       "0  https://www.pymnts.com/wp-content/uploads/2025...   \n",
       "1  https://img.diepresse.com/public/incoming/arhh...   \n",
       "2  https://d32r1sh890xpii.cloudfront.net/article/...   \n",
       "3  https://static0.howtogeekimages.com/wordpress/...   \n",
       "4             https://a.fsdn.com/sd/topics/ai_64.png   \n",
       "5  https://cdn.searchenginejournal.com/wp-content...   \n",
       "6  https://www.livemint.com/lm-img/img/2025/12/28...   \n",
       "7  https://ipwatchdog.com/wp-content/uploads/2025...   \n",
       "8  https://cdn.mos.cms.futurecdn.net/FPheaHQoGz5G...   \n",
       "9  https://sf16-website-login.neutral.ttwstatic.c...   \n",
       "\n",
       "                                             content  \\\n",
       "0  Consumers are growing comfortable with letting...   \n",
       "1  The digital landscape has become a central are...   \n",
       "2  Trump Media is entering the…\\r\\nThe UK's artif...   \n",
       "3  The movie Flow was made on a small budget, wit...   \n",
       "4  Artist Keith Thomson is a modern (and whimsica...   \n",
       "5  Ahrefs tested how AI systems behave when they’...   \n",
       "6  Former Google employees, both aged 33, took a ...   \n",
       "7  Taken together, 2025 was a year of recalibrati...   \n",
       "8  The Astronauts CEO Adrian Chmielarz - develope...   \n",
       "9  Marketing a business in 2025 requires an onlin...   \n",
       "\n",
       "                                               query  \n",
       "0  (\"generative AI\" OR \"GenAI\") AND NOT (job OR j...  \n",
       "1  (\"generative AI\" OR \"GenAI\") AND NOT (job OR j...  \n",
       "2  (\"generative AI\" OR \"GenAI\") AND NOT (job OR j...  \n",
       "3  (\"generative AI\" OR \"GenAI\") AND NOT (job OR j...  \n",
       "4  (\"generative AI\" OR \"GenAI\") AND NOT (job OR j...  \n",
       "5  (\"generative AI\" OR \"GenAI\") AND NOT (job OR j...  \n",
       "6  (\"generative AI\" OR \"GenAI\") AND NOT (job OR j...  \n",
       "7  (\"generative AI\" OR \"GenAI\") AND NOT (job OR j...  \n",
       "8  (\"generative AI\" OR \"GenAI\") AND NOT (job OR j...  \n",
       "9  (\"generative AI\" OR \"GenAI\") AND NOT (job OR j...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ============================================================\n",
    "# Cell 6: Normalize Records\n",
    "# ============================================================\n",
    "\n",
    "# This cell converts the raw_records (list of dicts) into a normalized\n",
    "# tabular format suitable for downstream deduplication and storage.\n",
    "#\n",
    "# Notes:\n",
    "# - We keep the normalized schema intentionally \"raw\": minimal transformation,\n",
    "#   maximal traceability to the original NewsAPI fields.\n",
    "# - We also extract a few convenience fields (e.g., source_name) for analytics.\n",
    "# - Timestamps are kept as strings (ISO 8601) to avoid timezone ambiguity;\n",
    "#   downstream notebooks can parse as needed.\n",
    "\n",
    "if not raw_records:\n",
    "    print(\"No records to normalize (raw_records is empty).\")\n",
    "    normalized_df = pd.DataFrame()\n",
    "else:\n",
    "    df = pd.DataFrame(raw_records)\n",
    "\n",
    "    # Ensure expected top-level columns exist (NewsAPI may omit some fields)\n",
    "    expected_cols = [\n",
    "        \"keyword_id\", \"canonical\", \"category\", \"week_label\", \"fetched_at_utc\", \"query\",\n",
    "        \"source\", \"author\", \"title\", \"description\", \"url\", \"urlToImage\", \"publishedAt\", \"content\"\n",
    "    ]\n",
    "    for c in expected_cols:\n",
    "        if c not in df.columns:\n",
    "            df[c] = None\n",
    "\n",
    "    # Flatten nested \"source\" dict -> (source_id, source_name)\n",
    "    def _safe_source_id(x):\n",
    "        if isinstance(x, dict):\n",
    "            return x.get(\"id\")\n",
    "        return None\n",
    "\n",
    "    def _safe_source_name(x):\n",
    "        if isinstance(x, dict):\n",
    "            return x.get(\"name\")\n",
    "        return None\n",
    "\n",
    "    df[\"source_id\"] = df[\"source\"].apply(_safe_source_id)\n",
    "    df[\"source_name\"] = df[\"source\"].apply(_safe_source_name)\n",
    "\n",
    "    # Normalize text fields (strip whitespace)\n",
    "    text_cols = [\"author\", \"title\", \"description\", \"url\", \"urlToImage\", \"publishedAt\", \"content\", \"source_name\"]\n",
    "    for c in text_cols:\n",
    "        df[c] = df[c].astype(\"string\").str.strip()\n",
    "\n",
    "    # Create a stable URL key for dedup (light normalization)\n",
    "    # (Optional) remove common tracking params later if needed\n",
    "    df[\"url_key\"] = df[\"url\"]\n",
    "\n",
    "    # Minimal typed conversions\n",
    "    # Keep publishedAt as string for now; parse downstream if needed.\n",
    "    # df[\"published_at_ts\"] = pd.to_datetime(df[\"publishedAt\"], utc=True, errors=\"coerce\")\n",
    "\n",
    "    # Select and reorder output columns\n",
    "    normalized_cols = [\n",
    "        \"week_label\",\n",
    "        \"keyword_id\", \"canonical\", \"category\",\n",
    "        \"fetched_at_utc\",\n",
    "        \"publishedAt\",\n",
    "        \"source_id\", \"source_name\",\n",
    "        \"author\",\n",
    "        \"title\", \"description\",\n",
    "        \"url\", \"url_key\", \"urlToImage\",\n",
    "        \"content\",\n",
    "        \"query\",\n",
    "    ]\n",
    "\n",
    "    normalized_df = df[normalized_cols].copy()\n",
    "\n",
    "    print(\"Normalization completed.\")\n",
    "    print(f\"Normalized rows: {len(normalized_df):,}\")\n",
    "    display(normalized_df.head(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a393ed6b-1510-4ef4-9ff6-02a4861c95f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deduplication completed.\n",
      "Rows before dedup: 1,080\n",
      "Rows after  dedup: 1,013\n",
      "Duplicates removed: 67\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>week_label</th>\n",
       "      <th>keyword_id</th>\n",
       "      <th>canonical</th>\n",
       "      <th>category</th>\n",
       "      <th>fetched_at_utc</th>\n",
       "      <th>publishedAt</th>\n",
       "      <th>source_id</th>\n",
       "      <th>source_name</th>\n",
       "      <th>author</th>\n",
       "      <th>title</th>\n",
       "      <th>description</th>\n",
       "      <th>url</th>\n",
       "      <th>url_key</th>\n",
       "      <th>urlToImage</th>\n",
       "      <th>content</th>\n",
       "      <th>query</th>\n",
       "      <th>matched_keyword_ids</th>\n",
       "      <th>matched_canonicals</th>\n",
       "      <th>matched_categories</th>\n",
       "      <th>matched_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2025-W52</td>\n",
       "      <td>carbon_capture</td>\n",
       "      <td>Carbon Capture</td>\n",
       "      <td>Climate</td>\n",
       "      <td>2026-01-03T05:12:05.649879+00:00</td>\n",
       "      <td>2025-12-22T00:00:00Z</td>\n",
       "      <td>None</td>\n",
       "      <td>Nature.com</td>\n",
       "      <td>Hongli Wu, Jialun Xu, Junjie Tai, Jingkun Gao,...</td>\n",
       "      <td>Biomimetic Mn(III) porphyrin-catalyzed aromati...</td>\n",
       "      <td>Selective catalytic aromaticity-breaking epoxi...</td>\n",
       "      <td>https://www.nature.com/articles/s41467-025-673...</td>\n",
       "      <td>https://www.nature.com/articles/s41467-025-673...</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;li&gt;Zhang, L. &amp;amp; Ritter, T. A perspective o...</td>\n",
       "      <td>(\"carbon capture\" OR \"CCS\" OR \"carbon removal\"...</td>\n",
       "      <td>[carbon_capture]</td>\n",
       "      <td>[Carbon Capture]</td>\n",
       "      <td>[Climate]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2025-W52</td>\n",
       "      <td>advanced_materials</td>\n",
       "      <td>Advanced Materials</td>\n",
       "      <td>Deeptech</td>\n",
       "      <td>2026-01-03T05:12:06.878686+00:00</td>\n",
       "      <td>2025-12-22T00:00:00Z</td>\n",
       "      <td>None</td>\n",
       "      <td>Royal Society of Chemistry</td>\n",
       "      <td>Maria A Malandina, Sergei S. Leonchuk, Cheng Z...</td>\n",
       "      <td>\"Flexible energy\": energy harvesting and stora...</td>\n",
       "      <td>J. Mater. Chem. A, 2026, Accepted ManuscriptDO...</td>\n",
       "      <td>https://pubs.rsc.org/en/content/articlelanding...</td>\n",
       "      <td>https://pubs.rsc.org/en/content/articlelanding...</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>The rapid development of flexible electronics ...</td>\n",
       "      <td>(\"advanced materials\" OR \"material science\" OR...</td>\n",
       "      <td>[advanced_materials]</td>\n",
       "      <td>[Advanced Materials]</td>\n",
       "      <td>[Deeptech]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2025-W52</td>\n",
       "      <td>advanced_materials</td>\n",
       "      <td>Advanced Materials</td>\n",
       "      <td>Deeptech</td>\n",
       "      <td>2026-01-03T05:12:06.878670+00:00</td>\n",
       "      <td>2025-12-22T00:00:00Z</td>\n",
       "      <td>None</td>\n",
       "      <td>Royal Society of Chemistry</td>\n",
       "      <td>Akash Balakrishnan, Milan Tom, Natarajan Rajam...</td>\n",
       "      <td>Engineered Cellulose-Supported Photocatalysts ...</td>\n",
       "      <td>J. Mater. Chem. A, 2026, Accepted ManuscriptDO...</td>\n",
       "      <td>https://pubs.rsc.org/en/content/articlelanding...</td>\n",
       "      <td>https://pubs.rsc.org/en/content/articlelanding...</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>The growing global demand for sustainable ener...</td>\n",
       "      <td>(\"advanced materials\" OR \"material science\" OR...</td>\n",
       "      <td>[advanced_materials]</td>\n",
       "      <td>[Advanced Materials]</td>\n",
       "      <td>[Deeptech]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2025-W52</td>\n",
       "      <td>advanced_materials</td>\n",
       "      <td>Advanced Materials</td>\n",
       "      <td>Deeptech</td>\n",
       "      <td>2026-01-03T05:12:06.878675+00:00</td>\n",
       "      <td>2025-12-22T00:00:00Z</td>\n",
       "      <td>None</td>\n",
       "      <td>Royal Society of Chemistry</td>\n",
       "      <td>Mengran Liu, Kexin Liu, Zhuang Yan, Chenfei Ya...</td>\n",
       "      <td>Metal–(organic cocrystal) framework with a pho...</td>\n",
       "      <td>J. Mater. Chem. A, 2026, Advance ArticleDOI: 1...</td>\n",
       "      <td>https://pubs.rsc.org/en/content/articlelanding...</td>\n",
       "      <td>https://pubs.rsc.org/en/content/articlelanding...</td>\n",
       "      <td>https://pubs.rsc.org/en/Content/Image/GA/D5TA0...</td>\n",
       "      <td>* \\r\\n Corresponding authors\\r\\na\\r\\n Key Labo...</td>\n",
       "      <td>(\"advanced materials\" OR \"material science\" OR...</td>\n",
       "      <td>[advanced_materials]</td>\n",
       "      <td>[Advanced Materials]</td>\n",
       "      <td>[Deeptech]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2025-W52</td>\n",
       "      <td>advanced_materials</td>\n",
       "      <td>Advanced Materials</td>\n",
       "      <td>Deeptech</td>\n",
       "      <td>2026-01-03T05:12:06.878681+00:00</td>\n",
       "      <td>2025-12-22T00:00:00Z</td>\n",
       "      <td>None</td>\n",
       "      <td>Royal Society of Chemistry</td>\n",
       "      <td>Olusegun Oluwaseun Jimoh, Tolulope Ajuwon, Som...</td>\n",
       "      <td>PLGA nanoparticles in otoprotection and inner ...</td>\n",
       "      <td>RSC Adv., 2026, 16,76-106DOI: 10.1039/D5RA0600...</td>\n",
       "      <td>https://pubs.rsc.org/en/content/articlelanding...</td>\n",
       "      <td>https://pubs.rsc.org/en/content/articlelanding...</td>\n",
       "      <td>https://pubs.rsc.org/en/Content/Image/GA/D5RA0...</td>\n",
       "      <td>* \\r\\n Corresponding authors\\r\\na\\r\\n Departme...</td>\n",
       "      <td>(\"advanced materials\" OR \"material science\" OR...</td>\n",
       "      <td>[advanced_materials]</td>\n",
       "      <td>[Advanced Materials]</td>\n",
       "      <td>[Deeptech]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2025-W52</td>\n",
       "      <td>edge_ai</td>\n",
       "      <td>Edge AI</td>\n",
       "      <td>AI</td>\n",
       "      <td>2026-01-03T05:12:02.404709+00:00</td>\n",
       "      <td>2025-12-22T00:00:08Z</td>\n",
       "      <td>None</td>\n",
       "      <td>CNX Software</td>\n",
       "      <td>Debashis Das</td>\n",
       "      <td>Forlinx FCU3011 – An NVIDIA Jetson Orin Nano f...</td>\n",
       "      <td>Forlinx Embedded has recently released the FCU...</td>\n",
       "      <td>https://www.cnx-software.com/2025/12/22/forlin...</td>\n",
       "      <td>https://www.cnx-software.com/2025/12/22/forlin...</td>\n",
       "      <td>https://www.cnx-software.com/wp-content/upload...</td>\n",
       "      <td>Forlinx Embedded has recently released the FCU...</td>\n",
       "      <td>(\"edge AI\" OR \"on-device AI\" OR \"on device AI\"...</td>\n",
       "      <td>[edge_ai]</td>\n",
       "      <td>[Edge AI]</td>\n",
       "      <td>[AI]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2025-W52</td>\n",
       "      <td>climate_tech</td>\n",
       "      <td>Climate Tech</td>\n",
       "      <td>Climate</td>\n",
       "      <td>2026-01-03T05:12:05.337034+00:00</td>\n",
       "      <td>2025-12-22T00:31:12Z</td>\n",
       "      <td>None</td>\n",
       "      <td>Livemint</td>\n",
       "      <td>Sakshi Sadashiv</td>\n",
       "      <td>Aavishkaar Group looks to balance long-cycle c...</td>\n",
       "      <td>Aavishkaar Group is recalibrating its climate ...</td>\n",
       "      <td>https://www.livemint.com/companies/news/aavish...</td>\n",
       "      <td>https://www.livemint.com/companies/news/aavish...</td>\n",
       "      <td>https://www.livemint.com/lm-img/img/2025/12/21...</td>\n",
       "      <td>Impact investor Aavishkaar Capital is increasi...</td>\n",
       "      <td>(\"climate tech\" OR \"climate technology\" OR \"cl...</td>\n",
       "      <td>[carbon_capture, climate_tech]</td>\n",
       "      <td>[Carbon Capture, Climate Tech]</td>\n",
       "      <td>[Climate]</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2025-W52</td>\n",
       "      <td>advanced_materials</td>\n",
       "      <td>Advanced Materials</td>\n",
       "      <td>Deeptech</td>\n",
       "      <td>2026-01-03T05:12:06.878664+00:00</td>\n",
       "      <td>2025-12-22T00:45:20Z</td>\n",
       "      <td>the-times-of-india</td>\n",
       "      <td>The Times of India</td>\n",
       "      <td>Dipanjan Roy Chaudhury</td>\n",
       "      <td>Italian companies ready to seize opportunities...</td>\n",
       "      <td>Italy's Deputy Prime Minister Antonio Tajani h...</td>\n",
       "      <td>https://economictimes.indiatimes.com/news/econ...</td>\n",
       "      <td>https://economictimes.indiatimes.com/news/econ...</td>\n",
       "      <td>https://img.etimg.com/thumb/msid-126112178,wid...</td>\n",
       "      <td>Italy, the second industrial powerhouse in Eur...</td>\n",
       "      <td>(\"advanced materials\" OR \"material science\" OR...</td>\n",
       "      <td>[advanced_materials]</td>\n",
       "      <td>[Advanced Materials]</td>\n",
       "      <td>[Deeptech]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2025-W52</td>\n",
       "      <td>climate_tech</td>\n",
       "      <td>Climate Tech</td>\n",
       "      <td>Climate</td>\n",
       "      <td>2026-01-03T05:12:05.337027+00:00</td>\n",
       "      <td>2025-12-22T00:55:00Z</td>\n",
       "      <td>None</td>\n",
       "      <td>BusinessLine</td>\n",
       "      <td>K Vaitheeswaran</td>\n",
       "      <td>How a founder’s management style shapes the st...</td>\n",
       "      <td>Explore how different founder management style...</td>\n",
       "      <td>https://www.thehindubusinessline.com/specials/...</td>\n",
       "      <td>https://www.thehindubusinessline.com/specials/...</td>\n",
       "      <td>https://bl-i.thgim.com/public/incoming/yipxv7/...</td>\n",
       "      <td>Founders are wired differently from corporate ...</td>\n",
       "      <td>(\"climate tech\" OR \"climate technology\" OR \"cl...</td>\n",
       "      <td>[climate_tech]</td>\n",
       "      <td>[Climate Tech]</td>\n",
       "      <td>[Climate]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2025-W52</td>\n",
       "      <td>ai_healthcare</td>\n",
       "      <td>AI in Healthcare</td>\n",
       "      <td>Healthtech</td>\n",
       "      <td>2026-01-03T05:12:04.298492+00:00</td>\n",
       "      <td>2025-12-22T01:51:58Z</td>\n",
       "      <td>None</td>\n",
       "      <td>Histalk2.com</td>\n",
       "      <td>Jennifer</td>\n",
       "      <td>Morning Headlines 12/22/25</td>\n",
       "      <td>Holt exits New Mountain to create $30 billion ...</td>\n",
       "      <td>https://histalk2.com/2025/12/21/morning-headli...</td>\n",
       "      <td>https://histalk2.com/2025/12/21/morning-headli...</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>Holt exits New Mountain to create $30 billion ...</td>\n",
       "      <td>(\"healthcare AI\" OR \"medical AI\" OR \"AI in hea...</td>\n",
       "      <td>[ai_healthcare]</td>\n",
       "      <td>[AI in Healthcare]</td>\n",
       "      <td>[Healthtech]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  week_label          keyword_id           canonical    category  \\\n",
       "0   2025-W52      carbon_capture      Carbon Capture     Climate   \n",
       "1   2025-W52  advanced_materials  Advanced Materials    Deeptech   \n",
       "2   2025-W52  advanced_materials  Advanced Materials    Deeptech   \n",
       "3   2025-W52  advanced_materials  Advanced Materials    Deeptech   \n",
       "4   2025-W52  advanced_materials  Advanced Materials    Deeptech   \n",
       "5   2025-W52             edge_ai             Edge AI          AI   \n",
       "6   2025-W52        climate_tech        Climate Tech     Climate   \n",
       "7   2025-W52  advanced_materials  Advanced Materials    Deeptech   \n",
       "8   2025-W52        climate_tech        Climate Tech     Climate   \n",
       "9   2025-W52       ai_healthcare    AI in Healthcare  Healthtech   \n",
       "\n",
       "                     fetched_at_utc           publishedAt           source_id  \\\n",
       "0  2026-01-03T05:12:05.649879+00:00  2025-12-22T00:00:00Z                None   \n",
       "1  2026-01-03T05:12:06.878686+00:00  2025-12-22T00:00:00Z                None   \n",
       "2  2026-01-03T05:12:06.878670+00:00  2025-12-22T00:00:00Z                None   \n",
       "3  2026-01-03T05:12:06.878675+00:00  2025-12-22T00:00:00Z                None   \n",
       "4  2026-01-03T05:12:06.878681+00:00  2025-12-22T00:00:00Z                None   \n",
       "5  2026-01-03T05:12:02.404709+00:00  2025-12-22T00:00:08Z                None   \n",
       "6  2026-01-03T05:12:05.337034+00:00  2025-12-22T00:31:12Z                None   \n",
       "7  2026-01-03T05:12:06.878664+00:00  2025-12-22T00:45:20Z  the-times-of-india   \n",
       "8  2026-01-03T05:12:05.337027+00:00  2025-12-22T00:55:00Z                None   \n",
       "9  2026-01-03T05:12:04.298492+00:00  2025-12-22T01:51:58Z                None   \n",
       "\n",
       "                  source_name  \\\n",
       "0                  Nature.com   \n",
       "1  Royal Society of Chemistry   \n",
       "2  Royal Society of Chemistry   \n",
       "3  Royal Society of Chemistry   \n",
       "4  Royal Society of Chemistry   \n",
       "5                CNX Software   \n",
       "6                    Livemint   \n",
       "7          The Times of India   \n",
       "8                BusinessLine   \n",
       "9                Histalk2.com   \n",
       "\n",
       "                                              author  \\\n",
       "0  Hongli Wu, Jialun Xu, Junjie Tai, Jingkun Gao,...   \n",
       "1  Maria A Malandina, Sergei S. Leonchuk, Cheng Z...   \n",
       "2  Akash Balakrishnan, Milan Tom, Natarajan Rajam...   \n",
       "3  Mengran Liu, Kexin Liu, Zhuang Yan, Chenfei Ya...   \n",
       "4  Olusegun Oluwaseun Jimoh, Tolulope Ajuwon, Som...   \n",
       "5                                       Debashis Das   \n",
       "6                                    Sakshi Sadashiv   \n",
       "7                             Dipanjan Roy Chaudhury   \n",
       "8                                    K Vaitheeswaran   \n",
       "9                                           Jennifer   \n",
       "\n",
       "                                               title  \\\n",
       "0  Biomimetic Mn(III) porphyrin-catalyzed aromati...   \n",
       "1  \"Flexible energy\": energy harvesting and stora...   \n",
       "2  Engineered Cellulose-Supported Photocatalysts ...   \n",
       "3  Metal–(organic cocrystal) framework with a pho...   \n",
       "4  PLGA nanoparticles in otoprotection and inner ...   \n",
       "5  Forlinx FCU3011 – An NVIDIA Jetson Orin Nano f...   \n",
       "6  Aavishkaar Group looks to balance long-cycle c...   \n",
       "7  Italian companies ready to seize opportunities...   \n",
       "8  How a founder’s management style shapes the st...   \n",
       "9                         Morning Headlines 12/22/25   \n",
       "\n",
       "                                         description  \\\n",
       "0  Selective catalytic aromaticity-breaking epoxi...   \n",
       "1  J. Mater. Chem. A, 2026, Accepted ManuscriptDO...   \n",
       "2  J. Mater. Chem. A, 2026, Accepted ManuscriptDO...   \n",
       "3  J. Mater. Chem. A, 2026, Advance ArticleDOI: 1...   \n",
       "4  RSC Adv., 2026, 16,76-106DOI: 10.1039/D5RA0600...   \n",
       "5  Forlinx Embedded has recently released the FCU...   \n",
       "6  Aavishkaar Group is recalibrating its climate ...   \n",
       "7  Italy's Deputy Prime Minister Antonio Tajani h...   \n",
       "8  Explore how different founder management style...   \n",
       "9  Holt exits New Mountain to create $30 billion ...   \n",
       "\n",
       "                                                 url  \\\n",
       "0  https://www.nature.com/articles/s41467-025-673...   \n",
       "1  https://pubs.rsc.org/en/content/articlelanding...   \n",
       "2  https://pubs.rsc.org/en/content/articlelanding...   \n",
       "3  https://pubs.rsc.org/en/content/articlelanding...   \n",
       "4  https://pubs.rsc.org/en/content/articlelanding...   \n",
       "5  https://www.cnx-software.com/2025/12/22/forlin...   \n",
       "6  https://www.livemint.com/companies/news/aavish...   \n",
       "7  https://economictimes.indiatimes.com/news/econ...   \n",
       "8  https://www.thehindubusinessline.com/specials/...   \n",
       "9  https://histalk2.com/2025/12/21/morning-headli...   \n",
       "\n",
       "                                             url_key  \\\n",
       "0  https://www.nature.com/articles/s41467-025-673...   \n",
       "1  https://pubs.rsc.org/en/content/articlelanding...   \n",
       "2  https://pubs.rsc.org/en/content/articlelanding...   \n",
       "3  https://pubs.rsc.org/en/content/articlelanding...   \n",
       "4  https://pubs.rsc.org/en/content/articlelanding...   \n",
       "5  https://www.cnx-software.com/2025/12/22/forlin...   \n",
       "6  https://www.livemint.com/companies/news/aavish...   \n",
       "7  https://economictimes.indiatimes.com/news/econ...   \n",
       "8  https://www.thehindubusinessline.com/specials/...   \n",
       "9  https://histalk2.com/2025/12/21/morning-headli...   \n",
       "\n",
       "                                          urlToImage  \\\n",
       "0                                               <NA>   \n",
       "1                                               <NA>   \n",
       "2                                               <NA>   \n",
       "3  https://pubs.rsc.org/en/Content/Image/GA/D5TA0...   \n",
       "4  https://pubs.rsc.org/en/Content/Image/GA/D5RA0...   \n",
       "5  https://www.cnx-software.com/wp-content/upload...   \n",
       "6  https://www.livemint.com/lm-img/img/2025/12/21...   \n",
       "7  https://img.etimg.com/thumb/msid-126112178,wid...   \n",
       "8  https://bl-i.thgim.com/public/incoming/yipxv7/...   \n",
       "9                                               <NA>   \n",
       "\n",
       "                                             content  \\\n",
       "0  <li>Zhang, L. &amp; Ritter, T. A perspective o...   \n",
       "1  The rapid development of flexible electronics ...   \n",
       "2  The growing global demand for sustainable ener...   \n",
       "3  * \\r\\n Corresponding authors\\r\\na\\r\\n Key Labo...   \n",
       "4  * \\r\\n Corresponding authors\\r\\na\\r\\n Departme...   \n",
       "5  Forlinx Embedded has recently released the FCU...   \n",
       "6  Impact investor Aavishkaar Capital is increasi...   \n",
       "7  Italy, the second industrial powerhouse in Eur...   \n",
       "8  Founders are wired differently from corporate ...   \n",
       "9  Holt exits New Mountain to create $30 billion ...   \n",
       "\n",
       "                                               query  \\\n",
       "0  (\"carbon capture\" OR \"CCS\" OR \"carbon removal\"...   \n",
       "1  (\"advanced materials\" OR \"material science\" OR...   \n",
       "2  (\"advanced materials\" OR \"material science\" OR...   \n",
       "3  (\"advanced materials\" OR \"material science\" OR...   \n",
       "4  (\"advanced materials\" OR \"material science\" OR...   \n",
       "5  (\"edge AI\" OR \"on-device AI\" OR \"on device AI\"...   \n",
       "6  (\"climate tech\" OR \"climate technology\" OR \"cl...   \n",
       "7  (\"advanced materials\" OR \"material science\" OR...   \n",
       "8  (\"climate tech\" OR \"climate technology\" OR \"cl...   \n",
       "9  (\"healthcare AI\" OR \"medical AI\" OR \"AI in hea...   \n",
       "\n",
       "              matched_keyword_ids              matched_canonicals  \\\n",
       "0                [carbon_capture]                [Carbon Capture]   \n",
       "1            [advanced_materials]            [Advanced Materials]   \n",
       "2            [advanced_materials]            [Advanced Materials]   \n",
       "3            [advanced_materials]            [Advanced Materials]   \n",
       "4            [advanced_materials]            [Advanced Materials]   \n",
       "5                       [edge_ai]                       [Edge AI]   \n",
       "6  [carbon_capture, climate_tech]  [Carbon Capture, Climate Tech]   \n",
       "7            [advanced_materials]            [Advanced Materials]   \n",
       "8                  [climate_tech]                  [Climate Tech]   \n",
       "9                 [ai_healthcare]              [AI in Healthcare]   \n",
       "\n",
       "  matched_categories  matched_count  \n",
       "0          [Climate]              1  \n",
       "1         [Deeptech]              1  \n",
       "2         [Deeptech]              1  \n",
       "3         [Deeptech]              1  \n",
       "4         [Deeptech]              1  \n",
       "5               [AI]              1  \n",
       "6          [Climate]              2  \n",
       "7         [Deeptech]              1  \n",
       "8          [Climate]              1  \n",
       "9       [Healthtech]              1  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Keyword-hit mapping (preview):\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>week_label</th>\n",
       "      <th>url_key</th>\n",
       "      <th>matched_keyword_ids</th>\n",
       "      <th>matched_canonicals</th>\n",
       "      <th>matched_categories</th>\n",
       "      <th>matched_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2025-W52</td>\n",
       "      <td>http://deadline.com/2025/12/harry-styles-surpr...</td>\n",
       "      <td>[spacetech]</td>\n",
       "      <td>[SpaceTech]</td>\n",
       "      <td>[Deeptech]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2025-W52</td>\n",
       "      <td>http://digiday.com/media/here-are-the-biggest-...</td>\n",
       "      <td>[ai_agents, generative_ai]</td>\n",
       "      <td>[AI Agents, Generative AI]</td>\n",
       "      <td>[AI]</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2025-W52</td>\n",
       "      <td>http://electrek.co/2025/12/23/tesla-signs-mass...</td>\n",
       "      <td>[energy_storage]</td>\n",
       "      <td>[Energy Storage]</td>\n",
       "      <td>[Climate]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2025-W52</td>\n",
       "      <td>http://electrek.co/2025/12/26/elon-musk-drops-...</td>\n",
       "      <td>[energy_storage]</td>\n",
       "      <td>[Energy Storage]</td>\n",
       "      <td>[Climate]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2025-W52</td>\n",
       "      <td>http://electrek.co/2025/12/26/honda-buys-out-e...</td>\n",
       "      <td>[energy_storage]</td>\n",
       "      <td>[Energy Storage]</td>\n",
       "      <td>[Climate]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2025-W52</td>\n",
       "      <td>http://electrek.co/2025/12/28/opinion-its-time...</td>\n",
       "      <td>[energy_storage]</td>\n",
       "      <td>[Energy Storage]</td>\n",
       "      <td>[Climate]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2025-W52</td>\n",
       "      <td>http://futurism.com/advanced-transport/tesla-r...</td>\n",
       "      <td>[robotics]</td>\n",
       "      <td>[Robotics]</td>\n",
       "      <td>[AI]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2025-W52</td>\n",
       "      <td>http://futurism.com/future-society/trump-china...</td>\n",
       "      <td>[edge_ai]</td>\n",
       "      <td>[Edge AI]</td>\n",
       "      <td>[AI]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2025-W52</td>\n",
       "      <td>http://futurism.com/science-energy/doomsday-gl...</td>\n",
       "      <td>[spacetech]</td>\n",
       "      <td>[SpaceTech]</td>\n",
       "      <td>[Deeptech]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2025-W52</td>\n",
       "      <td>http://futurism.com/science-energy/google-co2-...</td>\n",
       "      <td>[energy_storage]</td>\n",
       "      <td>[Energy Storage]</td>\n",
       "      <td>[Climate]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  week_label                                            url_key  \\\n",
       "0   2025-W52  http://deadline.com/2025/12/harry-styles-surpr...   \n",
       "1   2025-W52  http://digiday.com/media/here-are-the-biggest-...   \n",
       "2   2025-W52  http://electrek.co/2025/12/23/tesla-signs-mass...   \n",
       "3   2025-W52  http://electrek.co/2025/12/26/elon-musk-drops-...   \n",
       "4   2025-W52  http://electrek.co/2025/12/26/honda-buys-out-e...   \n",
       "5   2025-W52  http://electrek.co/2025/12/28/opinion-its-time...   \n",
       "6   2025-W52  http://futurism.com/advanced-transport/tesla-r...   \n",
       "7   2025-W52  http://futurism.com/future-society/trump-china...   \n",
       "8   2025-W52  http://futurism.com/science-energy/doomsday-gl...   \n",
       "9   2025-W52  http://futurism.com/science-energy/google-co2-...   \n",
       "\n",
       "          matched_keyword_ids          matched_canonicals matched_categories  \\\n",
       "0                 [spacetech]                 [SpaceTech]         [Deeptech]   \n",
       "1  [ai_agents, generative_ai]  [AI Agents, Generative AI]               [AI]   \n",
       "2            [energy_storage]            [Energy Storage]          [Climate]   \n",
       "3            [energy_storage]            [Energy Storage]          [Climate]   \n",
       "4            [energy_storage]            [Energy Storage]          [Climate]   \n",
       "5            [energy_storage]            [Energy Storage]          [Climate]   \n",
       "6                  [robotics]                  [Robotics]               [AI]   \n",
       "7                   [edge_ai]                   [Edge AI]               [AI]   \n",
       "8                 [spacetech]                 [SpaceTech]         [Deeptech]   \n",
       "9            [energy_storage]            [Energy Storage]          [Climate]   \n",
       "\n",
       "   matched_count  \n",
       "0              1  \n",
       "1              2  \n",
       "2              1  \n",
       "3              1  \n",
       "4              1  \n",
       "5              1  \n",
       "6              1  \n",
       "7              1  \n",
       "8              1  \n",
       "9              1  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ============================================================\n",
    "# Cell 7: Deduplicate\n",
    "# ============================================================\n",
    "\n",
    "# This cell removes duplicates to avoid inflating counts and storage.\n",
    "#\n",
    "# Dedup strategy (practical / robust):\n",
    "# - Primary key: url_key (derived from url)\n",
    "# - Keep the first occurrence by (publishedAt, source_name, title) ordering\n",
    "#\n",
    "# Notes:\n",
    "# - The same article can match multiple keywords; we keep one \"article row\"\n",
    "#   per unique URL to create a clean raw corpus for downstream processing.\n",
    "# - We preserve a mapping table (keyword_hits_df) that records which keywords\n",
    "#   matched each URL, so you can still compute per-keyword metrics later.\n",
    "\n",
    "if normalized_df.empty:\n",
    "    print(\"No rows to deduplicate (normalized_df is empty).\")\n",
    "    dedup_df = normalized_df.copy()\n",
    "    keyword_hits_df = pd.DataFrame()\n",
    "else:\n",
    "    # --- 1) Build keyword-hit mapping (URL -> matched keywords) ---\n",
    "    keyword_hits_df = (\n",
    "        normalized_df[[\"week_label\", \"url_key\", \"keyword_id\", \"canonical\", \"category\"]]\n",
    "        .dropna(subset=[\"url_key\"])\n",
    "        .drop_duplicates()\n",
    "        .groupby([\"week_label\", \"url_key\"], as_index=False)\n",
    "        .agg(\n",
    "            matched_keyword_ids=(\"keyword_id\", lambda s: sorted(set(s))),\n",
    "            matched_canonicals=(\"canonical\", lambda s: sorted(set(s))),\n",
    "            matched_categories=(\"category\", lambda s: sorted(set(s))),\n",
    "            matched_count=(\"keyword_id\", lambda s: len(set(s))),\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # --- 2) Create a stable ordering so \"keep first\" is deterministic ---\n",
    "    work_df = normalized_df.copy()\n",
    "\n",
    "    # Ensure publishedAt exists for ordering; fall back to empty string\n",
    "    work_df[\"publishedAt_sort\"] = work_df[\"publishedAt\"].fillna(\"\").astype(str)\n",
    "\n",
    "    # Deterministic sort: oldest->newest, then source/title\n",
    "    work_df = work_df.sort_values(\n",
    "        by=[\"publishedAt_sort\", \"source_name\", \"title\", \"url_key\"],\n",
    "        ascending=[True, True, True, True],\n",
    "        kind=\"mergesort\",  # stable sort\n",
    "    )\n",
    "\n",
    "    before = len(work_df)\n",
    "\n",
    "    # --- 3) Deduplicate by URL key ---\n",
    "    dedup_df = (\n",
    "        work_df.dropna(subset=[\"url_key\"])\n",
    "              .drop_duplicates(subset=[\"week_label\", \"url_key\"], keep=\"first\")\n",
    "              .drop(columns=[\"publishedAt_sort\"])\n",
    "              .reset_index(drop=True)\n",
    "    )\n",
    "\n",
    "    after = len(dedup_df)\n",
    "\n",
    "    # --- 4) Join back the matched-keyword info (optional but useful) ---\n",
    "    dedup_df = dedup_df.merge(\n",
    "        keyword_hits_df,\n",
    "        on=[\"week_label\", \"url_key\"],\n",
    "        how=\"left\",\n",
    "        validate=\"one_to_one\",\n",
    "    )\n",
    "\n",
    "    print(\"Deduplication completed.\")\n",
    "    print(f\"Rows before dedup: {before:,}\")\n",
    "    print(f\"Rows after  dedup: {after:,}\")\n",
    "    print(f\"Duplicates removed: {before - after:,}\")\n",
    "\n",
    "    display(dedup_df.head(10))\n",
    "    print(\"\\nKeyword-hit mapping (preview):\")\n",
    "    display(keyword_hits_df.head(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "424957ea-0c93-4e77-85d3-c26c42a7b9d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved articles: /Users/yuetoya/Desktop/researchOS100-private/notebooks/data/raw/newsapi/2025-W52/articles.parquet  (rows=1,013)\n",
      "✅ Saved keyword hits: /Users/yuetoya/Desktop/researchOS100-private/notebooks/data/raw/newsapi/2025-W52/keyword_hits.parquet  (rows=1,013)\n",
      "✅ Saved metadata: /Users/yuetoya/Desktop/researchOS100-private/notebooks/data/raw/newsapi/2025-W52/metadata.json\n",
      "\n",
      "Saved files:\n",
      " - /Users/yuetoya/Desktop/researchOS100-private/notebooks/data/raw/newsapi/2025-W52/articles.parquet\n",
      " - /Users/yuetoya/Desktop/researchOS100-private/notebooks/data/raw/newsapi/2025-W52/keyword_hits.parquet\n",
      " - /Users/yuetoya/Desktop/researchOS100-private/notebooks/data/raw/newsapi/2025-W52/metadata.json\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# Cell 8: Save Raw Outputs (Week-partitioned)\n",
    "# ============================================================\n",
    "\n",
    "# This cell saves week-partitioned raw outputs to disk.\n",
    "#\n",
    "# Outputs:\n",
    "# - articles.parquet : deduplicated article corpus (one row per URL per week)\n",
    "# - keyword_hits.parquet : mapping table (URL -> matched keywords) for metrics\n",
    "# - metadata.json : run metadata (week, time window, counts, plan caps)\n",
    "#\n",
    "# Directory structure:\n",
    "# data/raw/newsapi/YYYY-WW/\n",
    "#   - articles.parquet\n",
    "#   - keyword_hits.parquet\n",
    "#   - metadata.json\n",
    "\n",
    "import json\n",
    "\n",
    "# --- Resolve output directory for the target ISO week ---\n",
    "week_dir = RAW_DATA_DIR / week_label\n",
    "week_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "articles_path = week_dir / \"articles.parquet\"\n",
    "hits_path = week_dir / \"keyword_hits.parquet\"\n",
    "meta_path = week_dir / \"metadata.json\"\n",
    "\n",
    "# --- Safety checks ---\n",
    "if dedup_df.empty:\n",
    "    print(\"⚠️ dedup_df is empty. Nothing to save for articles.\")\n",
    "else:\n",
    "    dedup_df.to_parquet(articles_path, index=False)\n",
    "    print(f\"✅ Saved articles: {articles_path}  (rows={len(dedup_df):,})\")\n",
    "\n",
    "if \"keyword_hits_df\" in globals() and not keyword_hits_df.empty:\n",
    "    keyword_hits_df.to_parquet(hits_path, index=False)\n",
    "    print(f\"✅ Saved keyword hits: {hits_path}  (rows={len(keyword_hits_df):,})\")\n",
    "else:\n",
    "    print(\"⚠️ keyword_hits_df is empty or not defined. Nothing to save for keyword hits.\")\n",
    "\n",
    "# --- Save run metadata (for traceability / reproducibility) ---\n",
    "run_metadata = {\n",
    "    \"week_label\": week_label,\n",
    "    \"newsapi_from\": newsapi_from,\n",
    "    \"newsapi_to\": newsapi_to,\n",
    "    \"language\": DEFAULT_LANGUAGE,\n",
    "    \"page_size\": DEFAULT_PAGE_SIZE,\n",
    "    \"sort_by\": DEFAULT_SORT_BY,\n",
    "    \"global_exclusions\": GLOBAL_EXCLUSIONS,\n",
    "    \"developer_max_results\": DEVELOPER_MAX_RESULTS,\n",
    "    \"fetched_at_utc\": datetime.now(timezone.utc).isoformat(),\n",
    "    \"counts\": {\n",
    "        \"raw_records\": int(len(raw_records)),\n",
    "        \"normalized_rows\": int(len(normalized_df)) if \"normalized_df\" in globals() else None,\n",
    "        \"dedup_rows\": int(len(dedup_df)) if \"dedup_df\" in globals() else None,\n",
    "        \"keyword_hits_rows\": int(len(keyword_hits_df)) if \"keyword_hits_df\" in globals() else None,\n",
    "    },\n",
    "}\n",
    "\n",
    "with open(meta_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(run_metadata, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(f\"✅ Saved metadata: {meta_path}\")\n",
    "\n",
    "# --- Preview saved paths ---\n",
    "print(\"\\nSaved files:\")\n",
    "for p in [articles_path, hits_path, meta_path]:\n",
    "    if p.exists():\n",
    "        print(f\" - {p}\")\n",
    "    else:\n",
    "        print(f\" - (not created) {p}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bc179c2c-80c8-477e-ad38-a8bd23346864",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Request Summary ===\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>event</th>\n",
       "      <th>requests</th>\n",
       "      <th>avg_elapsed_sec</th>\n",
       "      <th>max_elapsed_sec</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>success</td>\n",
       "      <td>15</td>\n",
       "      <td>0.450067</td>\n",
       "      <td>1.579</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     event  requests  avg_elapsed_sec  max_elapsed_sec\n",
       "0  success        15         0.450067            1.579"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Record Counts ===\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>week_label</th>\n",
       "      <th>raw_records</th>\n",
       "      <th>normalized_rows</th>\n",
       "      <th>dedup_rows</th>\n",
       "      <th>keyword_hits_rows</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2025-W52</td>\n",
       "      <td>1080</td>\n",
       "      <td>1080</td>\n",
       "      <td>1013</td>\n",
       "      <td>1013</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  week_label  raw_records  normalized_rows  dedup_rows  keyword_hits_rows\n",
       "0   2025-W52         1080             1080        1013               1013"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Keyword Coverage (unique URLs matched) ===\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>category</th>\n",
       "      <th>canonical</th>\n",
       "      <th>unique_urls</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AI</td>\n",
       "      <td>AI Agents</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AI</td>\n",
       "      <td>Generative AI</td>\n",
       "      <td>99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AI</td>\n",
       "      <td>Robotics</td>\n",
       "      <td>99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AI</td>\n",
       "      <td>Edge AI</td>\n",
       "      <td>76</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Climate</td>\n",
       "      <td>Energy Storage</td>\n",
       "      <td>98</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Climate</td>\n",
       "      <td>Carbon Capture</td>\n",
       "      <td>44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Climate</td>\n",
       "      <td>Climate Tech</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Deeptech</td>\n",
       "      <td>Quantum Computing</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Deeptech</td>\n",
       "      <td>SpaceTech</td>\n",
       "      <td>95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Deeptech</td>\n",
       "      <td>Advanced Materials</td>\n",
       "      <td>76</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Deeptech</td>\n",
       "      <td>Nuclear Fusion</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Healthtech</td>\n",
       "      <td>Digital Health</td>\n",
       "      <td>98</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Healthtech</td>\n",
       "      <td>Clinical AI</td>\n",
       "      <td>95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Healthtech</td>\n",
       "      <td>Precision Medicine</td>\n",
       "      <td>34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Healthtech</td>\n",
       "      <td>AI in Healthcare</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      category           canonical  unique_urls\n",
       "0           AI           AI Agents          100\n",
       "2           AI       Generative AI           99\n",
       "3           AI            Robotics           99\n",
       "1           AI             Edge AI           76\n",
       "6      Climate      Energy Storage           98\n",
       "4      Climate      Carbon Capture           44\n",
       "5      Climate        Climate Tech           21\n",
       "9     Deeptech   Quantum Computing          100\n",
       "10    Deeptech           SpaceTech           95\n",
       "7     Deeptech  Advanced Materials           76\n",
       "8     Deeptech      Nuclear Fusion           22\n",
       "13  Healthtech      Digital Health           98\n",
       "12  Healthtech         Clinical AI           95\n",
       "14  Healthtech  Precision Medicine           34\n",
       "11  Healthtech    AI in Healthcare           22"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Category Coverage (unique URLs matched) ===\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>category</th>\n",
       "      <th>unique_urls</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AI</td>\n",
       "      <td>374</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Deeptech</td>\n",
       "      <td>293</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Healthtech</td>\n",
       "      <td>249</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Climate</td>\n",
       "      <td>163</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     category  unique_urls\n",
       "0          AI          374\n",
       "2    Deeptech          293\n",
       "3  Healthtech          249\n",
       "1     Climate          163"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Potentially Capped Keywords (Developer Plan) ===\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>canonical</th>\n",
       "      <th>totalResults</th>\n",
       "      <th>likely_capped</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>SpaceTech</td>\n",
       "      <td>585</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Generative AI</td>\n",
       "      <td>387</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Robotics</td>\n",
       "      <td>275</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AI Agents</td>\n",
       "      <td>245</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Energy Storage</td>\n",
       "      <td>141</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Clinical AI</td>\n",
       "      <td>107</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Digital Health</td>\n",
       "      <td>107</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Quantum Computing</td>\n",
       "      <td>102</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           canonical  totalResults  likely_capped\n",
       "0          SpaceTech           585           True\n",
       "1      Generative AI           387           True\n",
       "2           Robotics           275           True\n",
       "3          AI Agents           245           True\n",
       "4     Energy Storage           141           True\n",
       "5        Clinical AI           107           True\n",
       "6     Digital Health           107           True\n",
       "7  Quantum Computing           102           True"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Top Sources (dedup corpus) ===\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source_name</th>\n",
       "      <th>articles</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>190</th>\n",
       "      <td>Pypi.org</td>\n",
       "      <td>125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>GlobeNewswire</td>\n",
       "      <td>68</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>282</th>\n",
       "      <td>Yahoo Entertainment</td>\n",
       "      <td>62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>255</th>\n",
       "      <td>The Times of India</td>\n",
       "      <td>58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>155</th>\n",
       "      <td>Nature.com</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130</th>\n",
       "      <td>Linkedin.com</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>204</th>\n",
       "      <td>Royal Society of Chemistry</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>Financial Post</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>129</th>\n",
       "      <td>Lifesciencesworld.com</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>Barchart.com</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>BusinessLine</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133</th>\n",
       "      <td>Livemint</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>169</th>\n",
       "      <td>OilPrice.com</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>220</th>\n",
       "      <td>Slashdot.org</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>252</th>\n",
       "      <td>The Punch</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>181</th>\n",
       "      <td>Plos.org</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>Digitimes</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>CleanTechnica</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>209</th>\n",
       "      <td>Science Daily</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>Bleeding Cool News</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>Freerepublic.com</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>238</th>\n",
       "      <td>TechRadar</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>237</th>\n",
       "      <td>TechCrunch</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>CNA</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112</th>\n",
       "      <td>International Business Times</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      source_name  articles\n",
       "190                      Pypi.org       125\n",
       "93                  GlobeNewswire        68\n",
       "282           Yahoo Entertainment        62\n",
       "255            The Times of India        58\n",
       "155                    Nature.com        31\n",
       "130                  Linkedin.com        24\n",
       "204    Royal Society of Chemistry        20\n",
       "75                 Financial Post        19\n",
       "129         Lifesciencesworld.com        18\n",
       "24                   Barchart.com        14\n",
       "37                   BusinessLine        13\n",
       "133                      Livemint        13\n",
       "169                  OilPrice.com        12\n",
       "220                  Slashdot.org        12\n",
       "252                     The Punch        12\n",
       "181                      Plos.org        11\n",
       "69                      Digitimes        11\n",
       "48                  CleanTechnica        10\n",
       "209                 Science Daily         9\n",
       "30             Bleeding Cool News         8\n",
       "80               Freerepublic.com         7\n",
       "238                     TechRadar         7\n",
       "237                    TechCrunch         6\n",
       "40                            CNA         6\n",
       "112  International Business Times         6"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Weekly fetch summary complete.\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# Cell 9: Weekly Fetch Summary (Lightweight Report)\n",
    "# ============================================================\n",
    "\n",
    "# This cell produces a lightweight weekly report for quick inspection.\n",
    "# It summarizes:\n",
    "# - Request outcomes (success / rate-limited / errors)\n",
    "# - Record counts by keyword/category\n",
    "# - Cap indicators for the Developer plan (if applicable)\n",
    "# - Top sources (by article count)\n",
    "#\n",
    "# This report is meant for human review after each weekly run.\n",
    "\n",
    "# -------------------------------\n",
    "# 1) Request-level summary\n",
    "# -------------------------------\n",
    "if \"fetch_log_df\" in globals() and not fetch_log_df.empty:\n",
    "    print(\"=== Request Summary ===\")\n",
    "    req_summary = (\n",
    "        fetch_log_df\n",
    "        .groupby([\"event\"], as_index=False)\n",
    "        .agg(\n",
    "            requests=(\"event\", \"count\"),\n",
    "            avg_elapsed_sec=(\"elapsed_sec\", \"mean\"),\n",
    "            max_elapsed_sec=(\"elapsed_sec\", \"max\"),\n",
    "        )\n",
    "        .sort_values(\"requests\", ascending=False)\n",
    "    )\n",
    "    display(req_summary)\n",
    "\n",
    "    # Errors by keyword (if any)\n",
    "    error_events = {\"http_error\", \"network_error\", \"rate_limited\", \"max_results_reached\"}\n",
    "    if fetch_log_df[\"event\"].isin(error_events).any():\n",
    "        print(\"\\n=== Non-success Events by Keyword ===\")\n",
    "        non_success = (\n",
    "            fetch_log_df[fetch_log_df[\"event\"] != \"success\"]\n",
    "            .groupby([\"canonical\", \"event\"], as_index=False)\n",
    "            .agg(n=(\"event\", \"count\"))\n",
    "            .sort_values([\"n\", \"canonical\"], ascending=[False, True])\n",
    "        )\n",
    "        display(non_success)\n",
    "else:\n",
    "    print(\"No request log available (fetch_log_df is empty or not defined).\")\n",
    "\n",
    "# -------------------------------\n",
    "# 2) Article counts (raw / normalized / dedup)\n",
    "# -------------------------------\n",
    "print(\"\\n=== Record Counts ===\")\n",
    "counts_tbl = pd.DataFrame([{\n",
    "    \"week_label\": week_label,\n",
    "    \"raw_records\": len(raw_records) if \"raw_records\" in globals() else 0,\n",
    "    \"normalized_rows\": len(normalized_df) if \"normalized_df\" in globals() else 0,\n",
    "    \"dedup_rows\": len(dedup_df) if \"dedup_df\" in globals() else 0,\n",
    "    \"keyword_hits_rows\": len(keyword_hits_df) if \"keyword_hits_df\" in globals() else 0,\n",
    "}])\n",
    "display(counts_tbl)\n",
    "\n",
    "# -------------------------------\n",
    "# 3) Per-keyword coverage (dedup corpus)\n",
    "# -------------------------------\n",
    "# Note: dedup_df contains one row per unique URL per week, but includes matched keyword arrays.\n",
    "# We use keyword_hits_df for clean per-keyword counts.\n",
    "if \"keyword_hits_df\" in globals() and not keyword_hits_df.empty:\n",
    "    print(\"\\n=== Keyword Coverage (unique URLs matched) ===\")\n",
    "    kw_cov = (\n",
    "        keyword_hits_df\n",
    "        .explode(\"matched_keyword_ids\")\n",
    "        .rename(columns={\"matched_keyword_ids\": \"keyword_id\"})\n",
    "        .merge(\n",
    "            keywords_df_final[[\"keyword_id\", \"canonical\", \"category\"]],\n",
    "            on=\"keyword_id\",\n",
    "            how=\"left\",\n",
    "            validate=\"many_to_one\",\n",
    "        )\n",
    "        .groupby([\"category\", \"canonical\"], as_index=False)\n",
    "        .agg(unique_urls=(\"url_key\", \"nunique\"))\n",
    "        .sort_values([\"category\", \"unique_urls\"], ascending=[True, False])\n",
    "    )\n",
    "    display(kw_cov)\n",
    "\n",
    "    print(\"\\n=== Category Coverage (unique URLs matched) ===\")\n",
    "    cat_cov = (\n",
    "        kw_cov\n",
    "        .groupby(\"category\", as_index=False)\n",
    "        .agg(unique_urls=(\"unique_urls\", \"sum\"))\n",
    "        .sort_values(\"unique_urls\", ascending=False)\n",
    "    )\n",
    "    display(cat_cov)\n",
    "else:\n",
    "    print(\"\\nNo keyword-hit mapping available (keyword_hits_df is empty or not defined).\")\n",
    "\n",
    "# -------------------------------\n",
    "# 4) Cap indicators (Developer plan)\n",
    "# -------------------------------\n",
    "# If you applied the Developer cap logic in Cell 5, you can infer likely capping by:\n",
    "# - keywords where totalResults reported > DEVELOPER_MAX_RESULTS\n",
    "# This requires totalResults to be in the request log.\n",
    "if (\n",
    "    \"fetch_log_df\" in globals()\n",
    "    and not fetch_log_df.empty\n",
    "    and \"totalResults\" in fetch_log_df.columns\n",
    "    and DEVELOPER_MAX_RESULTS is not None\n",
    "):\n",
    "    print(\"\\n=== Potentially Capped Keywords (Developer Plan) ===\")\n",
    "    # totalResults is recorded per request; use the first-page success rows.\n",
    "    first_page = fetch_log_df[(fetch_log_df[\"event\"] == \"success\") & (fetch_log_df[\"page\"] == 1)]\n",
    "    capped = (\n",
    "        first_page\n",
    "        .groupby(\"canonical\", as_index=False)\n",
    "        .agg(totalResults=(\"totalResults\", \"max\"))\n",
    "        .sort_values(\"totalResults\", ascending=False)\n",
    "    )\n",
    "    capped[\"likely_capped\"] = capped[\"totalResults\"] > DEVELOPER_MAX_RESULTS\n",
    "    display(capped[capped[\"likely_capped\"] == True].reset_index(drop=True))\n",
    "\n",
    "# -------------------------------\n",
    "# 5) Top sources (in dedup corpus)\n",
    "# -------------------------------\n",
    "if \"dedup_df\" in globals() and not dedup_df.empty:\n",
    "    print(\"\\n=== Top Sources (dedup corpus) ===\")\n",
    "    top_sources = (\n",
    "        dedup_df\n",
    "        .groupby(\"source_name\", as_index=False)\n",
    "        .agg(articles=(\"url_key\", \"nunique\"))\n",
    "        .sort_values(\"articles\", ascending=False)\n",
    "        .head(25)\n",
    "    )\n",
    "    display(top_sources)\n",
    "\n",
    "print(\"\\nWeekly fetch summary complete.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de1fea36-74ca-4cba-9468-162c1114b882",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
